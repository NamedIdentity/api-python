:py:mod:`abacusai`
==================

.. py:module:: abacusai


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   api_class/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   abacus_api/index.rst
   agent/index.rst
   agent_data_document_info/index.rst
   agent_data_execution_result/index.rst
   agent_data_upload_result/index.rst
   agent_version/index.rst
   ai_building_task/index.rst
   algorithm/index.rst
   annotation/index.rst
   annotation_config/index.rst
   annotation_document/index.rst
   annotation_entry/index.rst
   annotations_status/index.rst
   api_client_utils/index.rst
   api_endpoint/index.rst
   api_key/index.rst
   app_user_group/index.rst
   application_connector/index.rst
   batch_prediction/index.rst
   batch_prediction_version/index.rst
   batch_prediction_version_logs/index.rst
   categorical_range_violation/index.rst
   chat_message/index.rst
   chat_session/index.rst
   client/index.rst
   code_source/index.rst
   concatenation_config/index.rst
   cpu_gpu_memory_specs/index.rst
   cryptography/index.rst
   custom_loss_function/index.rst
   custom_metric/index.rst
   custom_metric_version/index.rst
   custom_train_function_info/index.rst
   data_consistency_duplication/index.rst
   data_metrics/index.rst
   data_prep_logs/index.rst
   data_quality_results/index.rst
   database_connector/index.rst
   database_connector_column/index.rst
   database_connector_schema/index.rst
   dataset/index.rst
   dataset_column/index.rst
   dataset_version/index.rst
   dataset_version_logs/index.rst
   deployment/index.rst
   deployment_auth_token/index.rst
   deployment_conversation/index.rst
   deployment_conversation_event/index.rst
   deployment_conversation_export/index.rst
   document/index.rst
   document_annotation/index.rst
   document_data/index.rst
   document_retriever/index.rst
   document_retriever_config/index.rst
   document_retriever_lookup_result/index.rst
   document_retriever_version/index.rst
   document_store/index.rst
   document_store_import/index.rst
   drift_distribution/index.rst
   drift_distributions/index.rst
   eda/index.rst
   eda_chart_description/index.rst
   eda_collinearity/index.rst
   eda_data_consistency/index.rst
   eda_feature_association/index.rst
   eda_feature_collinearity/index.rst
   eda_forecasting_analysis/index.rst
   eda_version/index.rst
   embedding_feature_drift_distribution/index.rst
   execute_feature_group_operation/index.rst
   external_application/index.rst
   extracted_fields/index.rst
   feature/index.rst
   feature_distribution/index.rst
   feature_drift_record/index.rst
   feature_drift_summary/index.rst
   feature_group/index.rst
   feature_group_document/index.rst
   feature_group_export/index.rst
   feature_group_export_config/index.rst
   feature_group_export_download_url/index.rst
   feature_group_lineage/index.rst
   feature_group_refresh_export_config/index.rst
   feature_group_row/index.rst
   feature_group_row_process/index.rst
   feature_group_row_process_logs/index.rst
   feature_group_row_process_summary/index.rst
   feature_group_template/index.rst
   feature_group_template_variable_options/index.rst
   feature_group_version/index.rst
   feature_importance/index.rst
   feature_mapping/index.rst
   feature_record/index.rst
   file_connector/index.rst
   file_connector_instructions/index.rst
   file_connector_verification/index.rst
   finetuned_pretrained_model/index.rst
   forecasting_analysis_graph_data/index.rst
   forecasting_monitor_item_analysis/index.rst
   forecasting_monitor_summary/index.rst
   function_logs/index.rst
   generated_pit_feature_config_option/index.rst
   graph_dashboard/index.rst
   holdout_analysis/index.rst
   holdout_analysis_version/index.rst
   hosted_model_token/index.rst
   indexing_config/index.rst
   inferred_feature_mappings/index.rst
   item_statistics/index.rst
   llm_code_block/index.rst
   llm_execution_preview/index.rst
   llm_execution_result/index.rst
   llm_generated_code/index.rst
   llm_input/index.rst
   llm_parameters/index.rst
   llm_response/index.rst
   memory_options/index.rst
   model/index.rst
   model_artifacts_export/index.rst
   model_blueprint_export/index.rst
   model_blueprint_stage/index.rst
   model_location/index.rst
   model_metrics/index.rst
   model_monitor/index.rst
   model_monitor_org_summary/index.rst
   model_monitor_summary/index.rst
   model_monitor_summary_from_org/index.rst
   model_monitor_version/index.rst
   model_monitor_version_metric_data/index.rst
   model_training_type_for_deployment/index.rst
   model_upload/index.rst
   model_version/index.rst
   model_version_feature_group_schema/index.rst
   modification_lock_info/index.rst
   module/index.rst
   monitor_alert/index.rst
   monitor_alert_version/index.rst
   monitor_drift_and_distributions/index.rst
   natural_language_explanation/index.rst
   nested_feature/index.rst
   nested_feature_schema/index.rst
   null_violation/index.rst
   organization_external_application_settings/index.rst
   organization_group/index.rst
   organization_search_result/index.rst
   organization_secret/index.rst
   page_data/index.rst
   pipeline/index.rst
   pipeline_reference/index.rst
   pipeline_step/index.rst
   pipeline_step_version/index.rst
   pipeline_step_version_logs/index.rst
   pipeline_step_version_reference/index.rst
   pipeline_version/index.rst
   pipeline_version_logs/index.rst
   point_in_time_feature/index.rst
   point_in_time_feature_info/index.rst
   point_in_time_group/index.rst
   point_in_time_group_feature/index.rst
   prediction_client/index.rst
   prediction_dataset/index.rst
   prediction_feature_group/index.rst
   prediction_input/index.rst
   prediction_metric/index.rst
   prediction_metric_version/index.rst
   prediction_operator/index.rst
   prediction_operator_version/index.rst
   problem_type/index.rst
   project/index.rst
   project_config/index.rst
   project_feature_group/index.rst
   project_feature_group_schema/index.rst
   project_feature_group_schema_version/index.rst
   project_validation/index.rst
   python_function/index.rst
   python_function_validator/index.rst
   python_plot_function/index.rst
   range_violation/index.rst
   refresh_pipeline_run/index.rst
   refresh_policy/index.rst
   refresh_schedule/index.rst
   resolved_feature_group_template/index.rst
   return_class/index.rst
   schema/index.rst
   streaming_auth_token/index.rst
   streaming_client/index.rst
   streaming_connector/index.rst
   streaming_row_count/index.rst
   streaming_sample_code/index.rst
   test_point_predictions/index.rst
   training_config_options/index.rst
   type_violation/index.rst
   upload/index.rst
   upload_part/index.rst
   use_case/index.rst
   use_case_requirements/index.rst
   user/index.rst
   user_exception/index.rst
   webhook/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   abacusai.ApiClass
   abacusai.FieldDescriptor
   abacusai._ApiClassFactory
   abacusai.BatchPredictionArgs
   abacusai.AnomalyDetectionBatchPredictionArgs
   abacusai.AnomalyOutliersBatchPredictionArgs
   abacusai.ForecastingBatchPredictionArgs
   abacusai.NamedEntityExtractionBatchPredictionArgs
   abacusai.PersonalizationBatchPredictionArgs
   abacusai.PredictiveModelingBatchPredictionArgs
   abacusai.PretrainedModelsBatchPredictionArgs
   abacusai.SentenceBoundaryDetectionBatchPredictionArgs
   abacusai.ThemeAnalysisBatchPredictionArgs
   abacusai.ChatLLMBatchPredictionArgs
   abacusai._BatchPredictionArgsFactory
   abacusai.BlobInput
   abacusai.OcrMode
   abacusai.ParsingConfig
   abacusai.DocumentProcessingConfig
   abacusai.IncrementalDatabaseConnectorConfig
   abacusai.DatasetConfig
   abacusai.ConfluenceDatasetConfig
   abacusai.GoogleAnalyticsDatasetConfig
   abacusai.GoogleDriveDatasetConfig
   abacusai.JiraDatasetConfig
   abacusai.OneDriveDatasetConfig
   abacusai.SharepointDatasetConfig
   abacusai.ZendeskDatasetConfig
   abacusai._DatasetConfigFactory
   abacusai.VectorStoreTextEncoder
   abacusai.VectorStoreConfig
   abacusai.DocumentRetrieverConfig
   abacusai.ApiEnum
   abacusai.ProblemType
   abacusai.RegressionObjective
   abacusai.RegressionTreeHPOMode
   abacusai.PartialDependenceAnalysis
   abacusai.RegressionAugmentationStrategy
   abacusai.RegressionTargetTransform
   abacusai.RegressionTypeOfSplit
   abacusai.RegressionTimeSplitMethod
   abacusai.RegressionLossFunction
   abacusai.ExplainerType
   abacusai.SamplingMethodType
   abacusai.MergeMode
   abacusai.FillLogic
   abacusai.BatchSize
   abacusai.HolidayCalendars
   abacusai.FileFormat
   abacusai.ExperimentationMode
   abacusai.PersonalizationTrainingMode
   abacusai.PersonalizationObjective
   abacusai.ForecastingObjective
   abacusai.ForecastingFrequency
   abacusai.ForecastingDataSplitType
   abacusai.ForecastingLossFunction
   abacusai.ForecastingLocalScaling
   abacusai.ForecastingFillMethod
   abacusai.ForecastingQuanitlesExtensionMethod
   abacusai.NERObjective
   abacusai.NERModelType
   abacusai.NLPDocumentFormat
   abacusai.SentimentType
   abacusai.ClusteringImputationMethod
   abacusai.ConnectorType
   abacusai.ApplicationConnectorType
   abacusai.PythonFunctionArgumentType
   abacusai.PythonFunctionOutputArgumentType
   abacusai.LLMName
   abacusai.MonitorAlertType
   abacusai.FeatureDriftType
   abacusai.DataIntegrityViolationType
   abacusai.BiasType
   abacusai.AlertActionType
   abacusai.PythonFunctionType
   abacusai.EvalArtifactType
   abacusai.FieldDescriptorType
   abacusai.SamplingConfig
   abacusai.NSamplingConfig
   abacusai.PercentSamplingConfig
   abacusai._SamplingConfigFactory
   abacusai.MergeConfig
   abacusai.LastNMergeConfig
   abacusai.TimeWindowMergeConfig
   abacusai._MergeConfigFactory
   abacusai.TrainingConfig
   abacusai.PersonalizationTrainingConfig
   abacusai.RegressionTrainingConfig
   abacusai.ForecastingTrainingConfig
   abacusai.NamedEntityExtractionTrainingConfig
   abacusai.NaturalLanguageSearchTrainingConfig
   abacusai.ChatLLMTrainingConfig
   abacusai.SentenceBoundaryDetectionTrainingConfig
   abacusai.SentimentDetectionTrainingConfig
   abacusai.DocumentClassificationTrainingConfig
   abacusai.DocumentSummarizationTrainingConfig
   abacusai.DocumentVisualizationTrainingConfig
   abacusai.ClusteringTrainingConfig
   abacusai.ClusteringTimeseriesTrainingConfig
   abacusai.EventAnomalyTrainingConfig
   abacusai.CumulativeForecastingTrainingConfig
   abacusai.AnomalyDetectionTrainingConfig
   abacusai.ThemeAnalysisTrainingConfig
   abacusai.AIAgentTrainingConfig
   abacusai.CustomTrainedModelTrainingConfig
   abacusai.CustomAlgorithmTrainingConfig
   abacusai.OptimizationTrainingConfig
   abacusai._TrainingConfigFactory
   abacusai.DeployableAlgorithm
   abacusai.ForecastingMonitorConfig
   abacusai.AlertConditionConfig
   abacusai.AccuracyBelowThresholdConditionConfig
   abacusai.FeatureDriftConditionConfig
   abacusai.TargetDriftConditionConfig
   abacusai.HistoryLengthDriftConditionConfig
   abacusai.DataIntegrityViolationConditionConfig
   abacusai.BiasViolationConditionConfig
   abacusai.PredictionCountConditionConfig
   abacusai._AlertConditionConfigFactory
   abacusai.AlertActionConfig
   abacusai.EmailActionConfig
   abacusai._AlertActionConfigFactory
   abacusai.FeatureMappingConfig
   abacusai.ProjectFeatureGroupTypeMappingsConfig
   abacusai.PythonFunctionArgument
   abacusai.OutputVariableMapping
   abacusai.FeatureGroupExportConfig
   abacusai.FileConnectorExportConfig
   abacusai.DatabaseConnectorExportConfig
   abacusai._FeatureGroupExportConfigFactory
   abacusai.AgentResponse
   abacusai.ApiClient
   abacusai.ClientOptions
   abacusai.ReadOnlyClient
   abacusai.PredictionClient
   abacusai.StreamingClient




Attributes
~~~~~~~~~~

.. autoapisummary::

   abacusai._request_context
   abacusai.__version__


.. py:class:: ApiClass


   Bases: :py:obj:`abc.ABC`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: _upper_snake_case_keys
      :type: bool

      

   .. py:attribute:: _support_kwargs
      :type: bool

      

   .. py:method:: __post_init__()


   .. py:method:: _get_builder()
      :classmethod:


   .. py:method:: __str__()

      Return str(self).


   .. py:method:: _repr_html_()


   .. py:method:: __getitem__(item)


   .. py:method:: __setitem__(item, value)


   .. py:method:: _unset_item(item)


   .. py:method:: get(item, default = None)


   .. py:method:: pop(item, default = NotImplemented)


   .. py:method:: to_dict()

      Standardizes converting an ApiClass to dictionary.
      Keys of response dictionary are converted to camel case.
      This also validates the fields ( type, value, etc ) received in the dictionary.


   .. py:method:: from_dict(input_dict)
      :classmethod:



.. py:class:: FieldDescriptor


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Configs for vector store indexing.

   :param field: The field to be extracted. This will be used as the key in the response.
   :type field: str
   :param description: The description of this field. If not included, the response_field will be used.
   :type description: str
   :param example_extraction: An example of this extracted field.
   :type example_extraction: Union[str, int, bool, float]
   :param type: The type of this field. If not provided, the default type is STRING.
   :type type: enums.FieldDescriptorType

   .. py:attribute:: field
      :type: str

      

   .. py:attribute:: description
      :type: str

      

   .. py:attribute:: example_extraction
      :type: Union[str, int, bool, float, list, dict]

      

   .. py:attribute:: type
      :type: abacusai.api_class.enums.FieldDescriptorType

      


.. py:class:: _ApiClassFactory


   Bases: :py:obj:`abc.ABC`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key

      

   .. py:attribute:: config_class_map

      

   .. py:method:: from_dict(config)
      :classmethod:



.. py:class:: BatchPredictionArgs


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: _support_kwargs
      :type: bool

      

   .. py:attribute:: kwargs
      :type: dict

      

   .. py:attribute:: problem_type
      :type: abacusai.api_class.enums.ProblemType

      

   .. py:method:: _get_builder()
      :classmethod:



.. py:class:: AnomalyDetectionBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the ANOMALY_DETECTION problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param prediction_time_endpoint: The end point for predictions.
   :type prediction_time_endpoint: str
   :param prediction_time_range: Over what period of time should we make predictions (in seconds).
   :type prediction_time_range: int
   :param minimum_anomaly_score: Exclude results with an anomaly score (1 in x event) below this threshold. Range: [1, 1_000_000_000_000].
   :type minimum_anomaly_score: int
   :param summary_mode: Only show top anomalies per ID.
   :type summary_mode: bool
   :param attach_raw_data: Return raw data along with anomalies.
   :type attach_raw_data: bool
   :param small_batch: Size of batch data guaranteed to be small.
   :type small_batch: bool

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: prediction_time_endpoint
      :type: str

      

   .. py:attribute:: prediction_time_range
      :type: int

      

   .. py:attribute:: minimum_anomaly_score
      :type: int

      

   .. py:attribute:: summary_mode
      :type: bool

      

   .. py:attribute:: attach_raw_data
      :type: bool

      

   .. py:attribute:: small_batch
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: AnomalyOutliersBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the ANOMALY_OUTLIERS problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param threshold: The threshold for detecting an anomaly. Range: [0.8, 0.99]
   :type threshold: float

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: threshold
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: ForecastingBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the FORECASTING problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation
   :type for_eval: bool
   :param predictions_start_date: The start date for predictions.
   :type predictions_start_date: str
   :param use_prediction_offset: If True, use prediction offset.
   :type use_prediction_offset: bool
   :param start_date_offset: Sets prediction start date as this offset relative to the prediction start date.
   :type start_date_offset: int
   :param forecasting_horizon: The number of timestamps to predict in the future. Range: [1, 1000].
   :type forecasting_horizon: int
   :param item_attributes_to_include_in_the_result: List of columns to include in the prediction output.
   :type item_attributes_to_include_in_the_result: list
   :param explain_predictions: If True, explain predictions for the forecast.
   :type explain_predictions: bool

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: predictions_start_date
      :type: str

      

   .. py:attribute:: use_prediction_offset
      :type: bool

      

   .. py:attribute:: start_date_offset
      :type: int

      

   .. py:attribute:: forecasting_horizon
      :type: int

      

   .. py:attribute:: item_attributes_to_include_in_the_result
      :type: list

      

   .. py:attribute:: explain_predictions
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: NamedEntityExtractionBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the NAMED_ENTITY_EXTRACTION problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param verbose_predictions: Return prediction inputs, predicted annotations and token label probabilities.
   :type verbose_predictions: bool

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: verbose_predictions
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: PersonalizationBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the PERSONALIZATION problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param number_of_items: Number of items to recommend.
   :type number_of_items: int
   :param result_columns: List of columns to include in the prediction output.
   :type result_columns: list
   :param score_field: If specified, relative item scores will be returned using a field with this name
   :type score_field: str

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: number_of_items
      :type: int

      

   .. py:attribute:: item_attributes_to_include_in_the_result
      :type: list

      

   .. py:attribute:: score_field
      :type: str

      

   .. py:method:: __post_init__()



.. py:class:: PredictiveModelingBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the PREDICTIVE_MODELING problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param explainer_type: The type of explainer to use to generate explanations on the batch prediction.
   :type explainer_type: enums.ExplainerType
   :param number_of_samples_to_use_for_explainer: Number Of Samples To Use For Kernel Explainer.
   :type number_of_samples_to_use_for_explainer: int
   :param include_multi_class_explanations: If True, Includes explanations for all classes in multi-class classification.
   :type include_multi_class_explanations: bool
   :param features_considered_constant_for_explanations: Comma separate list of fields to treat as constant in SHAP explanations.
   :type features_considered_constant_for_explanations: str
   :param importance_of_records_in_nested_columns: Returns importance of each index in the specified nested column instead of SHAP column explanations.
   :type importance_of_records_in_nested_columns: str
   :param explanation_filter_lower_bound: If set explanations will be limited to predictions above this value, Range: [0, 1].
   :type explanation_filter_lower_bound: float
   :param explanation_filter_upper_bound: If set explanations will be limited to predictions below this value, Range: [0, 1].
   :type explanation_filter_upper_bound: float
   :param bound_label: For classification problems specifies the label to which the explanation bounds are applied.
   :type bound_label: str
   :param output_columns: A list of column names to include in the prediction result.
   :type output_columns: list

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: explainer_type
      :type: abacusai.api_class.enums.ExplainerType

      

   .. py:attribute:: number_of_samples_to_use_for_explainer
      :type: int

      

   .. py:attribute:: include_multi_class_explanations
      :type: bool

      

   .. py:attribute:: features_considered_constant_for_explanations
      :type: str

      

   .. py:attribute:: importance_of_records_in_nested_columns
      :type: str

      

   .. py:attribute:: explanation_filter_lower_bound
      :type: float

      

   .. py:attribute:: explanation_filter_upper_bound
      :type: float

      

   .. py:attribute:: explanation_filter_label
      :type: str

      

   .. py:attribute:: output_columns
      :type: list

      

   .. py:method:: __post_init__()



.. py:class:: PretrainedModelsBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the PRETRAINED_MODELS problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param files_output_location_prefix: The output location prefix for the files.
   :type files_output_location_prefix: str
   :param channel_id_to_label_map: JSON string for the map from channel ids to their labels.
   :type channel_id_to_label_map: str

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: files_output_location_prefix
      :type: str

      

   .. py:attribute:: channel_id_to_label_map
      :type: str

      

   .. py:method:: __post_init__()



.. py:class:: SentenceBoundaryDetectionBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the SENTENCE_BOUNDARY_DETECTION problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation
   :type for_eval: bool
   :param explode_output: Explode data so there is one sentence per row.
   :type explode_output: bool

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: explode_output
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: ThemeAnalysisBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the THEME_ANALYSIS problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool
   :param analysis_frequency: The length of each analysis interval.
   :type analysis_frequency: str
   :param start_date: The end point for predictions.
   :type start_date: str
   :param analysis_days: How many days to analyze.
   :type analysis_days: int

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:attribute:: analysis_frequency
      :type: str

      

   .. py:attribute:: start_date
      :type: str

      

   .. py:attribute:: analysis_days
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: ChatLLMBatchPredictionArgs


   Bases: :py:obj:`BatchPredictionArgs`

   Batch Prediction Config for the ChatLLM problem type

   :param for_eval: If True, the test fold which was created during training and used for metrics calculation will be used as input data. These predictions are hence, used for model evaluation.
   :type for_eval: bool

   .. py:attribute:: for_eval
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: _BatchPredictionArgsFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key
      :value: 'problemType'

      

   .. py:attribute:: config_class_map

      


.. py:class:: BlobInput


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Binary large object input data.

   :param filename: The original filename of the blob.
   :type filename: str
   :param contents: The binary contents of the blob.
   :type contents: bytes
   :param mime_type: The mime type of the blob.
   :type mime_type: str
   :param size: The size of the blob in bytes.
   :type size: int

   .. py:attribute:: filename
      :type: str

      

   .. py:attribute:: contents
      :type: bytes

      

   .. py:attribute:: mime_type
      :type: str

      

   .. py:attribute:: size
      :type: int

      

   .. py:method:: from_local_file(file_path)
      :classmethod:


   .. py:method:: from_contents(contents, filename = None, mime_type = None)
      :classmethod:



.. py:class:: OcrMode


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: DEFAULT
      :value: 'DEFAULT'

      

   .. py:attribute:: LAYOUT
      :value: 'LAYOUT'

      

   .. py:attribute:: SCANNED
      :value: 'SCANNED'

      

   .. py:attribute:: COMPREHENSIVE
      :value: 'COMPREHENSIVE'

      

   .. py:attribute:: COMPREHENSIVE_V2
      :value: 'COMPREHENSIVE_V2'

      

   .. py:attribute:: COMPREHENSIVE_TABLE_MD
      :value: 'COMPREHENSIVE_TABLE_MD'

      


.. py:class:: ParsingConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: escape
      :type: str

      

   .. py:attribute:: csv_delimiter
      :type: str

      

   .. py:attribute:: file_path_with_schema
      :type: str

      


.. py:class:: DocumentProcessingConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Document processing configuration.

   :param extract_bounding_boxes: Whether to perform OCR and extract bounding boxes. If False, no OCR will be done but only the embedded text from digital documents will be extracted. Defaults to False.
   :type extract_bounding_boxes: bool
   :param ocr_mode: OCR mode. There are different OCR modes available for different kinds of documents and use cases. This option only takes effect when extract_bounding_boxes is True.
   :type ocr_mode: OcrMode
   :param use_full_ocr: Whether to perform full OCR. If True, OCR will be performed on the full page. If False, OCR will be performed on the non-text regions only. By default, it will be decided automatically based on the OCR mode and the document type. This option only takes effect when extract_bounding_boxes is True.
   :type use_full_ocr: bool
   :param remove_header_footer: Whether to remove headers and footers. Defaults to False. This option only takes effect when extract_bounding_boxes is True.
   :type remove_header_footer: bool
   :param remove_watermarks: Whether to remove watermarks. By default, it will be decided automatically based on the OCR mode and the document type. This option only takes effect when extract_bounding_boxes is True.
   :type remove_watermarks: bool
   :param convert_to_markdown: Whether to convert extracted text to markdown. Defaults to False. This option only takes effect when extract_bounding_boxes is True.
   :type convert_to_markdown: bool

   .. py:attribute:: extract_bounding_boxes
      :type: bool
      :value: False

      

   .. py:attribute:: ocr_mode
      :type: abacusai.api_class.enums.OcrMode

      

   .. py:attribute:: use_full_ocr
      :type: bool

      

   .. py:attribute:: remove_header_footer
      :type: bool
      :value: False

      

   .. py:attribute:: remove_watermarks
      :type: bool

      

   .. py:attribute:: convert_to_markdown
      :type: bool
      :value: False

      


.. py:class:: IncrementalDatabaseConnectorConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Config information for incremental datasets from database connectors

   :param timestamp_column: If dataset is incremental, this is the column name of the required column in the dataset. This column must contain timestamps in descending order which are used to determine the increments of the incremental dataset.
   :type timestamp_column: str

   .. py:attribute:: timestamp_column
      :type: str

      


.. py:class:: DatasetConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: application_connector_type
      :type: abacusai.api_class.enums.ApplicationConnectorType

      

   .. py:method:: _get_builder()
      :classmethod:



.. py:class:: ConfluenceDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for Confluence Application Connector
   :param pull_attachments: Whether to pull attachments for each page
   :type pull_attachments: bool, optional
   :param space_key: The space key to fetch pages from
   :type space_key: str, optional

   .. py:attribute:: pull_attachments
      :type: bool

      

   .. py:attribute:: space_key
      :type: str

      

   .. py:method:: __post_init__()



.. py:class:: GoogleAnalyticsDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for Google Analytics Application Connector

   :param location: The view id of the report in the connector to fetch
   :type location: str
   :param start_timestamp: Unix timestamp of the start of the period that will be queried
   :type start_timestamp: int, optional
   :param end_timestamp: Unix timestamp of the end of the period that will be queried
   :type end_timestamp: int, optional

   .. py:attribute:: location
      :type: str

      

   .. py:attribute:: start_timestamp
      :type: int

      

   .. py:attribute:: end_timestamp
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: GoogleDriveDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for Google Drive Application Connector

   :param location: The regex location of the files to fetch
   :type location: str
   :param is_documentset: Whether the dataset is a document set
   :type is_documentset: bool
   :param csv_delimiter: If the file format is CSV, use a specific csv delimiter
   :type csv_delimiter: str, optional
   :param extract_bounding_boxes: Signifies whether to extract bounding boxes out of the documents. Only valid if is_documentset if True
   :type extract_bounding_boxes: bool, optional
   :param merge_file_schemas: Signifies if the merge file schema policy is enabled. Not applicable if is_documentset is True
   :type merge_file_schemas: bool, optional

   .. py:attribute:: location
      :type: str

      

   .. py:attribute:: is_documentset
      :type: bool

      

   .. py:attribute:: csv_delimiter
      :type: str

      

   .. py:attribute:: extract_bounding_boxes
      :type: bool

      

   .. py:attribute:: merge_file_schemas
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: JiraDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for Jira Application Connector

   :param jql: The JQL query for fetching issues
   :type jql: str
   :param custom_fields: A list of custom fields to include in the dataset
   :type custom_fields: list, optional
   :param include_comments: Fetch comments for each issue
   :type include_comments: bool, optional
   :param include_watchers: Fetch watchers for each issue
   :type include_watchers: bool, optional

   .. py:attribute:: jql
      :type: str

      

   .. py:attribute:: custom_fields
      :type: list

      

   .. py:attribute:: include_comments
      :type: bool

      

   .. py:attribute:: include_watchers
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: OneDriveDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for OneDrive Application Connector

   :param location: The regex location of the files to fetch
   :type location: str
   :param is_documentset: Whether the dataset is a document set
   :type is_documentset: bool
   :param csv_delimiter: If the file format is CSV, use a specific csv delimiter
   :type csv_delimiter: str, optional
   :param extract_bounding_boxes: Signifies whether to extract bounding boxes out of the documents. Only valid if is_documentset if True
   :type extract_bounding_boxes: bool, optional
   :param merge_file_schemas: Signifies if the merge file schema policy is enabled. Not applicable if is_documentset is True
   :type merge_file_schemas: bool, optional

   .. py:attribute:: location
      :type: str

      

   .. py:attribute:: is_documentset
      :type: bool

      

   .. py:attribute:: csv_delimiter
      :type: str

      

   .. py:attribute:: extract_bounding_boxes
      :type: bool

      

   .. py:attribute:: merge_file_schemas
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: SharepointDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for Sharepoint Application Connector

   :param location: The regex location of the files to fetch
   :type location: str
   :param is_documentset: Whether the dataset is a document set
   :type is_documentset: bool
   :param csv_delimiter: If the file format is CSV, use a specific csv delimiter
   :type csv_delimiter: str, optional
   :param extract_bounding_boxes: Signifies whether to extract bounding boxes out of the documents. Only valid if is_documentset if True
   :type extract_bounding_boxes: bool, optional
   :param merge_file_schemas: Signifies if the merge file schema policy is enabled. Not applicable if is_documentset is True
   :type merge_file_schemas: bool, optional

   .. py:attribute:: location
      :type: str

      

   .. py:attribute:: is_documentset
      :type: bool

      

   .. py:attribute:: csv_delimiter
      :type: str

      

   .. py:attribute:: extract_bounding_boxes
      :type: bool

      

   .. py:attribute:: merge_file_schemas
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: ZendeskDatasetConfig


   Bases: :py:obj:`DatasetConfig`

   Dataset config for Zendesk Application Connector

   .. py:method:: __post_init__()



.. py:class:: _DatasetConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key
      :value: 'applicationConnectorType'

      

   .. py:attribute:: config_class_map

      


.. py:class:: VectorStoreTextEncoder


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: E5
      :value: 'E5'

      

   .. py:attribute:: OPENAI
      :value: 'OPENAI'

      

   .. py:attribute:: SENTENCE_BERT
      :value: 'SENTENCE_BERT'

      

   .. py:attribute:: E5_SMALL
      :value: 'E5_SMALL'

      


.. py:class:: VectorStoreConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Configs for vector store indexing.

   :param chunk_size: The size of text chunks in the vector store.
   :type chunk_size: int
   :param chunk_overlap_fraction: The fraction of overlap between chunks.
   :type chunk_overlap_fraction: float
   :param text_encoder: Encoder used to index texts from the documents.
   :type text_encoder: VectorStoreTextEncoder

   .. py:attribute:: chunk_size
      :type: int

      

   .. py:attribute:: chunk_overlap_fraction
      :type: float

      

   .. py:attribute:: text_encoder
      :type: abacusai.api_class.enums.VectorStoreTextEncoder

      


.. py:class:: DocumentRetrieverConfig


   Bases: :py:obj:`VectorStoreConfig`

   Configs for document retriever.


.. py:class:: ApiEnum


   Bases: :py:obj:`enum.Enum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:method:: __eq__(other)

      Return self==value.


   .. py:method:: __hash__()

      Return hash(self).



.. py:class:: ProblemType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AI_AGENT
      :value: 'ai_agent'

      

   .. py:attribute:: ANOMALY_DETECTION
      :value: 'anomaly_new'

      

   .. py:attribute:: ANOMALY_OUTLIERS
      :value: 'anomaly'

      

   .. py:attribute:: EVENT_ANOMALY
      :value: 'event_anomaly'

      

   .. py:attribute:: CLUSTERING
      :value: 'clustering'

      

   .. py:attribute:: CLUSTERING_TIMESERIES
      :value: 'clustering_timeseries'

      

   .. py:attribute:: CUMULATIVE_FORECASTING
      :value: 'cumulative_forecasting'

      

   .. py:attribute:: NAMED_ENTITY_EXTRACTION
      :value: 'nlp_ner'

      

   .. py:attribute:: NATURAL_LANGUAGE_SEARCH
      :value: 'nlp_search'

      

   .. py:attribute:: CHAT_LLM
      :value: 'chat_llm'

      

   .. py:attribute:: SENTENCE_BOUNDARY_DETECTION
      :value: 'nlp_sentence_boundary_detection'

      

   .. py:attribute:: SENTIMENT_DETECTION
      :value: 'nlp_sentiment'

      

   .. py:attribute:: DOCUMENT_CLASSIFICATION
      :value: 'nlp_classification'

      

   .. py:attribute:: DOCUMENT_SUMMARIZATION
      :value: 'nlp_summarization'

      

   .. py:attribute:: DOCUMENT_VISUALIZATION
      :value: 'nlp_document_visualization'

      

   .. py:attribute:: PERSONALIZATION
      :value: 'personalization'

      

   .. py:attribute:: PREDICTIVE_MODELING
      :value: 'regression'

      

   .. py:attribute:: FINETUNED_LLM
      :value: 'finetuned_llm'

      

   .. py:attribute:: FORECASTING
      :value: 'forecasting'

      

   .. py:attribute:: CUSTOM_TRAINED_MODEL
      :value: 'plug_and_play'

      

   .. py:attribute:: CUSTOM_ALGORITHM
      :value: 'trainable_plug_and_play'

      

   .. py:attribute:: FEATURE_STORE
      :value: 'feature_store'

      

   .. py:attribute:: IMAGE_CLASSIFICATION
      :value: 'vision_classification'

      

   .. py:attribute:: OBJECT_DETECTION
      :value: 'vision_object_detection'

      

   .. py:attribute:: IMAGE_VALUE_PREDICTION
      :value: 'vision_regression'

      

   .. py:attribute:: MODEL_MONITORING
      :value: 'model_monitoring'

      

   .. py:attribute:: LANGUAGE_DETECTION
      :value: 'language_detection'

      

   .. py:attribute:: OPTIMIZATION
      :value: 'optimization'

      

   .. py:attribute:: PRETRAINED_MODELS
      :value: 'pretrained'

      

   .. py:attribute:: THEME_ANALYSIS
      :value: 'theme_analysis'

      


.. py:class:: RegressionObjective


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AUC
      :value: 'auc'

      

   .. py:attribute:: ACCURACY
      :value: 'acc'

      

   .. py:attribute:: LOG_LOSS
      :value: 'log_loss'

      

   .. py:attribute:: PRECISION
      :value: 'precision'

      

   .. py:attribute:: RECALL
      :value: 'recall'

      

   .. py:attribute:: F1_SCORE
      :value: 'fscore'

      

   .. py:attribute:: MAE
      :value: 'mae'

      

   .. py:attribute:: MAPE
      :value: 'mape'

      

   .. py:attribute:: WAPE
      :value: 'wape'

      

   .. py:attribute:: RMSE
      :value: 'rmse'

      

   .. py:attribute:: R_SQUARED_COEFFICIENT_OF_DETERMINATION
      :value: 'r^2'

      


.. py:class:: RegressionTreeHPOMode


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: RAPID
      :value: 'rapid'

      

   .. py:attribute:: THOROUGH
      :value: 'thorough'

      


.. py:class:: PartialDependenceAnalysis


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: RAPID
      :value: 'rapid'

      

   .. py:attribute:: THOROUGH
      :value: 'thorough'

      


.. py:class:: RegressionAugmentationStrategy


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: SMOTE
      :value: 'smote'

      

   .. py:attribute:: RESAMPLE
      :value: 'resample'

      


.. py:class:: RegressionTargetTransform


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: LOG
      :value: 'log'

      

   .. py:attribute:: QUANTILE
      :value: 'quantile'

      

   .. py:attribute:: YEO_JOHNSON
      :value: 'yeo-johnson'

      

   .. py:attribute:: BOX_COX
      :value: 'box-cox'

      


.. py:class:: RegressionTypeOfSplit


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: RANDOM
      :value: 'Random Sampling'

      

   .. py:attribute:: TIMESTAMP_BASED
      :value: 'Timestamp Based'

      

   .. py:attribute:: ROW_INDICATOR_BASED
      :value: 'Row Indicator Based'

      


.. py:class:: RegressionTimeSplitMethod


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: TEST_SPLIT_PERCENTAGE_BASED
      :value: 'Test Split Percentage Based'

      

   .. py:attribute:: TEST_START_TIMESTAMP_BASED
      :value: 'Test Start Timestamp Based'

      


.. py:class:: RegressionLossFunction


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: HUBER
      :value: 'Huber'

      

   .. py:attribute:: MSE
      :value: 'Mean Squared Error'

      

   .. py:attribute:: MAE
      :value: 'Mean Absolute Error'

      

   .. py:attribute:: MAPE
      :value: 'Mean Absolute Percentage Error'

      

   .. py:attribute:: MSLE
      :value: 'Mean Squared Logarithmic Error'

      

   .. py:attribute:: TWEEDIE
      :value: 'Tweedie'

      

   .. py:attribute:: CROSS_ENTROPY
      :value: 'Cross Entropy'

      

   .. py:attribute:: FOCAL_CROSS_ENTROPY
      :value: 'Focal Cross Entropy'

      

   .. py:attribute:: AUTOMATIC
      :value: 'Automatic'

      

   .. py:attribute:: CUSTOM
      :value: 'Custom'

      


.. py:class:: ExplainerType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: KERNEL_EXPLAINER
      :value: 'KERNEL_EXPLAINER'

      

   .. py:attribute:: LIME_EXPLAINER
      :value: 'LIME_EXPLAINER'

      

   .. py:attribute:: TREE_EXPLAINER
      :value: 'TREE_EXPLAINER'

      

   .. py:attribute:: EBM_EXPLAINER
      :value: 'EBM_EXPLAINER'

      


.. py:class:: SamplingMethodType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: N_SAMPLING
      :value: 'N_SAMPLING'

      

   .. py:attribute:: PERCENT_SAMPLING
      :value: 'PERCENT_SAMPLING'

      


.. py:class:: MergeMode


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: LAST_N
      :value: 'LAST_N'

      

   .. py:attribute:: TIME_WINDOW
      :value: 'TIME_WINDOW'

      


.. py:class:: FillLogic


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AVERAGE
      :value: 'average'

      

   .. py:attribute:: MAX
      :value: 'max'

      

   .. py:attribute:: MEDIAN
      :value: 'median'

      

   .. py:attribute:: MIN
      :value: 'min'

      

   .. py:attribute:: CUSTOM
      :value: 'custom'

      

   .. py:attribute:: BACKFILL
      :value: 'bfill'

      

   .. py:attribute:: FORWARDFILL
      :value: 'ffill'

      

   .. py:attribute:: LINEAR
      :value: 'linear'

      

   .. py:attribute:: NEAREST
      :value: 'nearest'

      


.. py:class:: BatchSize


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: BATCH_8
      :value: 8

      

   .. py:attribute:: BATCH_16
      :value: 16

      

   .. py:attribute:: BATCH_32
      :value: 32

      

   .. py:attribute:: BATCH_64
      :value: 64

      

   .. py:attribute:: BATCH_128
      :value: 128

      

   .. py:attribute:: BATCH_256
      :value: 256

      

   .. py:attribute:: BATCH_384
      :value: 384

      

   .. py:attribute:: BATCH_512
      :value: 512

      

   .. py:attribute:: BATCH_740
      :value: 740

      

   .. py:attribute:: BATCH_1024
      :value: 1024

      


.. py:class:: HolidayCalendars


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AU
      :value: 'AU'

      

   .. py:attribute:: UK
      :value: 'UK'

      

   .. py:attribute:: US
      :value: 'US'

      


.. py:class:: FileFormat


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AVRO
      :value: 'AVRO'

      

   .. py:attribute:: PARQUET
      :value: 'PARQUET'

      

   .. py:attribute:: TFRECORD
      :value: 'TFRECORD'

      

   .. py:attribute:: TSV
      :value: 'TSV'

      

   .. py:attribute:: CSV
      :value: 'CSV'

      

   .. py:attribute:: ORC
      :value: 'ORC'

      

   .. py:attribute:: JSON
      :value: 'JSON'

      

   .. py:attribute:: ODS
      :value: 'ODS'

      

   .. py:attribute:: XLS
      :value: 'XLS'

      

   .. py:attribute:: GZ
      :value: 'GZ'

      

   .. py:attribute:: ZIP
      :value: 'ZIP'

      

   .. py:attribute:: TAR
      :value: 'TAR'

      

   .. py:attribute:: DOCX
      :value: 'DOCX'

      

   .. py:attribute:: PDF
      :value: 'PDF'

      

   .. py:attribute:: RAR
      :value: 'RAR'

      

   .. py:attribute:: JPEG
      :value: 'JPG'

      

   .. py:attribute:: PNG
      :value: 'PNG'

      

   .. py:attribute:: TIF
      :value: 'TIFF'

      

   .. py:attribute:: NUMBERS
      :value: 'NUMBERS'

      

   .. py:attribute:: PPTX
      :value: 'PPTX'

      

   .. py:attribute:: PPT
      :value: 'PPT'

      


.. py:class:: ExperimentationMode


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: RAPID
      :value: 'rapid'

      

   .. py:attribute:: THOROUGH
      :value: 'thorough'

      


.. py:class:: PersonalizationTrainingMode


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: EXPERIMENTAL
      :value: 'EXP'

      

   .. py:attribute:: PRODUCTION
      :value: 'PROD'

      


.. py:class:: PersonalizationObjective


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: NDCG
      :value: 'ndcg'

      

   .. py:attribute:: NDCG_5
      :value: 'ndcg@5'

      

   .. py:attribute:: NDCG_10
      :value: 'ndcg@10'

      

   .. py:attribute:: MAP
      :value: 'map'

      

   .. py:attribute:: MAP_5
      :value: 'map@5'

      

   .. py:attribute:: MAP_10
      :value: 'map@10'

      

   .. py:attribute:: MRR
      :value: 'mrr'

      

   .. py:attribute:: PERSONALIZATION
      :value: 'personalization@10'

      

   .. py:attribute:: COVERAGE
      :value: 'coverage'

      


.. py:class:: ForecastingObjective


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: ACCURACY
      :value: 'w_c_accuracy'

      

   .. py:attribute:: WAPE
      :value: 'wape'

      

   .. py:attribute:: MAPE
      :value: 'mape'

      

   .. py:attribute:: CMAPE
      :value: 'cmape'

      

   .. py:attribute:: RMSE
      :value: 'rmse'

      

   .. py:attribute:: CV
      :value: 'coefficient_of_variation'

      

   .. py:attribute:: BIAS
      :value: 'bias'

      

   .. py:attribute:: SRMSE
      :value: 'srmse'

      


.. py:class:: ForecastingFrequency


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: HOURLY
      :value: '1H'

      

   .. py:attribute:: DAILY
      :value: '1D'

      

   .. py:attribute:: WEEKLY_SUNDAY_START
      :value: '1W'

      

   .. py:attribute:: WEEKLY_MONDAY_START
      :value: 'W-MON'

      

   .. py:attribute:: WEEKLY_SATURDAY_START
      :value: 'W-SAT'

      

   .. py:attribute:: MONTH_START
      :value: 'MS'

      

   .. py:attribute:: MONTH_END
      :value: '1M'

      

   .. py:attribute:: QUARTER_START
      :value: 'QS'

      

   .. py:attribute:: QUARTER_END
      :value: '1Q'

      

   .. py:attribute:: YEARLY
      :value: '1Y'

      


.. py:class:: ForecastingDataSplitType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AUTO
      :value: 'Automatic Time Based'

      

   .. py:attribute:: TIMESTAMP
      :value: 'Timestamp Based'

      

   .. py:attribute:: ITEM
      :value: 'Item Based'

      

   .. py:attribute:: PREDICTION_LENGTH
      :value: 'Force Prediction Length'

      


.. py:class:: ForecastingLossFunction


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: CUSTOM
      :value: 'Custom'

      

   .. py:attribute:: MEAN_ABSOLUTE_ERROR
      :value: 'mae'

      

   .. py:attribute:: NORMALIZED_MEAN_ABSOLUTE_ERROR
      :value: 'nmae'

      

   .. py:attribute:: PEAKS_MEAN_ABSOLUTE_ERROR
      :value: 'peaks_mae'

      

   .. py:attribute:: MEAN_ABSOLUTE_PERCENTAGE_ERROR
      :value: 'stable_mape'

      

   .. py:attribute:: POINTWISE_ACCURACY
      :value: 'accuracy'

      

   .. py:attribute:: ROOT_MEAN_SQUARE_ERROR
      :value: 'rmse'

      

   .. py:attribute:: NORMALIZED_ROOT_MEAN_SQUARE_ERROR
      :value: 'nrmse'

      

   .. py:attribute:: ASYMMETRIC_MEAN_ABSOLUTE_PERCENTAGE_ERROR
      :value: 'asymmetric_mape'

      

   .. py:attribute:: STABLE_STANDARDIZED_MEAN_ABSOLUTE_PERCENTAGE_ERROR
      :value: 'stable_standardized_mape_with_cmape'

      

   .. py:attribute:: GAUSSIAN
      :value: 'mle_gaussian_local'

      

   .. py:attribute:: GAUSSIAN_FULL_COVARIANCE
      :value: 'mle_gaussfullcov'

      

   .. py:attribute:: GUASSIAN_EXPONENTIAL
      :value: 'mle_gaussexp'

      

   .. py:attribute:: MIX_GAUSSIANS
      :value: 'mle_gaussmix'

      

   .. py:attribute:: WEIBULL
      :value: 'mle_weibull'

      

   .. py:attribute:: NEGATIVE_BINOMIAL
      :value: 'mle_negbinom'

      

   .. py:attribute:: LOG_ROOT_MEAN_SQUARE_ERROR
      :value: 'log_rmse'

      


.. py:class:: ForecastingLocalScaling


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: ZSCORE
      :value: 'zscore'

      

   .. py:attribute:: SLIDING_ZSCORE
      :value: 'sliding_zscore'

      

   .. py:attribute:: LAST_POINT
      :value: 'lastpoint'

      

   .. py:attribute:: MIN_MAX
      :value: 'minmax'

      

   .. py:attribute:: MIN_STD
      :value: 'minstd'

      

   .. py:attribute:: ROBUST
      :value: 'robust'

      

   .. py:attribute:: ITEM
      :value: 'item'

      


.. py:class:: ForecastingFillMethod


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: BACK
      :value: 'BACK'

      

   .. py:attribute:: MIDDLE
      :value: 'MIDDLE'

      

   .. py:attribute:: FUTURE
      :value: 'FUTURE'

      


.. py:class:: ForecastingQuanitlesExtensionMethod


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: DIRECT
      :value: 'direct'

      

   .. py:attribute:: QUADRATIC
      :value: 'quadratic'

      

   .. py:attribute:: ANCESTRAL_SIMULATION
      :value: 'simulation'

      


.. py:class:: NERObjective


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: LOG_LOSS
      :value: 'log_loss'

      

   .. py:attribute:: AUC
      :value: 'auc'

      

   .. py:attribute:: PRECISION
      :value: 'precision'

      

   .. py:attribute:: RECALL
      :value: 'recall'

      

   .. py:attribute:: ANNOTATIONS_PRECISION
      :value: 'annotations_precision'

      

   .. py:attribute:: ANNOTATIONS_RECALL
      :value: 'annotations_recall'

      


.. py:class:: NERModelType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: PRETRAINED_BERT
      :value: 'pretrained_bert'

      

   .. py:attribute:: PRETRAINED_ROBERTA_27
      :value: 'pretrained_roberta_27'

      

   .. py:attribute:: PRETRAINED_ROBERTA_43
      :value: 'pretrained_roberta_43'

      

   .. py:attribute:: PRETRAINED_MULTILINGUAL
      :value: 'pretrained_multilingual'

      

   .. py:attribute:: LEARNED
      :value: 'learned'

      


.. py:class:: NLPDocumentFormat


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AUTO
      :value: 'auto'

      

   .. py:attribute:: TEXT
      :value: 'text'

      

   .. py:attribute:: DOC
      :value: 'doc'

      

   .. py:attribute:: TOKENS
      :value: 'tokens'

      


.. py:class:: SentimentType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: VALENCE
      :value: 'valence'

      

   .. py:attribute:: EMOTION
      :value: 'emotion'

      


.. py:class:: ClusteringImputationMethod


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: AUTOMATIC
      :value: 'Automatic'

      

   .. py:attribute:: ZEROS
      :value: 'Zeros'

      

   .. py:attribute:: INTERPOLATE
      :value: 'Interpolate'

      


.. py:class:: ConnectorType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: FILE
      :value: 'FILE'

      

   .. py:attribute:: DATABASE
      :value: 'DATABASE'

      

   .. py:attribute:: STREAMING
      :value: 'STREAMING'

      

   .. py:attribute:: APPLICATION
      :value: 'APPLICATION'

      


.. py:class:: ApplicationConnectorType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: GOOGLEANALYTICS
      :value: 'GOOGLEANALYTICS'

      

   .. py:attribute:: GOOGLEDRIVE
      :value: 'GOOGLEDRIVE'

      

   .. py:attribute:: GIT
      :value: 'GIT'

      

   .. py:attribute:: CONFLUENCE
      :value: 'CONFLUENCE'

      

   .. py:attribute:: JIRA
      :value: 'JIRA'

      

   .. py:attribute:: ONEDRIVE
      :value: 'ONEDRIVE'

      

   .. py:attribute:: ZENDESK
      :value: 'ZENDESK'

      

   .. py:attribute:: SLACK
      :value: 'SLACK'

      

   .. py:attribute:: SHAREPOINT
      :value: 'SHAREPOINT'

      

   .. py:attribute:: TEAMS
      :value: 'TEAMS'

      


.. py:class:: PythonFunctionArgumentType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: FEATURE_GROUP
      :value: 'FEATURE_GROUP'

      

   .. py:attribute:: INTEGER
      :value: 'INTEGER'

      

   .. py:attribute:: STRING
      :value: 'STRING'

      

   .. py:attribute:: BOOLEAN
      :value: 'BOOLEAN'

      

   .. py:attribute:: FLOAT
      :value: 'FLOAT'

      

   .. py:attribute:: JSON
      :value: 'JSON'

      

   .. py:attribute:: LIST
      :value: 'LIST'

      

   .. py:attribute:: DATASET_ID
      :value: 'DATASET_ID'

      

   .. py:attribute:: MODEL_ID
      :value: 'MODEL_ID'

      

   .. py:attribute:: FEATURE_GROUP_ID
      :value: 'FEATURE_GROUP_ID'

      

   .. py:attribute:: MONITOR_ID
      :value: 'MONITOR_ID'

      

   .. py:attribute:: BATCH_PREDICTION_ID
      :value: 'BATCH_PREDICTION_ID'

      

   .. py:attribute:: DEPLOYMENT_ID
      :value: 'DEPLOYMENT_ID'

      


.. py:class:: PythonFunctionOutputArgumentType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: NTEGER
      :value: 'INTEGER'

      

   .. py:attribute:: STRING
      :value: 'STRING'

      

   .. py:attribute:: BOOLEAN
      :value: 'BOOLEAN'

      

   .. py:attribute:: FLOAT
      :value: 'FLOAT'

      

   .. py:attribute:: JSON
      :value: 'JSON'

      

   .. py:attribute:: LIST
      :value: 'LIST'

      

   .. py:attribute:: DATASET_ID
      :value: 'DATASET_ID'

      

   .. py:attribute:: MODEL_ID
      :value: 'MODEL_ID'

      

   .. py:attribute:: FEATURE_GROUP_ID
      :value: 'FEATURE_GROUP_ID'

      

   .. py:attribute:: MONITOR_ID
      :value: 'MONITOR_ID'

      

   .. py:attribute:: BATCH_PREDICTION_ID
      :value: 'BATCH_PREDICTION_ID'

      

   .. py:attribute:: DEPLOYMENT_ID
      :value: 'DEPLOYMENT_ID'

      

   .. py:attribute:: ANY
      :value: 'ANY'

      


.. py:class:: LLMName


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: OPENAI_GPT4
      :value: 'OPENAI_GPT4'

      

   .. py:attribute:: OPENAI_GPT4_32K
      :value: 'OPENAI_GPT4_32K'

      

   .. py:attribute:: OPENAI_GPT4_128K
      :value: 'OPENAI_GPT4_128K'

      

   .. py:attribute:: OPENAI_GPT3_5
      :value: 'OPENAI_GPT3_5'

      

   .. py:attribute:: OPENAI_GPT3_5_TEXT
      :value: 'OPENAI_GPT3_5_TEXT'

      

   .. py:attribute:: CLAUDE_V2_1
      :value: 'CLAUDE_V2_1'

      

   .. py:attribute:: ABACUS_GIRAFFE
      :value: 'ABACUS_GIRAFFE'

      

   .. py:attribute:: ABACUS_GIRAFFE_LARGE
      :value: 'ABACUS_GIRAFFE_LARGE'

      

   .. py:attribute:: ABACUS_LLAMA2_QA
      :value: 'ABACUS_LLAMA2_QA'

      

   .. py:attribute:: LLAMA2_CHAT
      :value: 'LLAMA2_CHAT'

      

   .. py:attribute:: PALM
      :value: 'PALM'

      

   .. py:attribute:: PALM_TEXT
      :value: 'PALM_TEXT'

      

   .. py:attribute:: GEMINI_PRO
      :value: 'GEMINI_PRO'

      

   .. py:attribute:: MIXTRAL_CHAT
      :value: 'MIXTRAL_CHAT'

      

   .. py:attribute:: MISTRAL_MEDIUM
      :value: 'MISTRAL_MEDIUM'

      


.. py:class:: MonitorAlertType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: ACCURACY_BELOW_THRESHOLD
      :value: 'AccuracyBelowThreshold'

      

   .. py:attribute:: FEATURE_DRIFT
      :value: 'FeatureDrift'

      

   .. py:attribute:: DATA_INTEGRITY_VIOLATIONS
      :value: 'DataIntegrityViolations'

      

   .. py:attribute:: BIAS_VIOLATIONS
      :value: 'BiasViolations'

      

   .. py:attribute:: HISTORY_LENGTH_DRIFT
      :value: 'HistoryLengthDrift'

      

   .. py:attribute:: TARGET_DRIFT
      :value: 'TargetDrift'

      

   .. py:attribute:: PREDICTION_COUNT
      :value: 'PredictionCount'

      


.. py:class:: FeatureDriftType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: KL
      :value: 'kl'

      

   .. py:attribute:: KS
      :value: 'ks'

      

   .. py:attribute:: WS
      :value: 'ws'

      

   .. py:attribute:: JS
      :value: 'js'

      

   .. py:attribute:: PSI
      :value: 'psi'

      

   .. py:attribute:: CHI_SQUARE
      :value: 'chi_square'

      

   .. py:attribute:: CSI
      :value: 'csi'

      


.. py:class:: DataIntegrityViolationType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: NULL_VIOLATIONS
      :value: 'null_violations'

      

   .. py:attribute:: TYPE_MISMATCH_VIOLATIONS
      :value: 'type_mismatch_violations'

      

   .. py:attribute:: RANGE_VIOLATIONS
      :value: 'range_violations'

      

   .. py:attribute:: CATEGORICAL_RANGE_VIOLATION
      :value: 'categorical_range_violations'

      

   .. py:attribute:: TOTAL_VIOLATIONS
      :value: 'total_violations'

      


.. py:class:: BiasType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: DEMOGRAPHIC_PARITY
      :value: 'demographic_parity'

      

   .. py:attribute:: EQUAL_OPPORTUNITY
      :value: 'equal_opportunity'

      

   .. py:attribute:: GROUP_BENEFIT_EQUALITY
      :value: 'group_benefit'

      

   .. py:attribute:: TOTAL
      :value: 'total'

      


.. py:class:: AlertActionType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: EMAIL
      :value: 'Email'

      


.. py:class:: PythonFunctionType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: FEATURE_GROUP
      :value: 'FEATURE_GROUP'

      

   .. py:attribute:: PLOTLY_FIG
      :value: 'PLOTLY_FIG'

      

   .. py:attribute:: STEP_FUNCTION
      :value: 'STEP_FUNCTION'

      


.. py:class:: EvalArtifactType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: FORECASTING_ACCURACY
      :value: 'bar_chart'

      

   .. py:attribute:: FORECASTING_VOLUME
      :value: 'bar_chart_volume'

      

   .. py:attribute:: FORECASTING_HISTORY_LENGTH_ACCURACY
      :value: 'bar_chart_accuracy_by_history'

      


.. py:class:: FieldDescriptorType


   Bases: :py:obj:`ApiEnum`

   Generic enumeration.

   Derive from this class to define new enumerations.

   .. py:attribute:: STRING
      :value: 'STRING'

      

   .. py:attribute:: INTEGER
      :value: 'INTEGER'

      

   .. py:attribute:: FLOAT
      :value: 'FLOAT'

      

   .. py:attribute:: BOOLEAN
      :value: 'BOOLEAN'

      

   .. py:attribute:: DATETIME
      :value: 'DATETIME'

      

   .. py:attribute:: DATE
      :value: 'DATE'

      


.. py:class:: SamplingConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   An abstract class for the sampling config of a feature group

   .. py:method:: _get_builder()
      :classmethod:


   .. py:method:: __post_init__()



.. py:class:: NSamplingConfig


   Bases: :py:obj:`SamplingConfig`

   The number of distinct values of the key columns to include in the sample, or number of rows if key columns not specified.

   :param sampling_method: N_SAMPLING
   :type sampling_method: SamplingMethodType
   :param sample_count: The number of rows to include in the sample
   :type sample_count: int
   :param key_columns: The feature(s) to use as the key(s) when sampling
   :type key_columns: List[str]

   .. py:attribute:: sample_count
      :type: int

      

   .. py:attribute:: key_columns
      :type: List[str]

      

   .. py:attribute:: sampling_method
      :type: abacusai.api_class.enums.SamplingMethodType

      


.. py:class:: PercentSamplingConfig


   Bases: :py:obj:`SamplingConfig`

   The fraction of distinct values of the feature group to include in the sample.

   :param sampling_method: PERCENT_SAMPLING
   :type sampling_method: SamplingMethodType
   :param sample_percent: The percentage of the rows to sample
   :type sample_percent: float
   :param key_columns: The feature(s) to use as the key(s) when sampling
   :type key_columns: List[str]

   .. py:attribute:: sample_percent
      :type: float

      

   .. py:attribute:: key_columns
      :type: List[str]

      

   .. py:attribute:: sampling_method
      :type: abacusai.api_class.enums.SamplingMethodType

      


.. py:class:: _SamplingConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_class_key
      :value: 'sampling_method'

      

   .. py:attribute:: config_class_map

      


.. py:class:: MergeConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   An abstract class for the merge config of a feature group

   .. py:method:: _get_builder()
      :classmethod:


   .. py:method:: __post_init__()



.. py:class:: LastNMergeConfig


   Bases: :py:obj:`MergeConfig`

   Merge LAST N chunks/versions of an incremental dataset.

   :param merge_mode: LAST_N
   :type merge_mode: MergeMode
   :param num_versions: The number of versions to merge. num_versions == 0 means merge all versions.
   :type num_versions: int

   .. py:attribute:: num_versions
      :type: int

      

   .. py:attribute:: merge_mode
      :type: abacusai.api_class.enums.MergeMode

      


.. py:class:: TimeWindowMergeConfig


   Bases: :py:obj:`MergeConfig`

   Merge rows within a given timewindow of the most recent timestamp

   :param merge_mode: TIME_WINDOW
   :type merge_mode: MergeMode
   :param feature_name: Time based column to index on
   :type feature_name: str
   :param time_window_size_ms: Range of merged rows will be [MAX_TIME - time_window_size_ms, MAX_TIME]
   :type time_window_size_ms: int

   .. py:attribute:: feature_name
      :type: str

      

   .. py:attribute:: time_window_size_ms
      :type: int

      

   .. py:attribute:: merge_mode
      :type: abacusai.api_class.enums.MergeMode

      


.. py:class:: _MergeConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_class_key
      :value: 'merge_mode'

      

   .. py:attribute:: config_class_map

      


.. py:class:: TrainingConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: _upper_snake_case_keys
      :type: bool

      

   .. py:attribute:: _support_kwargs
      :type: bool

      

   .. py:attribute:: kwargs
      :type: dict

      

   .. py:attribute:: problem_type
      :type: abacusai.api_class.enums.ProblemType

      

   .. py:attribute:: algorithm
      :type: str

      

   .. py:method:: _get_builder()
      :classmethod:



.. py:class:: PersonalizationTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the PERSONALIZATION problem type

   :param objective: Ranking scheme used to select final best model.
   :type objective: PersonalizationObjective
   :param sort_objective: Ranking scheme used to sort models on the metrics page.
   :type sort_objective: PersonalizationObjective
   :param training_mode: whether to train in production or experimental mode. Defaults to EXP.
   :type training_mode: PersonalizationTrainingMode
   :param target_action_types: List of action types to use as targets for training.
   :type target_action_types: List[str]
   :param target_action_weights: Dictionary of action types to weights for training.
   :type target_action_weights: Dict[str, float]
   :param session_event_types: List of event types to treat as occurrences of sessions.
   :type session_event_types: List[str]
   :param test_split: Percent of dataset to use for test data. We support using a range between 6% to 20% of your dataset to use as test data.
   :type test_split: int
   :param recent_days_for_training: Limit training data to a certain latest number of days.
   :type recent_days_for_training: int
   :param training_start_date: Only consider training interaction data after this date. Specified in the timezone of the dataset.
   :type training_start_date: str
   :param test_on_user_split: Use user splits instead of using time splits, when validating and testing the model.
   :type test_on_user_split: bool
   :param test_split_on_last_k_items: Use last k items instead of global timestamp splits, when validating and testing the model.
   :type test_split_on_last_k_items: bool
   :param test_last_items_length: Number of items to leave out for each user when using leave k out folds.
   :type test_last_items_length: int
   :param test_window_length_hours: Duration (in hours) of most recent time window to use when validating and testing the model.
   :type test_window_length_hours: int
   :param explicit_time_split: Sets an explicit time-based test boundary.
   :type explicit_time_split: bool
   :param test_row_indicator: Column indicating which rows to use for training (TRAIN), validation (VAL) and testing (TEST).
   :type test_row_indicator: str
   :param full_data_retraining: Train models separately with all the data.
   :type full_data_retraining: bool
   :param sequential_training: Train a mode sequentially through time.
   :type sequential_training: bool
   :param data_split_feature_group_table_name: Specify the table name of the feature group to export training data with the fold column.
   :type data_split_feature_group_table_name: str
   :param optimized_event_type: The final event type to optimize for and compute metrics on.
   :type optimized_event_type: str
   :param dropout_rate: Dropout rate for neural network.
   :type dropout_rate: int
   :param batch_size: Batch size for neural network.
   :type batch_size: BatchSize
   :param disable_transformer: Disable training the transformer algorithm.
   :type disable_transformer: bool
   :param disable_gpu: Disable training on GPU.
   :type disable_gpu: boo
   :param filter_history: Do not recommend items the user has already interacted with.
   :type filter_history: bool
   :param action_types_exclusion_days: Mapping from action type to number of days for which we exclude previously interacted items from prediction
   :type action_types_exclusion_days: Dict[str, float]
   :param session_dedupe_mins: Minimum number of minutes between two sessions for a user.
   :type session_dedupe_mins: float
   :param max_history_length: Maximum length of user-item history to include user in training examples.
   :type max_history_length: int
   :param compute_rerank_metrics: Compute metrics based on rerank results.
   :type compute_rerank_metrics: bool
   :param add_time_features: Include interaction time as a feature.
   :type add_time_features: bool
   :param disable_timestamp_scalar_features: Exclude timestamp scalar features.
   :type disable_timestamp_scalar_features: bool
   :param compute_session_metrics: Evaluate models based on how well they are able to predict the next session of interactions.
   :type compute_session_metrics: bool
   :param max_user_history_len_percentile: Filter out users with history length above this percentile.
   :type max_user_history_len_percentile: int
   :param downsample_item_popularity_percentile: Downsample items more popular than this percentile.
   :type downsample_item_popularity_percentile: float
   :param use_user_id_feature: Use user id as a feature in CTR models.
   :type use_user_id_feature: bool

   .. py:attribute:: objective
      :type: abacusai.api_class.enums.PersonalizationObjective

      

   .. py:attribute:: sort_objective
      :type: abacusai.api_class.enums.PersonalizationObjective

      

   .. py:attribute:: training_mode
      :type: abacusai.api_class.enums.PersonalizationTrainingMode

      

   .. py:attribute:: target_action_types
      :type: List[str]

      

   .. py:attribute:: target_action_weights
      :type: Dict[str, float]

      

   .. py:attribute:: session_event_types
      :type: List[str]

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: recent_days_for_training
      :type: int

      

   .. py:attribute:: training_start_date
      :type: str

      

   .. py:attribute:: test_on_user_split
      :type: bool

      

   .. py:attribute:: test_split_on_last_k_items
      :type: bool

      

   .. py:attribute:: test_last_items_length
      :type: int

      

   .. py:attribute:: test_window_length_hours
      :type: int

      

   .. py:attribute:: explicit_time_split
      :type: bool

      

   .. py:attribute:: test_row_indicator
      :type: str

      

   .. py:attribute:: full_data_retraining
      :type: bool

      

   .. py:attribute:: sequential_training
      :type: bool

      

   .. py:attribute:: data_split_feature_group_table_name
      :type: str

      

   .. py:attribute:: optimized_event_type
      :type: str

      

   .. py:attribute:: dropout_rate
      :type: int

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:attribute:: disable_transformer
      :type: bool

      

   .. py:attribute:: disable_gpu
      :type: bool

      

   .. py:attribute:: filter_history
      :type: bool

      

   .. py:attribute:: action_types_exclusion_days
      :type: Dict[str, float]

      

   .. py:attribute:: max_history_length
      :type: int

      

   .. py:attribute:: compute_rerank_metrics
      :type: bool

      

   .. py:attribute:: add_time_features
      :type: bool

      

   .. py:attribute:: disable_timestamp_scalar_features
      :type: bool

      

   .. py:attribute:: compute_session_metrics
      :type: bool

      

   .. py:attribute:: query_column
      :type: str

      

   .. py:attribute:: item_query_column
      :type: str

      

   .. py:attribute:: use_user_id_feature
      :type: bool

      

   .. py:attribute:: session_dedupe_mins
      :type: float

      

   .. py:attribute:: max_user_history_len_percentile
      :type: int

      

   .. py:attribute:: downsample_item_popularity_percentile
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: RegressionTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the PREDICTIVE_MODELING problem type

   :param objective: Ranking scheme used to select final best model.
   :type objective: RegressionObjective
   :param sort_objective: Ranking scheme used to sort models on the metrics page.
   :type sort_objective: RegressionObjective
   :param tree_hpo_mode: (RegressionTreeHPOMode): Turning off Rapid Experimentation will take longer to train.
   :param type_of_split: Type of data splitting into train/test (validation also).
   :type type_of_split: RegressionTypeOfSplit
   :param test_split: Percent of dataset to use for test data. We support using a range between 5% to 20% of your dataset to use as test data.
   :type test_split: int
   :param disable_test_val_fold: Do not create a TEST_VAL set. All records which would be part of the TEST_VAL fold otherwise, remain in the TEST fold.
   :type disable_test_val_fold: bool
   :param k_fold_cross_validation: Use this to force k-fold cross validation bagging on or off.
   :type k_fold_cross_validation: bool
   :param num_cv_folds: Specify the value of k in k-fold cross validation.
   :type num_cv_folds: int
   :param timestamp_based_splitting_column: Timestamp column selected for splitting into test and train.
   :type timestamp_based_splitting_column: str
   :param timestamp_based_splitting_method: Method of selecting TEST set, top percentile wise or after a given timestamp.
   :type timestamp_based_splitting_method: RegressionTimeSplitMethod
   :param test_splitting_timestamp: Rows with timestamp greater than this will be considered to be in the test set.
   :type test_splitting_timestamp: str
   :param sampling_unit_keys: Constrain train/test separation to partition a column.
   :type sampling_unit_keys: List[str]
   :param test_row_indicator: Column indicating which rows to use for training (TRAIN) and testing (TEST). Validation (VAL) can also be specified.
   :type test_row_indicator: str
   :param full_data_retraining: Train models separately with all the data.
   :type full_data_retraining: bool
   :param rebalance_classes: Class weights are computed as the inverse of the class frequency from the training dataset when this option is selected as "Yes". It is useful when the classes in the dataset are unbalanced.
                             Re-balancing classes generally boosts recall at the cost of precision on rare classes.
   :type rebalance_classes: bool
   :param rare_class_augmentation_threshold: Augments any rare class whose relative frequency with respect to the most frequent class is less than this threshold. Default = 0.1 for classification problems with rare classes.
   :type rare_class_augmentation_threshold: float
   :param augmentation_strategy: Strategy to deal with class imbalance and data augmentation.
   :type augmentation_strategy: RegressionAugmentationStrategy
   :param training_rows_downsample_ratio: Uses this ratio to train on a sample of the dataset provided.
   :type training_rows_downsample_ratio: float
   :param active_labels_column: Specify a column to use as the active columns in a multi label setting.
   :type active_labels_column: str
   :param min_categorical_count: Minimum threshold to consider a value different from the unknown placeholder.
   :type min_categorical_count: int
   :param sample_weight: Specify a column to use as the weight of a sample for training and eval.
   :type sample_weight: str
   :param numeric_clipping_percentile: Uses this option to clip the top and bottom x percentile of numeric feature columns where x is the value of this option.
   :type numeric_clipping_percentile: float
   :param target_transform: Specify a transform (e.g. log, quantile) to apply to the target variable.
   :type target_transform: RegressionTargetTransform
   :param ignore_datetime_features: Remove all datetime features from the model. Useful while generalizing to different time periods.
   :type ignore_datetime_features: bool
   :param max_text_words: Maximum number of words to use from text fields.
   :type max_text_words: int
   :param perform_feature_selection: If enabled, additional algorithms which support feature selection as a pretraining step will be trained separately with the selected subset of features. The details about their selected features can be found in their respective logs.
   :type perform_feature_selection: bool
   :param feature_selection_intensity: This determines the strictness with which features will be filtered out. 1 being very lenient (more features kept), 100 being very strict.
   :type feature_selection_intensity: int
   :param batch_size: Batch size.
   :type batch_size: BatchSize
   :param dropout_rate: Dropout percentage rate.
   :type dropout_rate: int
   :param pretrained_model_name: Enable algorithms which process text using pretrained multilingual NLP models.
   :type pretrained_model_name: str
   :param is_multilingual: Enable algorithms which process text using pretrained multilingual NLP models.
   :type is_multilingual: bool
   :param loss_function: Loss function to be used as objective for model training.
   :type loss_function: RegressionLossFunction
   :param loss_parameters: Loss function params in format <key>=<value>;<key>=<value>;.....
   :type loss_parameters: str
   :param target_encode_categoricals: Use this to turn target encoding on categorical features on or off.
   :type target_encode_categoricals: bool
   :param drop_original_categoricals: This option helps us choose whether to also feed the original label encoded categorical columns to the mdoels along with their target encoded versions.
   :type drop_original_categoricals: bool
   :param monotonically_increasing_features: Constrain the model such that it behaves as if the target feature is monotonically increasing with the selected features
   :type monotonically_increasing_features: List[str]
   :param monotonically_decreasing_features: Constrain the model such that it behaves as if the target feature is monotonically decreasing with the selected features
   :type monotonically_decreasing_features: List[str]
   :param data_split_feature_group_table_name: Specify the table name of the feature group to export training data with the fold column.
   :type data_split_feature_group_table_name: str
   :param custom_loss_functions: Registered custom losses available for selection.
   :type custom_loss_functions: List[str]
   :param custom_metrics: Registered custom metrics available for selection.
   :type custom_metrics: List[str]
   :param partial_dependence_analysis: Specify whether to run partial dependence plots for all features or only some features.
   :type partial_dependence_analysis: PartialDependenceAnalysis

   .. py:attribute:: objective
      :type: abacusai.api_class.enums.RegressionObjective

      

   .. py:attribute:: sort_objective
      :type: abacusai.api_class.enums.RegressionObjective

      

   .. py:attribute:: tree_hpo_mode
      :type: abacusai.api_class.enums.RegressionTreeHPOMode

      

   .. py:attribute:: partial_dependence_analysis
      :type: abacusai.api_class.enums.PartialDependenceAnalysis

      

   .. py:attribute:: type_of_split
      :type: abacusai.api_class.enums.RegressionTypeOfSplit

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: disable_test_val_fold
      :type: bool

      

   .. py:attribute:: k_fold_cross_validation
      :type: bool

      

   .. py:attribute:: num_cv_folds
      :type: int

      

   .. py:attribute:: timestamp_based_splitting_column
      :type: str

      

   .. py:attribute:: timestamp_based_splitting_method
      :type: abacusai.api_class.enums.RegressionTimeSplitMethod

      

   .. py:attribute:: test_splitting_timestamp
      :type: str

      

   .. py:attribute:: sampling_unit_keys
      :type: List[str]

      

   .. py:attribute:: test_row_indicator
      :type: str

      

   .. py:attribute:: full_data_retraining
      :type: bool

      

   .. py:attribute:: rebalance_classes
      :type: bool

      

   .. py:attribute:: rare_class_augmentation_threshold
      :type: float

      

   .. py:attribute:: augmentation_strategy
      :type: abacusai.api_class.enums.RegressionAugmentationStrategy

      

   .. py:attribute:: training_rows_downsample_ratio
      :type: float

      

   .. py:attribute:: active_labels_column
      :type: str

      

   .. py:attribute:: min_categorical_count
      :type: int

      

   .. py:attribute:: sample_weight
      :type: str

      

   .. py:attribute:: numeric_clipping_percentile
      :type: float

      

   .. py:attribute:: target_transform
      :type: abacusai.api_class.enums.RegressionTargetTransform

      

   .. py:attribute:: ignore_datetime_features
      :type: bool

      

   .. py:attribute:: max_text_words
      :type: int

      

   .. py:attribute:: perform_feature_selection
      :type: bool

      

   .. py:attribute:: feature_selection_intensity
      :type: int

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:attribute:: dropout_rate
      :type: int

      

   .. py:attribute:: pretrained_model_name
      :type: str

      

   .. py:attribute:: is_multilingual
      :type: bool

      

   .. py:attribute:: loss_function
      :type: abacusai.api_class.enums.RegressionLossFunction

      

   .. py:attribute:: loss_parameters
      :type: str

      

   .. py:attribute:: target_encode_categoricals
      :type: bool

      

   .. py:attribute:: drop_original_categoricals
      :type: bool

      

   .. py:attribute:: monotonically_increasing_features
      :type: List[str]

      

   .. py:attribute:: monotonically_decreasing_features
      :type: List[str]

      

   .. py:attribute:: data_split_feature_group_table_name
      :type: str

      

   .. py:attribute:: custom_loss_functions
      :type: List[str]

      

   .. py:attribute:: custom_metrics
      :type: List[str]

      

   .. py:method:: __post_init__()



.. py:class:: ForecastingTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the FORECASTING problem type

   :param prediction_length: How many timesteps in the future to predict.
   :type prediction_length: int
   :param objective: Ranking scheme used to select final best model.
   :type objective: ForecastingObjective
   :param sort_objective: Ranking scheme used to sort models on the metrics page.
   :type sort_objective: ForecastingObjective
   :param forecast_frequency: Forecast frequency.
   :type forecast_frequency: ForecastingFrequency
   :param probability_quantiles: Prediction quantiles.
   :type probability_quantiles: List[float]
   :param force_prediction_length: Force length of test window to be the same as prediction length.
   :type force_prediction_length: int
   :param filter_items: Filter items with small history and volume.
   :type filter_items: bool
   :param enable_feature_selection: Enable feature selection.
   :type enable_feature_selection: bool
   :param enable_padding: Pad series to the max_date of the dataset
   :type enable_padding: bool
   :param enable_cold_start: Enable cold start forecasting by training/predicting for zero history items.
   :type enable_cold_start: bool
   :param enable_multiple_backtests: Whether to enable multiple backtesting or not.
   :type enable_multiple_backtests: bool
   :param num_backtesting_windows: Total backtesting windows to use for the training.
   :type num_backtesting_windows: int
   :param backtesting_window_step_size: Use this step size to shift backtesting windows for model training.
   :type backtesting_window_step_size: int
   :param full_data_retraining: Train models separately with all the data.
   :type full_data_retraining: bool
   :param additional_forecast_keys: List[str]: List of categoricals in timeseries that can act as multi-identifier.
   :param experimentation_mode: Selecting Thorough Experimentation will take longer to train.
   :type experimentation_mode: ExperimentationMode
   :param type_of_split: Type of data splitting into train/test.
   :type type_of_split: ForecastingDataSplitType
   :param test_by_item: Partition train/test data by item rather than time if true.
   :type test_by_item: bool
   :param test_start: Limit training data to dates before the given test start.
   :type test_start: str
   :param test_split: Percent of dataset to use for test data. We support using a range between 5% to 20% of your dataset to use as test data.
   :type test_split: int
   :param loss_function: Loss function for training neural network.
   :type loss_function: ForecastingLossFunction
   :param underprediction_weight: Weight for underpredictions
   :type underprediction_weight: float
   :param disable_networks_without_analytic_quantiles: Disable neural networks, which quantile functions do not have analytic expressions (e.g, mixture models)
   :type disable_networks_without_analytic_quantiles: bool
   :param initial_learning_rate: Initial learning rate.
   :type initial_learning_rate: float
   :param l2_regularization_factor: L2 regularization factor.
   :type l2_regularization_factor: float
   :param dropout_rate: Dropout percentage rate.
   :type dropout_rate: int
   :param recurrent_layers: Number of recurrent layers to stack in network.
   :type recurrent_layers: int
   :param recurrent_units: Number of units in each recurrent layer.
   :type recurrent_units: int
   :param convolutional_layers: Number of convolutional layers to stack on top of recurrent layers in network.
   :type convolutional_layers: int
   :param convolution_filters: Number of filters in each convolution.
   :type convolution_filters: int
   :param local_scaling_mode: Options to make NN inputs stationary in high dynamic range datasets.
   :type local_scaling_mode: ForecastingLocalScaling
   :param zero_predictor: Include subnetwork to classify points where target equals zero.
   :type zero_predictor: bool
   :param skip_missing: Make the RNN ignore missing entries rather instead of processing them.
   :type skip_missing: bool
   :param batch_size: Batch size.
   :type batch_size: ForecastingBatchSize
   :param batch_renormalization: Enable batch renormalization between layers.
   :type batch_renormalization: bool
   :param history_length: While training, how much history to consider.
   :type history_length: int
   :param prediction_step_size: Number of future periods to include in objective for each training sample.
   :type prediction_step_size: int
   :param training_point_overlap: Amount of overlap to allow between training samples.
   :type training_point_overlap: float
   :param max_scale_context: Maximum context to use for local scaling.
   :type max_scale_context: int
   :param quantiles_extension_method: Quantile extension method
   :type quantiles_extension_method: ForecastingQuanitlesExtensionMethod
   :param number_of_samples: Number of samples for ancestral simulation
   :type number_of_samples: int
   :param symmetrize_quantiles: Force symmetric quantiles (like in Gaussian distribution)
   :type symmetrize_quantiles: bool
   :param use_log_transforms: Apply logarithmic transformations to input data.
   :type use_log_transforms: bool
   :param smooth_history: Smooth (low pass filter) the timeseries.
   :type smooth_history: float
   :param local_scale_target: Using per training/prediction window target scaling.
   :type local_scale_target: bool
   :param use_clipping: Apply clipping to input data to stabilize the training.
   :type use_clipping: bool
   :param timeseries_weight_column: If set, we use the values in this column from timeseries data to assign time dependent item weights during training and evaluation.
   :type timeseries_weight_column: str
   :param item_attributes_weight_column: If set, we use the values in this column from item attributes data to assign weights to items during training and evaluation.
   :type item_attributes_weight_column: str
   :param use_timeseries_weights_in_objective: If True, we include weights from column set as "TIMESERIES WEIGHT COLUMN" in objective functions.
   :type use_timeseries_weights_in_objective: bool
   :param use_item_weights_in_objective: If True, we include weights from column set as "ITEM ATTRIBUTES WEIGHT COLUMN" in objective functions.
   :type use_item_weights_in_objective: bool
   :param skip_timeseries_weight_scaling: If True, we will avoid normalizing the weights.
   :type skip_timeseries_weight_scaling: bool
   :param timeseries_loss_weight_column: Use value in this column to weight the loss while training.
   :type timeseries_loss_weight_column: str
   :param use_item_id: Include a feature to indicate the item being forecast.
   :type use_item_id: bool
   :param use_all_item_totals: Include as input total target across items.
   :type use_all_item_totals: bool
   :param handle_zeros_as_missing_values: If True, handle zero values in demand as missing data.
   :type handle_zeros_as_missing_values: bool
   :param datetime_holiday_calendars: Holiday calendars to augment training with.
   :type datetime_holiday_calendars: List[HolidayCalendars]
   :param fill_missing_values: Strategy for filling in missing values.
   :type fill_missing_values: List[dict]
   :param enable_clustering: Enable clustering in forecasting.
   :type enable_clustering: bool
   :param data_split_feature_group_table_name: Specify the table name of the feature group to export training data with the fold column.
   :type data_split_feature_group_table_name: str
   :param custom_loss_functions: Registered custom losses available for selection.
   :type custom_loss_functions: List[str]
   :param custom_metrics: Registered custom metrics available for selection.
   :type custom_metrics: List[str]
   :param return_fractional_forecasts: Use this to return fractional forecast values while prediction

   .. py:attribute:: prediction_length
      :type: int

      

   .. py:attribute:: objective
      :type: abacusai.api_class.enums.ForecastingObjective

      

   .. py:attribute:: sort_objective
      :type: abacusai.api_class.enums.ForecastingObjective

      

   .. py:attribute:: forecast_frequency
      :type: abacusai.api_class.enums.ForecastingFrequency

      

   .. py:attribute:: probability_quantiles
      :type: List[float]

      

   .. py:attribute:: force_prediction_length
      :type: bool

      

   .. py:attribute:: filter_items
      :type: bool

      

   .. py:attribute:: enable_feature_selection
      :type: bool

      

   .. py:attribute:: enable_padding
      :type: bool

      

   .. py:attribute:: enable_cold_start
      :type: bool

      

   .. py:attribute:: enable_multiple_backtests
      :type: bool

      

   .. py:attribute:: num_backtesting_windows
      :type: int

      

   .. py:attribute:: backtesting_window_step_size
      :type: int

      

   .. py:attribute:: full_data_retraining
      :type: bool

      

   .. py:attribute:: additional_forecast_keys
      :type: List[str]

      

   .. py:attribute:: experimentation_mode
      :type: abacusai.api_class.enums.ExperimentationMode

      

   .. py:attribute:: type_of_split
      :type: abacusai.api_class.enums.ForecastingDataSplitType

      

   .. py:attribute:: test_by_item
      :type: bool

      

   .. py:attribute:: test_start
      :type: str

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: loss_function
      :type: abacusai.api_class.enums.ForecastingLossFunction

      

   .. py:attribute:: underprediction_weight
      :type: float

      

   .. py:attribute:: disable_networks_without_analytic_quantiles
      :type: bool

      

   .. py:attribute:: initial_learning_rate
      :type: float

      

   .. py:attribute:: l2_regularization_factor
      :type: float

      

   .. py:attribute:: dropout_rate
      :type: int

      

   .. py:attribute:: recurrent_layers
      :type: int

      

   .. py:attribute:: recurrent_units
      :type: int

      

   .. py:attribute:: convolutional_layers
      :type: int

      

   .. py:attribute:: convolution_filters
      :type: int

      

   .. py:attribute:: local_scaling_mode
      :type: abacusai.api_class.enums.ForecastingLocalScaling

      

   .. py:attribute:: zero_predictor
      :type: bool

      

   .. py:attribute:: skip_missing
      :type: bool

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:attribute:: batch_renormalization
      :type: bool

      

   .. py:attribute:: history_length
      :type: int

      

   .. py:attribute:: prediction_step_size
      :type: int

      

   .. py:attribute:: training_point_overlap
      :type: float

      

   .. py:attribute:: max_scale_context
      :type: int

      

   .. py:attribute:: quantiles_extension_method
      :type: abacusai.api_class.enums.ForecastingQuanitlesExtensionMethod

      

   .. py:attribute:: number_of_samples
      :type: int

      

   .. py:attribute:: symmetrize_quantiles
      :type: bool

      

   .. py:attribute:: use_log_transforms
      :type: bool

      

   .. py:attribute:: smooth_history
      :type: float

      

   .. py:attribute:: local_scale_target
      :type: bool

      

   .. py:attribute:: use_clipping
      :type: bool

      

   .. py:attribute:: timeseries_weight_column
      :type: str

      

   .. py:attribute:: item_attributes_weight_column
      :type: str

      

   .. py:attribute:: use_timeseries_weights_in_objective
      :type: bool

      

   .. py:attribute:: use_item_weights_in_objective
      :type: bool

      

   .. py:attribute:: skip_timeseries_weight_scaling
      :type: bool

      

   .. py:attribute:: timeseries_loss_weight_column
      :type: str

      

   .. py:attribute:: use_item_id
      :type: bool

      

   .. py:attribute:: use_all_item_totals
      :type: bool

      

   .. py:attribute:: handle_zeros_as_missing_values
      :type: bool

      

   .. py:attribute:: datetime_holiday_calendars
      :type: List[abacusai.api_class.enums.HolidayCalendars]

      

   .. py:attribute:: fill_missing_values
      :type: List[dict]

      

   .. py:attribute:: enable_clustering
      :type: bool

      

   .. py:attribute:: data_split_feature_group_table_name
      :type: str

      

   .. py:attribute:: custom_loss_functions
      :type: List[str]

      

   .. py:attribute:: custom_metrics
      :type: List[str]

      

   .. py:attribute:: return_fractional_forecasts
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: NamedEntityExtractionTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the NAMED_ENTITY_EXTRACTION problem type

   :param objective: Ranking scheme used to select final best model.
   :type objective: NERObjective
   :param sort_objective: Ranking scheme used to sort models on the metrics page.
   :type sort_objective: NERObjective
   :param ner_model_type: Type of NER model to use.
   :type ner_model_type: NERModelType
   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param test_row_indicator: Column indicating which rows to use for training (TRAIN) and testing (TEST).
   :type test_row_indicator: str
   :param active_labels_column: Entities that have been marked in a particular text
   :type active_labels_column: str
   :param document_format: Format of the input documents.
   :type document_format: NLPDocumentFormat
   :param include_longformer: Whether to include the longformer model.
   :type include_longformer: bool
   :param save_predicted_pdf: Whether to save predicted PDF documents
   :type save_predicted_pdf: bool
   :param enhanced_ocr: Enhanced text extraction from predicted digital documents
   :type enhanced_ocr: bool

   .. py:attribute:: objective
      :type: abacusai.api_class.enums.NERObjective

      

   .. py:attribute:: sort_objective
      :type: abacusai.api_class.enums.NERObjective

      

   .. py:attribute:: ner_model_type
      :type: abacusai.api_class.enums.NERModelType

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: test_row_indicator
      :type: str

      

   .. py:attribute:: active_labels_column
      :type: str

      

   .. py:attribute:: document_format
      :type: abacusai.api_class.enums.NLPDocumentFormat

      

   .. py:attribute:: include_longformer
      :type: bool

      

   .. py:attribute:: save_predicted_pdf
      :type: bool
      :value: True

      

   .. py:attribute:: enhanced_ocr
      :type: bool
      :value: False

      

   .. py:method:: __post_init__()



.. py:class:: NaturalLanguageSearchTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the NATURAL_LANGUAGE_SEARCH problem type

   :param abacus_internal_model: Use a Abacus.AI LLM to answer questions about your data without using any external APIs
   :type abacus_internal_model: bool
   :param num_completion_tokens: Default for maximum number of tokens for chat answers. Reducing this will get faster responses which are more succinct
   :type num_completion_tokens: int
   :param larger_embeddings: Use a higher dimension embedding model.
   :type larger_embeddings: bool
   :param search_chunk_size: Chunk size for indexing the documents.
   :type search_chunk_size: int
   :param chunk_overlap_fraction: Overlap in chunks while indexing the documents.
   :type chunk_overlap_fraction: float
   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int

   .. py:attribute:: abacus_internal_model
      :type: bool

      

   .. py:attribute:: num_completion_tokens
      :type: int

      

   .. py:attribute:: larger_embeddings
      :type: bool

      

   .. py:attribute:: search_chunk_size
      :type: int

      

   .. py:attribute:: index_fraction
      :type: float

      

   .. py:attribute:: chunk_overlap_fraction
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: ChatLLMTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the CHAT_LLM problem type

   :param document_retrievers: List of document retriever names to use for the feature stores this model was trained with.
   :type document_retrievers: List[str]
   :param num_completion_tokens: Default for maximum number of tokens for chat answers. Reducing this will get faster responses which are more succinct
   :type num_completion_tokens: int
   :param temperature: The generative LLM temperature
   :type temperature: float
   :param metadata_columns: Include the metadata column values in the retrieved search results.
   :type metadata_columns: list
   :param include_general_knowledge: Allow the LLM to rely not just on search results, but to fall back on general knowledge.
   :type include_general_knowledge: bool
   :param behavior_instructions: Customize the overall role instructions for the LLM.
   :type behavior_instructions: str
   :param response_instructions: Customize instructions for what the LLM responses should look like.
   :type response_instructions: str
   :param max_search_results: Maximum number of search results in the retrieval augmentation step. If we know that the questions are likely to have snippets which are easily matched in the documents, then a lower number will help with accuracy.
   :type max_search_results: int
   :param data_feature_group_ids: (List[str]): List of feature group ids to use to possibly query for the chatllm.
   :param data_prompt_context: Prompt context for the data feature group ids.
   :type data_prompt_context: str

   .. py:attribute:: document_retrievers
      :type: List[str]

      

   .. py:attribute:: num_completion_tokens
      :type: int

      

   .. py:attribute:: temperature
      :type: float

      

   .. py:attribute:: metadata_columns
      :type: list

      

   .. py:attribute:: include_general_knowledge
      :type: bool

      

   .. py:attribute:: behavior_instructions
      :type: str

      

   .. py:attribute:: response_instructions
      :type: str

      

   .. py:attribute:: max_search_results
      :type: int

      

   .. py:attribute:: data_feature_group_ids
      :type: List[str]

      

   .. py:attribute:: data_prompt_context
      :type: str

      

   .. py:method:: __post_init__()



.. py:class:: SentenceBoundaryDetectionTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the SENTENCE_BOUNDARY_DETECTION problem type

   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param dropout_rate: Dropout rate for neural network.
   :type dropout_rate: float
   :param batch_size: Batch size for neural network.
   :type batch_size: BatchSize

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: dropout_rate
      :type: float

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:method:: __post_init__()



.. py:class:: SentimentDetectionTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the SENTIMENT_DETECTION problem type

   :param sentiment_type: Type of sentiment to detect.
   :type sentiment_type: SentimentType
   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param dropout_rate: Dropout rate for neural network.
   :type dropout_rate: float
   :param batch_size: Batch size for neural network.
   :type batch_size: BatchSize
   :param compute_metrics: Whether to compute metrics.
   :type compute_metrics: bool

   .. py:attribute:: sentiment_type
      :type: abacusai.api_class.enums.SentimentType

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: dropout_rate
      :type: float

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:attribute:: compute_metrics
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: DocumentClassificationTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the DOCUMENT_CLASSIFICATION problem type

   :param zero_shot_hypotheses: Zero shot hypotheses. Example text: 'This text is about pricing'.
   :type zero_shot_hypotheses: List[str]
   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param dropout_rate: Dropout rate for neural network.
   :type dropout_rate: float
   :param batch_size: Batch size for neural network.
   :type batch_size: BatchSize

   .. py:attribute:: zero_shot_hypotheses
      :type: List[str]

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: dropout_rate
      :type: float

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:method:: __post_init__()



.. py:class:: DocumentSummarizationTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the DOCUMENT_SUMMARIZATION problem type

   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param dropout_rate: Dropout rate for neural network.
   :type dropout_rate: float
   :param batch_size: Batch size for neural network.
   :type batch_size: BatchSize

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: dropout_rate
      :type: float

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:method:: __post_init__()



.. py:class:: DocumentVisualizationTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the DOCUMENT_VISUALIZATION problem type

   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param dropout_rate: Dropout rate for neural network.
   :type dropout_rate: float
   :param batch_size: Batch size for neural network.
   :type batch_size: BatchSize

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: dropout_rate
      :type: float

      

   .. py:attribute:: batch_size
      :type: abacusai.api_class.enums.BatchSize

      

   .. py:method:: __post_init__()



.. py:class:: ClusteringTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the CLUSTERING problem type

   :param num_clusters_selection: Number of clusters. If None, will be selected automatically.
   :type num_clusters_selection: int

   .. py:attribute:: num_clusters_selection
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: ClusteringTimeseriesTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the CLUSTERING_TIMESERIES problem type

   :param num_clusters_selection: Number of clusters. If None, will be selected automatically.
   :type num_clusters_selection: int
   :param imputation: Imputation method for missing values.
   :type imputation: ClusteringImputationMethod

   .. py:attribute:: num_clusters_selection
      :type: int

      

   .. py:attribute:: imputation
      :type: abacusai.api_class.enums.ClusteringImputationMethod

      

   .. py:method:: __post_init__()



.. py:class:: EventAnomalyTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the EVENT_ANOMALY problem type

   :param anomaly_fraction: The fraction of the dataset to classify as anomalous, between 0 and 0.5
   :type anomaly_fraction: float

   .. py:attribute:: anomaly_fraction
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: CumulativeForecastingTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the CUMULATIVE_FORECASTING problem type

   :param test_split: Percent of dataset to use for test data. We support using a range between 5 ( i.e. 5% ) to 20 ( i.e. 20% ) of your dataset.
   :type test_split: int
   :param historical_frequency: Forecast frequency
   :type historical_frequency: str
   :param cumulative_prediction_lengths: List of Cumulative Prediction Frequencies. Each prediction length must be between 1 and 365.
   :type cumulative_prediction_lengths: List[int]
   :param skip_input_transform: Avoid doing numeric scaling transformations on the input.
   :type skip_input_transform: bool
   :param skip_target_transform: Avoid doing numeric scaling transformations on the target.
   :type skip_target_transform: bool
   :param predict_residuals: Predict residuals instead of totals at each prediction step.
   :type predict_residuals: bool

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: historical_frequency
      :type: str

      

   .. py:attribute:: cumulative_prediction_lengths
      :type: List[int]

      

   .. py:attribute:: skip_input_transform
      :type: bool

      

   .. py:attribute:: skip_target_transform
      :type: bool

      

   .. py:attribute:: predict_residuals
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: AnomalyDetectionTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the ANOMALY_DETECTION problem type

   :param test_split: Percent of dataset to use for test data. We support using a range between 5 (i.e. 5%) to 20 (i.e. 20%) of your dataset to use as test data.
   :type test_split: int
   :param value_high: Detect unusually high values.
   :type value_high: bool
   :param mixture_of_gaussians: Detect unusual combinations of values using mixture of Gaussians.
   :type mixture_of_gaussians: bool
   :param variational_autoencoder: Use variational autoencoder for anomaly detection.
   :type variational_autoencoder: bool
   :param spike_up: Detect outliers with a high value.
   :type spike_up: bool
   :param spike_down: Detect outliers with a low value.
   :type spike_down: bool
   :param trend_change: Detect changes to the trend.
   :type trend_change: bool

   .. py:attribute:: test_split
      :type: int

      

   .. py:attribute:: value_high
      :type: bool

      

   .. py:attribute:: mixture_of_gaussians
      :type: bool

      

   .. py:attribute:: variational_autoencoder
      :type: bool

      

   .. py:attribute:: spike_up
      :type: bool

      

   .. py:attribute:: spike_down
      :type: bool

      

   .. py:attribute:: trend_change
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: ThemeAnalysisTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the THEME ANALYSIS problem type

   .. py:method:: __post_init__()



.. py:class:: AIAgentTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the AI_AGENT problem type

   :param description: Description of the agent function.
   :type description: str
   :param enable_binary_input: If True, the agent will be able to accept binary data as inputs.
   :type enable_binary_input: bool

   .. py:attribute:: description
      :type: str

      

   .. py:attribute:: enable_binary_input
      :type: bool

      

   .. py:method:: __post_init__()



.. py:class:: CustomTrainedModelTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the CUSTOM_TRAINED_MODEL problem type

   :param max_catalog_size: Maximum expected catalog size.
   :type max_catalog_size: int
   :param max_dimension: Maximum expected dimension of the catalog.
   :type max_dimension: int
   :param index_output_path: Fully qualified cloud location (GCS, S3, etc) to export snapshots of the embedding to.
   :type index_output_path: str
   :param docker_image_uri: Docker image URI.
   :type docker_image_uri: str
   :param service_port: Service port.
   :type service_port: int

   .. py:attribute:: max_catalog_size
      :type: int

      

   .. py:attribute:: max_dimension
      :type: int

      

   .. py:attribute:: index_output_path
      :type: str

      

   .. py:attribute:: docker_image_uri
      :type: str

      

   .. py:attribute:: service_port
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: CustomAlgorithmTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the CUSTOM_ALGORITHM problem type

   :param train_function_name: The name of the train function.
   :type train_function_name: str
   :param predict_many_function_name: The name of the predict many function.
   :type predict_many_function_name: str
   :param training_input_tables: List of tables to use for training.
   :type training_input_tables: List[str]
   :param predict_function_name: Optional name of the predict function if the predict many function is not given.
   :type predict_function_name: str
   :param train_module_name: The name of the train module - only relevant if model is being uploaded from a zip file or github repositoty.
   :type train_module_name: str
   :param predict_module_name: The name of the predict module - only relevant if model is being uploaded from a zip file or github repositoty.
   :type predict_module_name: str
   :param test_split: Percent of dataset to use for test data. We support using a range between 6% to 20% of your dataset to use as test data.
   :type test_split: int

   .. py:attribute:: train_function_name
      :type: str

      

   .. py:attribute:: predict_many_function_name
      :type: str

      

   .. py:attribute:: training_input_tables
      :type: List[str]

      

   .. py:attribute:: predict_function_name
      :type: str

      

   .. py:attribute:: train_module_name
      :type: str

      

   .. py:attribute:: predict_module_name
      :type: str

      

   .. py:attribute:: test_split
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: OptimizationTrainingConfig


   Bases: :py:obj:`TrainingConfig`

   Training config for the OPTIMIZATION problem type

   :param solve_time_limit: The maximum time in seconds to spend solving the problem. Accepts values between 0 and 86400.
   :type solve_time_limit: float

   .. py:attribute:: solve_time_limit
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: _TrainingConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key
      :value: 'problem_type'

      

   .. py:attribute:: config_class_map

      


.. py:class:: DeployableAlgorithm


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Algorithm that can be deployed to a model.

   :param algorithm: ID of the algorithm.
   :type algorithm: str
   :param name: Name of the algorithm.
   :type name: str
   :param only_offline_deployable: Whether the algorithm can only be deployed offline.
   :type only_offline_deployable: bool
   :param trained_model_types: List of trained model types.
   :type trained_model_types: List[dict]

   .. py:attribute:: algorithm
      :type: str

      

   .. py:attribute:: name
      :type: str

      

   .. py:attribute:: only_offline_deployable
      :type: bool

      

   .. py:attribute:: trained_model_types
      :type: List[dict]

      


.. py:class:: ForecastingMonitorConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: id_column
      :type: str

      

   .. py:attribute:: timestamp_column
      :type: str

      

   .. py:attribute:: target_column
      :type: str

      

   .. py:attribute:: start_time
      :type: str

      

   .. py:attribute:: end_time
      :type: str

      

   .. py:method:: to_dict()

      Standardizes converting an ApiClass to dictionary.
      Keys of response dictionary are converted to camel case.
      This also validates the fields ( type, value, etc ) received in the dictionary.



.. py:class:: AlertConditionConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: alert_type
      :type: abacusai.api_class.enums.MonitorAlertType

      

   .. py:method:: _get_builder()
      :classmethod:



.. py:class:: AccuracyBelowThresholdConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   Accuracy Below Threshold Condition Config for Monitor Alerts

   :param threshold: Threshold for when to consider a column to be in violation. The alert will only fire when the drift value is strictly greater than the threshold.
   :type threshold: float

   .. py:attribute:: threshold
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: FeatureDriftConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   Feature Drift Condition Config for Monitor Alerts

   :param feature_drift_type: Feature drift type to apply the threshold on to determine whether a column has drifted significantly enough to be a violation.
   :type feature_drift_type: FeatureDriftType
   :param threshold: Threshold for when to consider a column to be in violation. The alert will only fire when the drift value is strictly greater than the threshold.
   :type threshold: float
   :param minimum_violations: Number of columns that must exceed the specified threshold to trigger an alert.
   :type minimum_violations: int
   :param feature_names: List of feature names to monitor for this alert.
   :type feature_names: List[str]

   .. py:attribute:: feature_drift_type
      :type: abacusai.api_class.enums.FeatureDriftType

      

   .. py:attribute:: threshold
      :type: float

      

   .. py:attribute:: minimum_violations
      :type: int

      

   .. py:attribute:: feature_names
      :type: List[str]

      

   .. py:method:: __post_init__()



.. py:class:: TargetDriftConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   Target Drift Condition Config for Monitor Alerts

   :param feature_drift_type: Target drift type to apply the threshold on to determine whether a column has drifted significantly enough to be a violation.
   :type feature_drift_type: FeatureDriftType
   :param threshold: Threshold for when to consider the target column to be in violation. The alert will only fire when the drift value is strictly greater than the threshold.
   :type threshold: float

   .. py:attribute:: feature_drift_type
      :type: abacusai.api_class.enums.FeatureDriftType

      

   .. py:attribute:: threshold
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: HistoryLengthDriftConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   History Length Drift Condition Config for Monitor Alerts

   :param feature_drift_type: History length drift type to apply the threshold on to determine whether the history length has drifted significantly enough to be a violation.
   :type feature_drift_type: FeatureDriftType
   :param threshold: Threshold for when to consider the history length  to be in violation. The alert will only fire when the drift value is strictly greater than the threshold.
   :type threshold: float

   .. py:attribute:: feature_drift_type
      :type: abacusai.api_class.enums.FeatureDriftType

      

   .. py:attribute:: threshold
      :type: float

      

   .. py:method:: __post_init__()



.. py:class:: DataIntegrityViolationConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   Data Integrity Violation Condition Config for Monitor Alerts

   :param data_integrity_type: This option selects the data integrity violations to monitor for this alert.
   :type data_integrity_type: DataIntegrityViolationType
   :param minimum_violations: Number of columns that must exceed the specified threshold to trigger an alert.
   :type minimum_violations: int

   .. py:attribute:: data_integrity_type
      :type: abacusai.api_class.enums.DataIntegrityViolationType

      

   .. py:attribute:: minimum_violations
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: BiasViolationConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   Bias Violation Condition Config for Monitor Alerts

   :param bias_type: This option selects the bias metric to monitor for this alert.
   :type bias_type: BiasType
   :param threshold: Threshold for when to consider a column to be in violation. The alert will only fire when the drift value is strictly greater than the threshold.
   :type threshold: float
   :param minimum_violations: Number of columns that must exceed the specified threshold to trigger an alert.
   :type minimum_violations: int

   .. py:attribute:: bias_type
      :type: abacusai.api_class.enums.BiasType

      

   .. py:attribute:: threshold
      :type: float

      

   .. py:attribute:: minimum_violations
      :type: int

      

   .. py:method:: __post_init__()



.. py:class:: PredictionCountConditionConfig


   Bases: :py:obj:`AlertConditionConfig`

   Deployment Prediction Condition Config for Deployment Alerts. By default we monitor if predictions made over a time window has reduced significantly.
   :param threshold: Threshold for when to consider to be a violation. Negative means alert on reduction, positive means alert on increase.
   :type threshold: float
   :param aggregation_window: Time window to aggregate the predictions over, e.g. 1h, 10m. Only h(hour), m(minute) and s(second) are supported.
   :type aggregation_window: str
   :param aggregation_type: Aggregation type to use for the aggregation window, e.g. sum, avg.
   :type aggregation_type: str

   .. py:attribute:: threshold
      :type: float

      

   .. py:attribute:: aggregation_window
      :type: str

      

   .. py:attribute:: aggregation_type
      :type: str

      

   .. py:method:: __post_init__()



.. py:class:: _AlertConditionConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key
      :value: 'alert_type'

      

   .. py:attribute:: config_class_key_value_camel_case
      :value: True

      

   .. py:attribute:: config_class_map

      


.. py:class:: AlertActionConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: action_type
      :type: abacusai.api_class.enums.AlertActionType

      

   .. py:method:: _get_builder()
      :classmethod:



.. py:class:: EmailActionConfig


   Bases: :py:obj:`AlertActionConfig`

   Email Action Config for Monitor Alerts

   :param email_recipients: List of email addresses to send the alert to.
   :type email_recipients: List[str]
   :param email_body: Body of the email to send.
   :type email_body: str

   .. py:attribute:: email_recipients
      :type: List[str]

      

   .. py:attribute:: email_body
      :type: str

      

   .. py:method:: __post_init__()



.. py:class:: _AlertActionConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key
      :value: 'action_type'

      

   .. py:attribute:: config_class_map

      


.. py:class:: FeatureMappingConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: feature_name
      :type: str

      

   .. py:attribute:: feature_mapping
      :type: str

      

   .. py:attribute:: nested_feature_name
      :type: str

      


.. py:class:: ProjectFeatureGroupTypeMappingsConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: feature_group_id
      :type: str

      

   .. py:attribute:: feature_group_type
      :type: str

      

   .. py:attribute:: feature_mappings
      :type: List[FeatureMappingConfig]

      

   .. py:method:: from_dict(input_dict)
      :classmethod:



.. py:class:: PythonFunctionArgument


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   A config class for python function arguments

   :param variable_type: The type of the python function argument
   :type variable_type: PythonFunctionArgumentType
   :param name: The name of the python function variable
   :type name: str
   :param is_required: Whether the argument is required
   :type is_required: bool
   :param value: The value of the argument
   :type value: Any
   :param pipeline_variable: The name of the pipeline variable to use as the value
   :type pipeline_variable: str

   .. py:attribute:: variable_type
      :type: abacusai.api_class.enums.PythonFunctionArgumentType

      

   .. py:attribute:: name
      :type: str

      

   .. py:attribute:: is_required
      :type: bool

      

   .. py:attribute:: value
      :type: Any

      

   .. py:attribute:: pipeline_variable
      :type: str

      


.. py:class:: OutputVariableMapping


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   A config class for python function arguments

   :param variable_type: The type of the python function output argument
   :type variable_type: PythonFunctionOutputArgumentType
   :param name: The name of the python function variable
   :type name: str

   .. py:attribute:: variable_type
      :type: abacusai.api_class.enums.PythonFunctionOutputArgumentType

      

   .. py:attribute:: name
      :type: str

      


.. py:class:: FeatureGroupExportConfig


   Bases: :py:obj:`abacusai.api_class.abstract.ApiClass`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: connector_type
      :type: abacusai.api_class.enums.ConnectorType

      

   .. py:method:: _get_builder()
      :classmethod:



.. py:class:: FileConnectorExportConfig


   Bases: :py:obj:`FeatureGroupExportConfig`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: connector_type
      :type: abacusai.api_class.enums.ConnectorType

      

   .. py:attribute:: location
      :type: str

      

   .. py:attribute:: export_file_format
      :type: str

      

   .. py:method:: to_dict()

      Standardizes converting an ApiClass to dictionary.
      Keys of response dictionary are converted to camel case.
      This also validates the fields ( type, value, etc ) received in the dictionary.



.. py:class:: DatabaseConnectorExportConfig


   Bases: :py:obj:`FeatureGroupExportConfig`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: connector_type
      :type: abacusai.api_class.enums.ConnectorType

      

   .. py:attribute:: database_connector_id
      :type: str

      

   .. py:attribute:: mode
      :type: str

      

   .. py:attribute:: object_name
      :type: str

      

   .. py:attribute:: id_column
      :type: str

      

   .. py:attribute:: additional_id_columns
      :type: List[str]

      

   .. py:attribute:: data_columns
      :type: Dict[str, str]

      

   .. py:method:: to_dict()

      Standardizes converting an ApiClass to dictionary.
      Keys of response dictionary are converted to camel case.
      This also validates the fields ( type, value, etc ) received in the dictionary.



.. py:class:: _FeatureGroupExportConfigFactory


   Bases: :py:obj:`abacusai.api_class.abstract._ApiClassFactory`

   Helper class that provides a standard way to create an ABC using
   inheritance.

   .. py:attribute:: config_abstract_class

      

   .. py:attribute:: config_class_key
      :value: 'connectorType'

      

   .. py:attribute:: config_class_map

      


.. py:class:: AgentResponse(*args)


   Response Object for agent to support attachments


.. py:class:: ApiClient(api_key = None, server = None, client_options = None, skip_version_check = False)


   Bases: :py:obj:`ReadOnlyClient`

   Abacus.AI API Client

   :param api_key: The api key to use as authentication to the server
   :type api_key: str
   :param server: The base server url to use to send API requets to
   :type server: str
   :param client_options: Optional API client configurations
   :type client_options: ClientOptions
   :param skip_version_check: If true, will skip checking the server's current API version on initializing the client
   :type skip_version_check: bool

   .. py:method:: create_dataset_from_pandas(feature_group_table_name, df, clean_column_names = False)

      [Deprecated]
      Creates a Dataset from a pandas dataframe

      :param feature_group_table_name: The table name to assign to the feature group created by this call
      :type feature_group_table_name: str
      :param df: The dataframe to upload
      :type df: pandas.DataFrame
      :param clean_column_names: If true, the dataframe's column names will be automatically cleaned to be complaint with Abacus.AI's column requirements. Otherwise it will raise a ValueError.
      :type clean_column_names: bool

      :returns: The dataset object created
      :rtype: Dataset


   .. py:method:: create_dataset_version_from_pandas(table_name_or_id, df, clean_column_names = False)

      [Deprecated]
      Updates an existing dataset from a pandas dataframe

      :param table_name_or_id: The table name of the feature group or the ID of the dataset to update
      :type table_name_or_id: str
      :param df: The dataframe to upload
      :type df: pandas.DataFrame
      :param clean_column_names: If true, the dataframe's column names will be automatically cleaned to be complaint with Abacus.AI's column requirements. Otherwise it will raise a ValueError.
      :type clean_column_names: bool

      :returns: The dataset updated
      :rtype: Dataset


   .. py:method:: create_feature_group_from_pandas_df(table_name, df, clean_column_names = False)

      Create a Feature Group from a local Pandas DataFrame.

      :param table_name: The table name to assign to the feature group created by this call
      :type table_name: str
      :param df: The dataframe to upload
      :type df: pandas.DataFrame
      :param clean_column_names: If true, the dataframe's column names will be automatically cleaned to be complaint with Abacus.AI's column requirements. Otherwise it will raise a ValueError.
      :type clean_column_names: bool


   .. py:method:: update_feature_group_from_pandas_df(table_name, df, clean_column_names = False)

      Updates a DATASET Feature Group from a local Pandas DataFrame.

      :param table_name: The table name to assign to the feature group created by this call
      :type table_name: str
      :param df: The dataframe to upload
      :type df: pandas.DataFrame
      :param clean_column_names: If true, the dataframe's column names will be automatically cleaned to be complaint with Abacus.AI's column requirements. Otherwise it will raise a ValueError.
      :type clean_column_names: bool


   .. py:method:: create_feature_group_from_spark_df(table_name, df)

      Create a Feature Group from a local Spark DataFrame.

      :param df: The dataframe to upload
      :type df: pyspark.sql.DataFrame
      :param table_name: The table name to assign to the feature group created by this call
      :type table_name: str


   .. py:method:: update_feature_group_from_spark_df(table_name, df)

      Create a Feature Group from a local Spark DataFrame.

      :param df: The dataframe to upload
      :type df: pyspark.sql.DataFrame
      :param table_name: The table name to assign to the feature group created by this call
      :type table_name: str
      :param should_wait_for_upload: Wait for dataframe to upload before returning. Some FeatureGroup methods, like materialization, may not work until upload is complete.
      :type should_wait_for_upload: bool
      :param timeout: If waiting for upload, time out after this limit.
      :type timeout: int, optional


   .. py:method:: create_spark_df_from_feature_group_version(session, feature_group_version)

      Create a Spark Dataframe in the provided Spark Session context, for a materialized Abacus Feature Group Version.

      :param session: Spark session
      :type session: pyspark.sql.SparkSession
      :param feature_group_version: Feature group version to load from
      :type feature_group_version: str

      :returns: pyspark.sql.DataFrame


   .. py:method:: create_prediction_operator_from_functions(name, project_id, predict_function = None, initialize_function = None, feature_group_ids = None, cpu_size = None, memory = None, included_modules = None, package_requirements = None, use_gpu = False)

      Create a new prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str
      :param name: Name of the prediction operator.
      :type name: str
      :param function_source_code: Contents of a valid Python source code file. The source code should contain the function `predictFunctionName`, and the function 'initializeFunctionName' if defined.
      :type function_source_code: str
      :param predict_function_name: Name of the optional initialize function found in the source code. This function will generate anything used by predictions, based on input feature groups.
      :type predict_function_name: str
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions.
      :type predict_function_name: str
      :param feature_group_ids: List of feature groups that are supplied to the initialize function as parameters. Each of the parameters are materialized Dataframes.
      :type feature_group_ids: list
      :param cpu_size: Size of the CPU for the prediction operator.
      :type cpu_size: str
      :param memory: Memory (in GB) for the  prediction operator.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: list
      :param use_gpu: Whether this rediction operator needs gpu.
      :type use_gpu: bool

      Returns
          PredictionOperator: The updated prediction operator object.


   .. py:method:: update_prediction_operator_from_functions(prediction_operator_id, name = None, predict_function = None, initialize_function = None, feature_group_ids = None, cpu_size = None, memory = None, included_modules = None, package_requirements = None, use_gpu = False)

      Update an existing prediction operator.

      :param name: The name of the prediction operator
      :type name: str
      :param project_id: The project to create the prediction in
      :type project_id: str
      :param predict_function: The predict function callable to serialize and upload
      :type predict_function: callable
      :param initialize_function: The initialize function callable to serialize and upload
      :type initialize_function: callable
      :param initialize_input_tables: The input table names of the feature groups to pass to the train function
      :type initialize_input_tables: list
      :param cpu_size: Size of the cpu for the training function
      :type cpu_size: str
      :param memory: Memory (in GB) for the training function
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list
      :param use_gpu: Whether this prediction needs gpu
      :type use_gpu: bool


   .. py:method:: create_model_from_functions(project_id, train_function, predict_function = None, training_input_tables = None, predict_many_function = None, initialize_function = None, cpu_size = None, memory = None, training_config = None, exclusive_run = False, included_modules = None, package_requirements = None, name = None, use_gpu = False, is_thread_safe = None)

      Creates a model from a python function

      :param project_id: The project to create the model in
      :type project_id: str
      :param train_function: The training fucntion callable to serialize and upload
      :type train_function: callable
      :param predict_function: The predict function callable to serialize and upload
      :type predict_function: callable
      :param predict_many_function: The predict many function callable to serialize and upload
      :type predict_many_function: callable
      :param initialize_function: The initialize function callable to serialize and upload
      :type initialize_function: callable
      :param training_input_tables: The input table names of the feature groups to pass to the train function
      :type training_input_tables: list
      :param cpu_size: Size of the cpu for the training function
      :type cpu_size: str
      :param memory: Memory (in GB) for the training function
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list
      :param name: The name of the model
      :type name: str
      :param use_gpu: Whether this model needs gpu
      :type use_gpu: bool
      :param is_thread_safe: Whether the model is thread safe
      :type is_thread_safe: bool


   .. py:method:: update_model_from_functions(model_id, train_function, predict_function = None, predict_many_function = None, initialize_function = None, training_input_tables = None, cpu_size = None, memory = None, included_modules = None, package_requirements = None, use_gpu = False, is_thread_safe = None)

      Creates a model from a python function. Please pass in all the functions, even if you don't update it.

      :param model_id: The id of the model to update
      :type model_id: str
      :param train_function: The training fucntion callable to serialize and upload
      :type train_function: callable
      :param predict_function: The predict function callable to serialize and upload
      :type predict_function: callable
      :param predict_many_function: The predict many function callable to serialize and upload
      :type predict_many_function: callable
      :param initialize_function: The initialize function callable to serialize and upload
      :type initialize_function: callable
      :param training_input_tables: The input table names of the feature groups to pass to the train function
      :type training_input_tables: list
      :param cpu_size: Size of the cpu for the training function
      :type cpu_size: str
      :param memory: Memory (in GB) for the training function
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list
      :param use_gpu: Whether this model needs gpu
      :type use_gpu: bool
      :param is_thread_safe: Whether the model is thread safe
      :type is_thread_safe: bool


   .. py:method:: create_pipeline_step_from_function(pipeline_id, step_name, function, step_input_mappings = None, output_variable_mappings = None, step_dependencies = None, package_requirements = None, cpu_size = None, memory = None, included_modules = None)

      Creates a step in a given pipeline from a python function.

      :param pipeline_id: The ID of the pipeline to add the step to.
      :type pipeline_id: str
      :param step_name: The name of the step.
      :type step_name: str
      :param function: The python function.
      :type function: callable
      :param step_input_mappings: List of Python function arguments.
      :type step_input_mappings: List[PythonFunctionArguments]
      :param output_variable_mappings: List of Python function ouputs.
      :type output_variable_mappings: List[OutputVariableMapping]
      :param step_dependencies: List of step names this step depends on.
      :type step_dependencies: List[str]
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param cpu_size: Size of the CPU for the step function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the step function.
      :type memory: int
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list


   .. py:method:: update_pipeline_step_from_function(pipeline_step_id, function, step_input_mappings = None, output_variable_mappings = None, step_dependencies = None, package_requirements = None, cpu_size = None, memory = None, included_modules = None)

      Updates a pipeline step from a python function.

      :param pipeline_step_id: The ID of the pipeline_step to update.
      :type pipeline_step_id: str
      :param function: The python function.
      :type function: callable
      :param step_input_mappings: List of Python function arguments.
      :type step_input_mappings: List[PythonFunctionArguments]
      :param output_variable_mappings: List of Python function ouputs.
      :type output_variable_mappings: List[OutputVariableMapping]
      :param step_dependencies: List of step names this step depends on.
      :type step_dependencies: List[str]
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param cpu_size: Size of the CPU for the step function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the step function.
      :type memory: int
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list


   .. py:method:: create_python_function_from_function(name, function, function_variable_mappings = None, package_requirements = None, function_type = PythonFunctionType.FEATURE_GROUP.value)

      Creates a custom Python function

      :param name: The name to identify the Python function.
      :type name: str
      :param function: The function callable to serialize and upload.
      :type function: callable
      :param function_variable_mappings: List of Python function arguments.
      :type function_variable_mappings: List<PythonFunctionArguments>
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: List
      :param function_type: Type of Python function to create. Default is FEATURE_GROUP, but can also be PLOTLY_FIG.
      :type function_type: PythonFunctionType


   .. py:method:: create_feature_group_from_python_function(function, table_name, input_tables = None, python_function_name = None, python_function_bindings = None, cpu_size = None, memory = None, package_requirements = None, included_modules = None)

      Creates a feature group from a python function

      :param function: The function callable for the feature group
      :type function: callable
      :param table_name: The table name to give the feature group
      :type table_name: str
      :param input_tables: The input table names of the feature groups as input to the feature group function
      :type input_tables: list
      :param python_function_name: The name of the python function to create a feature group from.
      :type python_function_name: str
      :param python_function_bindings: List of python function arguments
      :type python_function_bindings: List<PythonFunctionArguments>
      :param cpu_size: Size of the cpu for the feature group function
      :type cpu_size: str
      :param memory: Memory (in GB) for the feature group function
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list


   .. py:method:: update_python_function_code(name, function = None, function_variable_mappings = None, package_requirements = None, included_modules = None)

      Update custom python function with user inputs for the given python function.

      :param name: The unique name to identify the python function in an organization.
      :type name: String
      :param function: The function callable to serialize and upload.
      :type function: callable
      :param function_variable_mappings: List of python function arguments
      :type function_variable_mappings: List<PythonFunctionArguments>
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list

      :returns: The python_function object.
      :rtype: PythonFunction


   .. py:method:: create_algorithm_from_function(name, problem_type, training_data_parameter_names_mapping = None, training_config_parameter_name = None, train_function = None, predict_function = None, predict_many_function = None, initialize_function = None, common_functions = None, config_options = None, is_default_enabled = False, project_id = None, use_gpu = False, package_requirements = None, included_modules = None)

      Create a new algorithm, or update existing algorithm if the name already exists

      :param name: The name to identify the algorithm, only uppercase letters, numbers and underscore allowed
      :type name: String
      :param problem_type: The type of the problem this algorithm will work on
      :type problem_type: str
      :param train_function: The training function callable to serialize and upload
      :type train_function: callable
      :param predict_function: The predict function callable to serialize and upload
      :type predict_function: callable
      :param predict_many_function: The predict many function callable to serialize and upload
      :type predict_many_function: callable
      :param initialize_function: The initialize function callable to serialize and upload
      :type initialize_function: callable
      :param common_functions: A list of functions that will be used by both train and predict functions, e.g. some data processing utilities
      :type common_functions: List of callables
      :param training_data_parameter_names_mapping: The mapping from feature group types to training data parameter names in the train function
      :type training_data_parameter_names_mapping: Dict
      :param training_config_parameter_name: The train config parameter name in the train function
      :type training_config_parameter_name: string
      :param config_options: Map dataset types and configs to train function parameter names
      :type config_options: Dict
      :param is_default_enabled: Whether train with the algorithm by default
      :param project_id: The unique version ID of the project
      :type project_id: Unique String Identifier
      :param use_gpu: Whether this algorithm needs to run on GPU
      :type use_gpu: Boolean
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list


   .. py:method:: update_algorithm_from_function(algorithm, training_data_parameter_names_mapping = None, training_config_parameter_name = None, train_function = None, predict_function = None, predict_many_function = None, initialize_function = None, common_functions = None, config_options = None, is_default_enabled = None, use_gpu = None, package_requirements = None, included_modules = None)

      Create a new algorithm, or update existing algorithm if the name already exists

      :param algorithm: The name to identify the algorithm, only uppercase letters, numbers and underscore allowed
      :type algorithm: String
      :param train_function: The training fucntion callable to serialize and upload
      :type train_function: callable
      :param predict_function: The predict function callable to serialize and upload
      :type predict_function: callable
      :param predict_many_function: The predict many function callable to serialize and upload
      :type predict_many_function: callable
      :param initialize_function: The initialize function callable to serialize and upload
      :type initialize_function: callable
      :param common_functions: A list of functions that will be used by both train and predict functions, e.g. some data processing utilities
      :type common_functions: List of callables
      :param training_data_parameter_names_mapping: The mapping from feature group types to training data parameter names in the train function
      :type training_data_parameter_names_mapping: Dict
      :param training_config_parameter_name: The train config parameter name in the train function
      :type training_config_parameter_name: string
      :param config_options: Map dataset types and configs to train function parameter names
      :type config_options: Dict
      :param is_default_enabled: Whether train with the algorithm by default
      :type is_default_enabled: Boolean
      :param use_gpu: Whether this algorithm needs to run on GPU
      :type use_gpu: Boolean
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param included_modules: A list of user-created modules that will be included, which is equivalent to 'from module import *'
      :type included_modules: list


   .. py:method:: get_train_function_input(project_id, training_table_names = None, training_data_parameter_name_override = None, training_config_parameter_name_override = None, training_config = None, custom_algorithm_config = None)

      Get the input data for the train function to test locally.

      :param project_id: The id of the project
      :type project_id: String
      :param training_table_names: A list of feature group tables used for training
      :type training_table_names: List
      :param training_data_parameter_name_override: The mapping from feature group types to training data parameter names in the train function
      :type training_data_parameter_name_override: Dict
      :param training_config_parameter_name_override: The train config parameter name in the train function
      :type training_config_parameter_name_override: String
      :param training_config: A dictionary for Abacus.AI defined training options and values
      :type training_config: Dict
      :param custom_algorithm_config: User-defined config that can be serialized by JSON
      :type custom_algorithm_config: Any

      :returns: A dictionary that maps train function parameter names to their values.


   .. py:method:: get_train_function_input_from_model_version(model_version, algorithm = None, training_config = None, custom_algorithm_config = None)

      Get the input data for the train function to test locally, based on a trained model version.

      :param model_version: The string identifier of the model version
      :type model_version: String
      :param algorithm: The particular algorithm's name, whose train function to test with
      :type algorithm: String
      :param training_config: A dictionary for Abacus.AI defined training options and values
      :type training_config: Dict
      :param custom_algorithm_config: User-defined config that can be serialized by JSON
      :type custom_algorithm_config: Any

      :returns: A dictionary that maps train function parameter names to their values.


   .. py:method:: create_custom_loss_function(name, loss_function_type, loss_function)

      Registers a new custom loss function which can be used as an objective function during model training.

      :param name: A name for the loss. Should be unique per organization. Limit - 50 chars. Only underscores, numbers, uppercase alphabets allowed
      :type name: String
      :param loss_function_type: The category of problems that this loss would be applicable to. Ex - REGRESSION_DL_TF, CLASSIFICATION_DL_TF, etc.
      :type loss_function_type: String
      :param loss_function: A python functor which can take required arguments (Ex - (y_true, y_pred)) and returns loss value(s) (Ex - An array of loss values of size batch size)
      :type loss_function: Callable

      :returns: A description of the registered custom loss function
      :rtype: CustomLossFunction

      :raises InvalidParameterError: If either loss function name or type or the passed function is invalid/incompatible
      :raises AlreadyExistsError: If the loss function with the same name already exists in the organization


   .. py:method:: update_custom_loss_function(name, loss_function)

      Updates a previously registered custom loss function with a new function implementation.

      :param name: name of the registered custom loss.
      :type name: String
      :param loss_function: A python functor which can take required arguments (Ex - (y_true, y_pred)) and returns loss value(s) (Ex - An array of loss values of size batch size)
      :type loss_function: Callable

      :returns: A description of the updated custom loss function
      :rtype: CustomLossFunction

      :raises InvalidParameterError: If either loss function name or type or the passed function is invalid/incompatible
      :raises DataNotFoundError: If a loss function with given name is not found in the organization


   .. py:method:: create_custom_metric_from_function(name, problem_type, custom_metric_function)

      Registers a new custom metric which can be used as an evaluation metric for the trained model.

      :param name: A name for the metric. Should be unique per organization. Limit - 50 chars. Only underscores, numbers, uppercase alphabets allowed.
      :type name: String
      :param problem_type: The problem type that this metric would be applicable to. e.g. - REGRESSION, FORECASTING, etc.
      :type problem_type: String
      :param custom_metric_function: A python functor which can take required arguments e.g. (y_true, y_pred) and returns the metric value.
      :type custom_metric_function: Callable

      :returns: The newly created custom metric.
      :rtype: CustomMetric

      :raises InvalidParameterError: If either custom metric name or type or the passed function is invalid/incompatible.
      :raises AlreadyExistsError: If a custom metric with given name already exists in the organization.


   .. py:method:: update_custom_metric_from_function(name, custom_metric_function)

      Updates a previously registered custom metric.

      :param name: A name for the metric. Should be unique per organization. Limit - 50 chars. Only underscores, numbers, uppercase alphabets allowed.
      :type name: String
      :param custom_metric_function: A python functor which can take required arguments e.g. (y_true, y_pred) and returns the metric value.
      :type custom_metric_function: Callable

      :returns: The updated custom metric.
      :rtype: CustomMetric

      :raises InvalidParameterError: If either custom metric name or type or the passed function is invalid/incompatible.
      :raises DataNotFoundError: If a custom metric with given name is not found in the organization.


   .. py:method:: create_module_from_notebook(file_path, name)

      Create a module with the code marked in the notebook. Use '#module_start#' to mark the starting code cell and '#module_end#' for the
      ending code cell.

      :param file_path: Notebook's relative path to the root directory, e.g. 'n1.ipynb'
      :type file_path: String
      :param name: Name of the module to create.
      :type name: String

      :returns: the created Abacus.ai module object
      :rtype: Module


   .. py:method:: update_module_from_notebook(file_path, name)

      Update the module with the code marked in the notebook. Use '#module_start#' to mark the starting code cell and '#module_end#' for the
      ending code cell.

      :param file_path: Notebook's relative path to the root directory, e.g. 'n1.ipynb'
      :type file_path: String
      :param name: Name of the module to create.
      :type name: String

      :returns: the created Abacus.ai module object
      :rtype: Module


   .. py:method:: import_module(name)

      Import a module created previously. It will reload if has been imported before.
      This will be equivalent to including from that module file.

      :param name: Name of the module to import.
      :type name: String

      :returns: the imported python module
      :rtype: module


   .. py:method:: create_agent_from_function(project_id, agent_function, name = None, memory = None, package_requirements = None)

      Creates the agent from a python function

      :param project_id: The project to create the model in
      :type project_id: str
      :param agent_function: The agent function callable to serialize and upload
      :type agent_function: callable
      :param name: The name of the agent
      :type name: str
      :param memory: Memory (in GB) for hosting the agent
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List


   .. py:method:: update_agent_with_function(model_id, agent_function, memory = None, package_requirements = None, enable_binary_input = None)

      Updates the agent with a new agent function.

      :param model_id: The unique ID associated with the AI Agent to be changed.
      :type model_id: str
      :param agent_function: The new agent function callable to serialize and upload
      :type agent_function: callable
      :param memory: Memory (in GB) for hosting the agent
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: List
      :param enable_binary_input: If True, the agent will be able to accept binary data as inputs.
      :type enable_binary_input: bool


   .. py:method:: execute_feature_group_sql(sql, fix_query_on_error = False, timeout=3600, delay=2)

      Execute a SQL query on the feature groups

      :param sql: The SQL query to execute.
      :type sql: str
      :param fix_query_on_error: If enabled, SQL query is auto fixed if parsing fails.
      :type fix_query_on_error: bool

      :returns: The result of the query.
      :rtype: pandas.DataFrame


   .. py:method:: get_agent_context_chat_history()

      Gets a history of chat messages from the current request context. Applicable within a AIAgent
      execute function.

      :returns: List[ChatMessage]:: The chat history for the current request being processed by the Agent.


   .. py:method:: set_agent_context_chat_history(chat_history)

      Sets the history of chat messages from the current request context.

      :param chat_history: The chat history associated with the current request context.
      :type chat_history: List[ChatMessage]


   .. py:method:: get_agent_context_conversation_id()

      Gets the deployment conversation ID from the current request context. Applicable within a AIAgent
      execute function.

      :returns: The deployment conversation ID for the current request being processed by the Agent.
      :rtype: str


   .. py:method:: set_agent_context_conversation_id(conversation_id)

      Sets the deployment conversation ID from the current request context.

      :param conversation_id: The deployment conversation ID for the current request being processed by the Agent.
      :type conversation_id: str


   .. py:method:: get_agent_context_external_session_id()

      Gets the external session ID from the current request context if it has been set with the request.
      Applicable within a AIAgent execute function.

      :returns: The external session ID for the current request being processed by the Agent.
      :rtype: str


   .. py:method:: set_agent_context_external_session_id(external_session_id)

      Sets the external session ID from the current request context if it has been set with the request.

      :param external_session_id: The external session ID for the current request being processed by the Agent.
      :type external_session_id: str


   .. py:method:: get_agent_context_doc_ids()

      Gets the document ID from the current request context if a document has been uploaded with the request.
      Applicable within a AIAgent execute function.

      :returns: The document IDs the current request being processed by the Agent.
      :rtype: List[str]


   .. py:method:: set_agent_context_doc_ids(doc_ids)

      Sets the doc_ids from the current request context.

      :param doc_ids: The doc_ids associated with the current request context.
      :type doc_ids: List[str]


   .. py:method:: get_agent_context_blob_inputs()

      Gets the BlobInputs from the current request context if a document has been uploaded with the request.
      Applicable within a AIAgent execute function.

      :returns: The BlobInput the current request being processed by the Agent.
      :rtype: List[BlobInput]


   .. py:method:: clear_agent_context()

      Clears the current request context.


   .. py:method:: streaming_evaluate_prompt(prompt = None, system_message = None, llm_name = None, max_tokens = None, temperature = 0.0, messages = None, response_type = None, json_response_schema = None)

      Generate response to the prompt using the specified model.
      This works similar to evaluate_prompt, but would stream the result as well so that user is aware of the current status of the generation.

      :param prompt: Prompt to use for generation.
      :type prompt: str
      :param system_message: System prompt for models that support it.
      :type system_message: str
      :param llm_name: Name of the underlying LLM to be used for generation. Default is auto selection.
      :type llm_name: LLMName
      :param max_tokens: Maximum number of tokens to generate. If set, the model will just stop generating after this token limit is reached.
      :type max_tokens: int
      :param temperature: Temperature to use for generation. Higher temperature makes more non-deterministic responses, a value of zero makes mostly deterministic reponses. Default is 0.0. A range of 0.0 - 2.0 is allowed.
      :type temperature: float
      :param messages: A list of messages to use as conversation history. For completion models like OPENAI_GPT3_5_TEXT and PALM_TEXT this should not be set. A message is a dict with attributes: is_user (bool): Whether the message is from the user. text (str): The message's text.
      :type messages: list
      :param response_type: Specifies the type of response to request from the LLM. One of 'text' and 'json'. If set to 'json', the LLM will respond with a json formatted string whose schema can be specified `json_response_schema`. Defaults to 'text'
      :type response_type: str
      :param json_response_schema: A dictionary specifying the keys/schema/parameters which LLM should adhere to in its response when `response_type` is 'json'. Each parameter is mapped to a dict with the following info - type (str) (required): Data type of the parameter description (str) (required): Description of the parameter is_required (bool) (optional): Whether the parameter is required or not.     Example:     json_response_schema={         'title': {'type': 'string', 'description': 'Article title', 'is_required': true},         'body': {'type': 'string', 'description': 'Article body'},     }
      :type json_response_schema: dict

      :returns: The response from the model.
      :rtype: text


   .. py:method:: _get_agent_app_request_id()

      Gets the current request ID for the current request context of async app. Applicable within a AIAgent execute function.

      :returns: The request ID for the current request being processed by the Agent.
      :rtype: str


   .. py:method:: _get_agent_async_app_caller()

      Gets the caller for the current request context of async app. Applicable within a AIAgent execute function.

      :returns: The caller for the current request being processed by the Agent.
      :rtype: str


   .. py:method:: _is_proxy_app_caller()

      Gets the caller for the current request context of async app. Applicable within a AIAgent execute function.

      :returns: True if the caller is proxy app.
      :rtype: bool


   .. py:method:: stream_message(message)

      Streams a message to the current request context. Applicable within a AIAgent execute function.
      If the request is from the abacus.ai app, the response will be streamed to the UI. otherwise would be logged info if used from notebook or python script.

      :param message: The message to be streamed.
      :type message: str


   .. py:method:: _stream_llm_call(**kwargs)


   .. py:method:: _call_aiagent_app_send_message(request_id, caller, message=None, llm_args=None, proxy_caller=False)

      Calls the AI Agent app send message endpoint.

      :param request_id: The request ID for the current request being processed by the Agent.
      :type request_id: str
      :param caller: The caller for the current request being processed by the Agent.
      :type caller: str
      :param message: The message to send to the AsyncApp.
      :type message: str
      :param llm_args: The LLM arguments to send to the AsyncApp.
      :type llm_args: dict

      :returns: The response from the AsyncApp.
      :rtype: str


   .. py:method:: _status_poll(url, wait_states, method, body = {}, headers = None, delay = 2, timeout = 600)


   .. py:method:: execute_data_query_using_llm(query, feature_group_ids, prompt_context = None, llm_name = None, temperature = None, preview = False, schema_document_retriever_ids = None, timeout=3600, delay=2)

      Execute a data query using a large language model.

      :param query: The natural language query to execute. The query is converted to a SQL query using the language model.
      :type query: str
      :param feature_group_ids: A list of feature group IDs that the query should be executed against.
      :type feature_group_ids: List[str]
      :param prompt_context: The context message used to construct the prompt for the language model. If not provide, a default context message is used.
      :type prompt_context: str
      :param llm_name: The name of the language model to use. If not provided, the default language model is used.
      :type llm_name: str
      :param temperature: The temperature to use for the language model if supported. If not provided, the default temperature is used.
      :type temperature: float
      :param preview: If True, a preview of the query execution is returned.
      :type preview: bool
      :param schema_document_retriever_ids: A list of document retrievers to retrieve schema information for the data query. Otherwise, they are retrieved from the feature group metadata.
      :type schema_document_retriever_ids: List[str]

      :returns: The result of the query execution. Execution results could be loaded as pandas using 'load_as_pandas', i.e., result.execution.load_as_pandas().
      :rtype: LlmExecutionResult


   .. py:method:: _get_doc_retriever_deployment_info(document_retriever_id)


   .. py:method:: get_matching_documents(document_retriever_id, query, filters = None, limit = None, result_columns = None, max_words = None, num_retrieval_margin_words = None, max_words_per_chunk = None, score_multiplier_column = None)

      Lookup document retrievers and return the matching documents from the document retriever deployed with given query.

      Original documents are splitted into chunks and stored in the document retriever. This lookup function will return the relevant chunks
      from the document retriever. The returned chunks could be expanded to include more words from the original documents and merged if they
      are overlapping, and permitted by the settings provided. The returned chunks are sorted by relevance.


      :param document_retriever_id: A unique string identifier associated with the document retriever.
      :type document_retriever_id: str
      :param query: The query to search for.
      :type query: str
      :param filters: A dictionary mapping column names to a list of values to restrict the retrieved search results.
      :type filters: dict
      :param limit: If provided, will limit the number of results to the value specified.
      :type limit: int
      :param result_columns: If provided, will limit the column properties present in each result to those specified in this list.
      :type result_columns: list
      :param max_words: If provided, will limit the total number of words in the results to the value specified.
      :type max_words: int
      :param num_retrieval_margin_words: If provided, will add this number of words from left and right of the returned chunks.
      :type num_retrieval_margin_words: int
      :param max_words_per_chunk: If provided, will limit the number of words in each chunk to the value specified. If the value provided is smaller than the actual size of chunk on disk, which is determined during document retriever creation, the actual size of chunk will be used. I.e, chunks looked up from document retrievers will not be split into smaller chunks during lookup due to this setting.
      :type max_words_per_chunk: int
      :param score_multiplier_column: If provided, will use the values in this column to modify the relevance score of the returned chunks. Values in this column must be numeric.
      :type score_multiplier_column: str

      :returns: The relevant documentation results found from the document retriever.
      :rtype: list[DocumentRetrieverLookupResult]


   .. py:method:: add_user_to_organization(email)

      Invite a user to your organization. This method will send the specified email address an invitation link to join your organization.

      :param email: The email address to invite to your organization.
      :type email: str


   .. py:method:: create_organization_group(group_name, permissions, default_group = False)

      Creates a new Organization Group.

      :param group_name: The name of the group.
      :type group_name: str
      :param permissions: The list of permissions to initialize the group with.
      :type permissions: list
      :param default_group: If True, this group will replace the current default group.
      :type default_group: bool

      :returns: Information about the created Organization Group.
      :rtype: OrganizationGroup


   .. py:method:: add_organization_group_permission(organization_group_id, permission)

      Adds a permission to the specified Organization Group.

      :param organization_group_id: Unique string identifier of the Organization Group.
      :type organization_group_id: str
      :param permission: Permission to add to the Organization Group.
      :type permission: str


   .. py:method:: remove_organization_group_permission(organization_group_id, permission)

      Removes a permission from the specified Organization Group.

      :param organization_group_id: Unique string identifier of the Organization Group.
      :type organization_group_id: str
      :param permission: The permission to remove from the Organization Group.
      :type permission: str


   .. py:method:: delete_organization_group(organization_group_id)

      Deletes the specified Organization Group

      :param organization_group_id: Unique string identifier of the organization group.
      :type organization_group_id: str


   .. py:method:: add_user_to_organization_group(organization_group_id, email)

      Adds a user to the specified Organization Group.

      :param organization_group_id: Unique string identifier of the Organization Group.
      :type organization_group_id: str
      :param email: Email of the user to be added to the group.
      :type email: str


   .. py:method:: remove_user_from_organization_group(organization_group_id, email)

      Removes a user from an Organization Group.

      :param organization_group_id: Unique string identifier of the Organization Group.
      :type organization_group_id: str
      :param email: Email of the user to remove.
      :type email: str


   .. py:method:: set_default_organization_group(organization_group_id)

      Sets the default Organization Group to which all new users joining an organization are automatically added.

      :param organization_group_id: Unique string identifier of the Organization Group.
      :type organization_group_id: str


   .. py:method:: delete_api_key(api_key_id)

      Delete a specified API key.

      :param api_key_id: The ID of the API key to delete.
      :type api_key_id: str


   .. py:method:: remove_user_from_organization(email)

      Removes the specified user from the organization. You can remove yourself; otherwise, you must be an organization administrator to use this method to remove other users from the organization.

      :param email: The email address of the user to remove from the organization.
      :type email: str


   .. py:method:: send_email(email, subject, body)

      Send an email to the specified email address with provided subject and body.

      :param email: The email address to send the email to.
      :type email: str
      :param subject: The subject of the email.
      :type subject: str
      :param body: The body of the email.
      :type body: str


   .. py:method:: create_deployment_webhook(deployment_id, endpoint, webhook_event_type, payload_template = None)

      Create a webhook attached to a given deployment ID.

      :param deployment_id: Unique string identifier for the deployment this webhook will attach to.
      :type deployment_id: str
      :param endpoint: URI that the webhook will send HTTP POST requests to.
      :type endpoint: str
      :param webhook_event_type: One of 'DEPLOYMENT_START', 'DEPLOYMENT_SUCCESS', or 'DEPLOYMENT_FAILED'.
      :type webhook_event_type: str
      :param payload_template: Template for the body of the HTTP POST requests. Defaults to {}.
      :type payload_template: dict

      :returns: The webhook attached to the deployment.
      :rtype: Webhook


   .. py:method:: update_webhook(webhook_id, endpoint = None, webhook_event_type = None, payload_template = None)

      Update the webhook

      :param webhook_id: The ID of the webhook to be updated.
      :type webhook_id: str
      :param endpoint: If provided, changes the webhook's endpoint.
      :type endpoint: str
      :param webhook_event_type: If provided, changes the event type.
      :type webhook_event_type: str
      :param payload_template: If provided, changes the payload template.
      :type payload_template: dict


   .. py:method:: delete_webhook(webhook_id)

      Delete the webhook

      :param webhook_id: Unique identifier of the target webhook.
      :type webhook_id: str


   .. py:method:: create_project(name, use_case)

      Creates a project with the specified project name and use case. Creating a project creates a container for all datasets and models associated with a particular problem/project. For example, if you want to create a model to detect fraud, you need to first create a project, upload datasets, create feature groups, and then create one or more models to get predictions for your use case.

      :param name: The project's name.
      :type name: str
      :param use_case: The use case that the project solves. Refer to our [guide on use cases](https://api.abacus.ai/app/help/useCases) for further details of each use case. The following enums are currently available for you to choose from:  LANGUAGE_DETECTION,  NLP_SENTIMENT,  NLP_SEARCH,  NLP_CHAT,  CHAT_LLM,  NLP_SENTENCE_BOUNDARY_DETECTION,  NLP_CLASSIFICATION,  NLP_SUMMARIZATION,  NLP_DOCUMENT_VISUALIZATION,  AI_AGENT,  EMBEDDINGS_ONLY,  MODEL_WITH_EMBEDDINGS,  TORCH_MODEL,  TORCH_MODEL_WITH_EMBEDDINGS,  PYTHON_MODEL,  NOTEBOOK_PYTHON_MODEL,  DOCKER_MODEL,  DOCKER_MODEL_WITH_EMBEDDINGS,  CUSTOMER_CHURN,  ENERGY,  EVENT_ANOMALY_DETECTION,  FINANCIAL_METRICS,  CUMULATIVE_FORECASTING,  FRAUD_ACCOUNT,  FRAUD_THREAT,  FRAUD_TRANSACTIONS,  OPERATIONS_CLOUD,  CLOUD_SPEND,  TIMESERIES_ANOMALY_DETECTION,  OPERATIONS_MAINTENANCE,  OPERATIONS_INCIDENT,  PERS_PROMOTIONS,  PREDICTING,  FEATURE_STORE,  RETAIL,  SALES_FORECASTING,  SALES_SCORING,  FEED_RECOMMEND,  USER_RANKINGS,  NAMED_ENTITY_RECOGNITION,  USER_RECOMMENDATIONS,  USER_RELATED,  VISION,  VISION_REGRESSION,  VISION_OBJECT_DETECTION,  FEATURE_DRIFT,  SCHEDULING,  GENERIC_FORECASTING,  PRETRAINED_IMAGE_TEXT_DESCRIPTION,  PRETRAINED_SPEECH_RECOGNITION,  PRETRAINED_STYLE_TRANSFER,  PRETRAINED_TEXT_TO_IMAGE_GENERATION,  PRETRAINED_OCR_DOCUMENT_TO_TEXT,  THEME_ANALYSIS,  CLUSTERING,  CLUSTERING_TIMESERIES,  FINETUNED_LLM,  PRETRAINED_INSTRUCT_PIX2PIX,  PRETRAINED_TEXT_CLASSIFICATION.
      :type use_case: str

      :returns: This object represents the newly created project.
      :rtype: Project


   .. py:method:: rename_project(project_id, name)

      This method renames a project after it is created.

      :param project_id: The unique identifier for the project.
      :type project_id: str
      :param name: The new name for the project.
      :type name: str


   .. py:method:: delete_project(project_id)

      Delete a specified project from your organization.

      This method deletes the project, its associated trained models, and deployments. The datasets attached to the specified project remain available for use with other projects in the organization.

      This method will not delete a project that contains active deployments. Ensure that all active deployments are stopped before using the delete option.

      Note: All projects, models, and deployments cannot be recovered once they are deleted.


      :param project_id: The unique ID of the project to delete.
      :type project_id: str


   .. py:method:: add_project_tags(project_id, tags)

      This method adds a tag to a project.

      :param project_id: The unique identifier for the project.
      :type project_id: str
      :param tags: The tags to add to the project.
      :type tags: list


   .. py:method:: remove_project_tags(project_id, tags)

      This method removes a tag from a project.

      :param project_id: The unique identifier for the project.
      :type project_id: str
      :param tags: The tags to remove from the project.
      :type tags: list


   .. py:method:: add_feature_group_to_project(feature_group_id, project_id, feature_group_type = 'CUSTOM_TABLE')

      Adds a feature group to a project.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_type: The feature group type of the feature group, based on the use case under which the feature group is being created.
      :type feature_group_type: str


   .. py:method:: set_project_feature_group_config(feature_group_id, project_id, project_config = None)

      Sets a feature group's project config

      :param feature_group_id: Unique string identifier for the feature group.
      :type feature_group_id: str
      :param project_id: Unique string identifier for the project.
      :type project_id: str
      :param project_config: JSON object for the feature group's project configuration.
      :type project_config: dict


   .. py:method:: remove_feature_group_from_project(feature_group_id, project_id)

      Removes a feature group from a project.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param project_id: The unique ID associated with the project.
      :type project_id: str


   .. py:method:: set_feature_group_type(feature_group_id, project_id, feature_group_type = 'CUSTOM_TABLE')

      Update the feature group type in a project. The feature group must already be added to the project.

      :param feature_group_id: Unique identifier associated with the feature group.
      :type feature_group_id: str
      :param project_id: Unique identifier associated with the project.
      :type project_id: str
      :param feature_group_type: The feature group type to set the feature group as.
      :type feature_group_type: str


   .. py:method:: set_feature_mapping(project_id, feature_group_id, feature_name, feature_mapping = None, nested_column_name = None)

      Set a column's feature mapping. If the column mapping is single-use and already set in another column in this feature group, this call will first remove the other column's mapping and move it to this column.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param feature_name: The name of the feature.
      :type feature_name: str
      :param feature_mapping: The mapping of the feature in the feature group.
      :type feature_mapping: str
      :param nested_column_name: The name of the nested column if the input feature is part of a nested feature group for the given feature_group_id.
      :type nested_column_name: str

      :returns: A list of objects that describes the resulting feature group's schema after the feature's featureMapping is set.
      :rtype: list[Feature]


   .. py:method:: add_annotation(annotation, feature_group_id, feature_name, doc_id = None, feature_group_row_identifier = None, annotation_source = 'ui', status = None, comments = None, project_id = None, save_metadata = False, pages = None)

      Add an annotation entry to the database.

      :param annotation: The annotation to add. Format of the annotation is determined by its annotation type.
      :type annotation: dict
      :param feature_group_id: The ID of the feature group the annotation is on.
      :type feature_group_id: str
      :param feature_name: The name of the feature the annotation is on.
      :type feature_name: str
      :param doc_id: The ID of the primary document the annotation is on. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type doc_id: str
      :param feature_group_row_identifier: The key value of the feature group row the annotation is on (cast to string). Usually the feature group's primary / identifier key value. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type feature_group_row_identifier: str
      :param annotation_source: Indicator of whether the annotation came from the UI, bulk upload, etc.
      :type annotation_source: str
      :param status: The status of the annotation. Can be one of 'todo', 'in_progress', 'done'. This is optional.
      :type status: str
      :param comments: Comments for the annotation. This is a dictionary of feature name to the corresponding comment. This is optional.
      :type comments: dict
      :param project_id: The ID of the project that the annotation is associated with. This is optional.
      :type project_id: str
      :param save_metadata: Whether to save the metadata for the annotation. This is optional.
      :type save_metadata: bool
      :param pages: pages (list): List of page numbers to consider while processing the annotation. This is optional. doc_id must be provided if pages is provided.
      :type pages: list

      :returns: The annotation entry that was added.
      :rtype: AnnotationEntry


   .. py:method:: describe_annotation(feature_group_id, feature_name = None, doc_id = None, feature_group_row_identifier = None)

      Get the latest annotation entry for a given feature group, feature, and document.

      :param feature_group_id: The ID of the feature group the annotation is on.
      :type feature_group_id: str
      :param feature_name: The name of the feature the annotation is on.
      :type feature_name: str
      :param doc_id: The ID of the primary document the annotation is on. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type doc_id: str
      :param feature_group_row_identifier: The key value of the feature group row the annotation is on (cast to string). Usually the feature group's primary / identifier key value. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type feature_group_row_identifier: str

      :returns: The latest annotation entry for the given feature group, feature, document, and/or annotation key value.
      :rtype: AnnotationEntry


   .. py:method:: update_annotation_status(feature_group_id, feature_name, status, doc_id = None, feature_group_row_identifier = None, save_metadata = False)

      Update the status of an annotation entry.

      :param feature_group_id: The ID of the feature group the annotation is on.
      :type feature_group_id: str
      :param feature_name: The name of the feature the annotation is on.
      :type feature_name: str
      :param status: The new status of the annotation. Must be one of the following: 'TODO', 'IN_PROGRESS', 'DONE'.
      :type status: str
      :param doc_id: The ID of the primary document the annotation is on. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type doc_id: str
      :param feature_group_row_identifier: The key value of the feature group row the annotation is on (cast to string). Usually the feature group's primary / identifier key value. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type feature_group_row_identifier: str
      :param save_metadata: If True, save the metadata for the annotation entry.
      :type save_metadata: bool

      :returns: The updated annotation entry.
      :rtype: AnnotationEntry


   .. py:method:: get_document_to_annotate(feature_group_id, project_id, feature_name, feature_group_row_identifier = None, get_previous = False)

      Get an available document that needs to be annotated for a annotation feature group.

      :param feature_group_id: The ID of the feature group the annotation is on.
      :type feature_group_id: str
      :param project_id: The ID of the project that the annotation is associated with.
      :type project_id: str
      :param feature_name: The name of the feature the annotation is on.
      :type feature_name: str
      :param feature_group_row_identifier: The key value of the feature group row the annotation is on (cast to string). Usually the primary key value. If provided, fetch the immediate next (or previous) available document.
      :type feature_group_row_identifier: str
      :param get_previous: If True, get the previous document instead of the next document. Applicable if feature_group_row_identifier is provided.
      :type get_previous: bool

      :returns: The document to annotate.
      :rtype: AnnotationDocument


   .. py:method:: import_annotation_labels(feature_group_id, file, annotation_type)

      Imports annotation labels from csv file. All valid values in the file will be imported as labels (including header row if present).

      :param feature_group_id: The unique string identifier of the feature group.
      :type feature_group_id: str
      :param file: The file to import. Must be a csv file.
      :type file: io.TextIOBase
      :param annotation_type: The type of the annotation.
      :type annotation_type: str

      :returns: The annotation config for the feature group.
      :rtype: AnnotationConfig


   .. py:method:: create_feature_group(table_name, sql, description = None)

      Creates a new FeatureGroup from a SQL statement.

      :param table_name: The unique name to be given to the FeatureGroup. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param sql: Input SQL statement for forming the FeatureGroup.
      :type sql: str
      :param description: The description about the FeatureGroup.
      :type description: str

      :returns: The created FeatureGroup.
      :rtype: FeatureGroup


   .. py:method:: create_feature_group_from_template(table_name, feature_group_template_id, template_bindings = None, should_attach_feature_group_to_template = True, description = None)

      Creates a new feature group from a SQL statement.

      :param table_name: The unique name to be given to the feature group. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param feature_group_template_id: The unique ID associated with the template that will be used to create this feature group.
      :type feature_group_template_id: str
      :param template_bindings: Variable bindings that override the template's variable values.
      :type template_bindings: list
      :param should_attach_feature_group_to_template: Set to `False` to create a feature group but not leave it attached to the template that created it.
      :type should_attach_feature_group_to_template: bool
      :param description: A user-friendly description of this feature group.
      :type description: str

      :returns: The created feature group.
      :rtype: FeatureGroup


   .. py:method:: create_feature_group_from_function(table_name, function_source_code = None, function_name = None, input_feature_groups = None, description = None, cpu_size = None, memory = None, package_requirements = None, use_original_csv_names = False, python_function_name = None, python_function_bindings = None, use_gpu = None)

      Creates a new feature in a Feature Group from user-provided code. Currently supported code languages are Python.

      If a list of input feature groups are supplied, we will provide DataFrames (pandas, in the case of Python) with the materialized feature groups for those input feature groups as arguments to the function.

      This method expects the source code to be a valid language source file containing a function. This function needs to return a DataFrame when executed; this DataFrame will be used as the materialized version of this feature group table.


      :param table_name: The unique name to be given to the feature group. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param function_source_code: Contents of a valid source code file in a supported Feature Group specification language (currently only Python). The source code should contain a function called function_name. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type function_source_code: str
      :param function_name: Name of the function found in the source code that will be executed (on the optional inputs) to materialize this feature group.
      :type function_name: str
      :param input_feature_groups: List of feature group names that are supplied to the function as parameters. Each of the parameters are materialized Dataframes (same type as the functions return value).
      :type input_feature_groups: list
      :param description: The description for this feature group.
      :type description: str
      :param cpu_size: Size of the CPU for the feature group function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the feature group function.
      :type memory: int
      :param package_requirements: List of package requirements for the feature group function. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: list
      :param use_original_csv_names: Defaults to False, if set it uses the original column names for input feature groups from CSV datasets.
      :type use_original_csv_names: bool
      :param python_function_name: Name of Python Function that contains the source code and function arguments.
      :type python_function_name: str
      :param python_function_bindings: List of arguments to be supplied to the function as parameters in the format [{'name': 'function_argument', 'variable_type': 'FEATURE_GROUP', 'value': 'name_of_feature_group'}].
      :type python_function_bindings: list
      :param use_gpu: Whether the feature group needs a gpu or not. Otherwise default to CPU.
      :type use_gpu: bool

      :returns: The created feature group
      :rtype: FeatureGroup


   .. py:method:: create_sampling_feature_group(feature_group_id, table_name, sampling_config, description = None)

      Creates a new Feature Group defined as a sample of rows from another Feature Group.

      For efficiency, sampling is approximate unless otherwise specified. (e.g. the number of rows may vary slightly from what was requested).


      :param feature_group_id: The unique ID associated with the pre-existing Feature Group that will be sampled by this new Feature Group. i.e. the input for sampling.
      :type feature_group_id: str
      :param table_name: The unique name to be given to this sampling Feature Group. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param sampling_config: Dictionary defining the sampling method and its parameters.
      :type sampling_config: SamplingConfig
      :param description: A human-readable description of this Feature Group.
      :type description: str

      :returns: The created Feature Group.
      :rtype: FeatureGroup


   .. py:method:: create_merge_feature_group(source_feature_group_id, table_name, merge_config, description = None)

      Creates a new feature group defined as the union of other feature group versions.

              Args:
                  source_feature_group_id (str): Unique string identifier corresponding to the dataset feature group that will have its versions merged into this feature group.
                  table_name (str): Unique string identifier to be given to this merge feature group. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
                  merge_config (MergeConfig): JSON object defining the merging method and its parameters.
                  description (str): Human-readable description of this feature group.

              Returns:
                  FeatureGroup: The created feature group.
      Description:
      Creates a new feature group defined as the union of other feature group versions.


   .. py:method:: create_operator_feature_group(source_feature_group_id, table_name, operator_config, description = None)

      Creates a new Feature Group defined by a pre-defined operator applied to another Feature Group.

      :param source_feature_group_id: Unique string identifier corresponding to the Feature Group to which the operator will be applied.
      :type source_feature_group_id: str
      :param table_name: Unique string identifier for the operator Feature Group. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param operator_config: JSON object (aka map) defining the operator and its parameters.
      :type operator_config: dict
      :param description: Human-readable description of the Feature Group.
      :type description: str

      :returns: The created Feature Group.
      :rtype: FeatureGroup


   .. py:method:: create_snapshot_feature_group(feature_group_version, table_name)

      Creates a Snapshot Feature Group corresponding to a specific Feature Group version.

      :param feature_group_version: Unique string identifier associated with the Feature Group version being snapshotted.
      :type feature_group_version: str
      :param table_name: Name for the newly created Snapshot Feature Group table. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str

      :returns: Feature Group corresponding to the newly created Snapshot.
      :rtype: FeatureGroup


   .. py:method:: create_online_feature_group(table_name, primary_key, description = None)

      Creates an Online Feature Group.

      :param table_name: Name for the newly created feature group. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param primary_key: The primary key for indexing the online feature group.
      :type primary_key: str
      :param description: Human-readable description of the Feature Group.
      :type description: str

      :returns: The created online feature group.
      :rtype: FeatureGroup


   .. py:method:: set_feature_group_sampling_config(feature_group_id, sampling_config)

      Set a FeatureGroup’s sampling to the config values provided, so that the rows the FeatureGroup returns will be a sample of those it would otherwise have returned.

      :param feature_group_id: The unique identifier associated with the FeatureGroup.
      :type feature_group_id: str
      :param sampling_config: A JSON string object specifying the sampling method and parameters specific to that sampling method. An empty sampling_config indicates no sampling.
      :type sampling_config: SamplingConfig

      :returns: The updated FeatureGroup.
      :rtype: FeatureGroup


   .. py:method:: set_feature_group_merge_config(feature_group_id, merge_config)

      Set a MergeFeatureGroup’s merge config to the values provided, so that the feature group only returns a bounded range of an incremental dataset.

      :param feature_group_id: Unique identifier associated with the feature group.
      :type feature_group_id: str
      :param merge_config: JSON object string specifying the merge rule. An empty merge_config will default to only including the latest dataset version.
      :type merge_config: MergeConfig

      :returns: The updated FeatureGroup.
      :rtype: FeatureGroup


   .. py:method:: set_feature_group_operator_config(feature_group_id, operator_config)

      Set a OperatorFeatureGroup’s operator config to the values provided.

      :param feature_group_id: A unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param operator_config: A dictionary object specifying the pre-defined operations.
      :type operator_config: dict


   .. py:method:: set_feature_group_schema(feature_group_id, schema)

      Creates a new schema and points the feature group to the new feature group schema ID.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param schema: JSON string containing an array of objects with 'name' and 'dataType' properties.
      :type schema: list


   .. py:method:: create_feature(feature_group_id, name, select_expression)

      Creates a new feature in a Feature Group from a SQL select statement.

      :param feature_group_id: The unique ID associated with the Feature Group.
      :type feature_group_id: str
      :param name: The name of the feature to add.
      :type name: str
      :param select_expression: SQL SELECT expression to create the feature.
      :type select_expression: str

      :returns: A Feature Group object with the newly added feature.
      :rtype: FeatureGroup


   .. py:method:: add_feature_group_tag(feature_group_id, tag)

      Adds a tag to the feature group

      :param feature_group_id: Unique identifier of the feature group.
      :type feature_group_id: str
      :param tag: The tag to add to the feature group.
      :type tag: str


   .. py:method:: remove_feature_group_tag(feature_group_id, tag)

      Removes a tag from the specified feature group.

      :param feature_group_id: Unique string identifier of the feature group.
      :type feature_group_id: str
      :param tag: The tag to remove from the feature group.
      :type tag: str


   .. py:method:: add_annotatable_feature(feature_group_id, name, annotation_type)

      Add an annotatable feature in a Feature Group

      :param feature_group_id: The unique string identifier for the feature group.
      :type feature_group_id: str
      :param name: The name of the feature to add.
      :type name: str
      :param annotation_type: The type of annotation to set.
      :type annotation_type: str

      :returns: The feature group after the feature has been set
      :rtype: FeatureGroup


   .. py:method:: set_feature_as_annotatable_feature(feature_group_id, feature_name, annotation_type, feature_group_row_identifier_feature = None, doc_id_feature = None)

      Sets an existing feature as an annotatable feature (Feature that can be annotated).

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param feature_name: The name of the feature to set as annotatable.
      :type feature_name: str
      :param annotation_type: The type of annotation label to add.
      :type annotation_type: str
      :param feature_group_row_identifier_feature: The key value of the feature group row the annotation is on (cast to string) and uniquely identifies the feature group row. At least one of the doc_id or key value must be provided so that the correct annotation can be identified.
      :type feature_group_row_identifier_feature: str
      :param doc_id_feature: The name of the document ID feature.
      :type doc_id_feature: str

      :returns: A feature group object with the newly added annotatable feature.
      :rtype: FeatureGroup


   .. py:method:: set_annotation_status_feature(feature_group_id, feature_name)

      Sets a feature as the annotation status feature for a feature group.

      :param feature_group_id: The ID of the feature group.
      :type feature_group_id: str
      :param feature_name: The name of the feature to set as the annotation status feature.
      :type feature_name: str

      :returns: The updated feature group.
      :rtype: FeatureGroup


   .. py:method:: unset_feature_as_annotatable_feature(feature_group_id, feature_name)

      Unsets a feature as annotatable

      :param feature_group_id: The unique string identifier of the feature group.
      :type feature_group_id: str
      :param feature_name: The name of the feature to unset.
      :type feature_name: str

      :returns: The feature group after unsetting the feature
      :rtype: FeatureGroup


   .. py:method:: add_feature_group_annotation_label(feature_group_id, label_name, annotation_type, label_definition = None)

      Adds an annotation label

      :param feature_group_id: The unique string identifier of the feature group.
      :type feature_group_id: str
      :param label_name: The name of the label.
      :type label_name: str
      :param annotation_type: The type of the annotation to set.
      :type annotation_type: str
      :param label_definition: the definition of the label.
      :type label_definition: str

      :returns: The feature group after adding the annotation label
      :rtype: FeatureGroup


   .. py:method:: remove_feature_group_annotation_label(feature_group_id, label_name)

      Removes an annotation label

      :param feature_group_id: The unique string identifier of the feature group.
      :type feature_group_id: str
      :param label_name: The name of the label to remove.
      :type label_name: str

      :returns: The feature group after adding the annotation label
      :rtype: FeatureGroup


   .. py:method:: add_feature_tag(feature_group_id, feature, tag)

      Adds a tag on a feature

      :param feature_group_id: The unique string identifier of the feature group.
      :type feature_group_id: str
      :param feature: The feature to set the tag on.
      :type feature: str
      :param tag: The tag to set on the feature.
      :type tag: str


   .. py:method:: remove_feature_tag(feature_group_id, feature, tag)

      Removes a tag from a feature

      :param feature_group_id: The unique string identifier of the feature group.
      :type feature_group_id: str
      :param feature: The feature to remove the tag from.
      :type feature: str
      :param tag: The tag to remove.
      :type tag: str


   .. py:method:: create_nested_feature(feature_group_id, nested_feature_name, table_name, using_clause, where_clause = None, order_clause = None)

      Creates a new nested feature in a feature group from a SQL statement.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param nested_feature_name: The name of the feature.
      :type nested_feature_name: str
      :param table_name: The table name of the feature group to nest. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param using_clause: The SQL join column or logic to join the nested table with the parent.
      :type using_clause: str
      :param where_clause: A SQL WHERE statement to filter the nested rows.
      :type where_clause: str
      :param order_clause: A SQL clause to order the nested rows.
      :type order_clause: str

      :returns: A feature group object with the newly added nested feature.
      :rtype: FeatureGroup


   .. py:method:: update_nested_feature(feature_group_id, nested_feature_name, table_name = None, using_clause = None, where_clause = None, order_clause = None, new_nested_feature_name = None)

      Updates a previously existing nested feature in a feature group.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param nested_feature_name: The name of the feature to be updated.
      :type nested_feature_name: str
      :param table_name: The name of the table. Can be up to 120 characters long and can only contain alphanumeric characters and underscores.
      :type table_name: str
      :param using_clause: The SQL join column or logic to join the nested table with the parent.
      :type using_clause: str
      :param where_clause: An SQL WHERE statement to filter the nested rows.
      :type where_clause: str
      :param order_clause: An SQL clause to order the nested rows.
      :type order_clause: str
      :param new_nested_feature_name: New name for the nested feature.
      :type new_nested_feature_name: str

      :returns: A feature group object with the updated nested feature.
      :rtype: FeatureGroup


   .. py:method:: delete_nested_feature(feature_group_id, nested_feature_name)

      Delete a nested feature.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param nested_feature_name: The name of the feature to be deleted.
      :type nested_feature_name: str

      :returns: A feature group object without the specified nested feature.
      :rtype: FeatureGroup


   .. py:method:: create_point_in_time_feature(feature_group_id, feature_name, history_table_name, aggregation_keys, timestamp_key, historical_timestamp_key, expression, lookback_window_seconds = None, lookback_window_lag_seconds = 0, lookback_count = None, lookback_until_position = 0)

      Creates a new point in time feature in a feature group using another historical feature group, window spec, and aggregate expression.

      We use the aggregation keys and either the lookbackWindowSeconds or the lookbackCount values to perform the window aggregation for every row in the current feature group.

      If the window is specified in seconds, then all rows in the history table which match the aggregation keys and with historicalTimeFeature greater than or equal to lookbackStartCount and less than the value of the current rows timeFeature are considered. An optional lookbackWindowLagSeconds (+ve or -ve) can be used to offset the current value of the timeFeature. If this value is negative, we will look at the future rows in the history table, so care must be taken to ensure that these rows are available in the online context when we are performing a lookup on this feature group. If the window is specified in counts, then we order the historical table rows aligning by time and consider rows from the window where the rank order is greater than or equal to lookbackCount and includes the row just prior to the current one. The lag is specified in terms of positions using lookbackUntilPosition.


      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param feature_name: The name of the feature to create.
      :type feature_name: str
      :param history_table_name: The table name of the history table.
      :type history_table_name: str
      :param aggregation_keys: List of keys to use for joining the historical table and performing the window aggregation.
      :type aggregation_keys: list
      :param timestamp_key: Name of feature which contains the timestamp value for the point in time feature.
      :type timestamp_key: str
      :param historical_timestamp_key: Name of feature which contains the historical timestamp.
      :type historical_timestamp_key: str
      :param expression: SQL aggregate expression which can convert a sequence of rows into a scalar value.
      :type expression: str
      :param lookback_window_seconds: If window is specified in terms of time, number of seconds in the past from the current time for start of the window.
      :type lookback_window_seconds: float
      :param lookback_window_lag_seconds: Optional lag to offset the closest point for the window. If it is positive, we delay the start of window. If it is negative, we are looking at the "future" rows in the history table.
      :type lookback_window_lag_seconds: float
      :param lookback_count: If window is specified in terms of count, the start position of the window (0 is the current row).
      :type lookback_count: int
      :param lookback_until_position: Optional lag to offset the closest point for the window. If it is positive, we delay the start of window by that many rows. If it is negative, we are looking at those many "future" rows in the history table.
      :type lookback_until_position: int

      :returns: A feature group object with the newly added nested feature.
      :rtype: FeatureGroup


   .. py:method:: update_point_in_time_feature(feature_group_id, feature_name, history_table_name = None, aggregation_keys = None, timestamp_key = None, historical_timestamp_key = None, expression = None, lookback_window_seconds = None, lookback_window_lag_seconds = None, lookback_count = None, lookback_until_position = None, new_feature_name = None)

      Updates an existing Point-in-Time (PiT) feature in a feature group. See `createPointInTimeFeature` for detailed semantics.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param feature_name: The name of the feature.
      :type feature_name: str
      :param history_table_name: The table name of the history table. If not specified, we use the current table to do a self join.
      :type history_table_name: str
      :param aggregation_keys: List of keys to use for joining the historical table and performing the window aggregation.
      :type aggregation_keys: list
      :param timestamp_key: Name of the feature which contains the timestamp value for the PiT feature.
      :type timestamp_key: str
      :param historical_timestamp_key: Name of the feature which contains the historical timestamp.
      :type historical_timestamp_key: str
      :param expression: SQL Aggregate expression which can convert a sequence of rows into a scalar value.
      :type expression: str
      :param lookback_window_seconds: If the window is specified in terms of time, the number of seconds in the past from the current time for the start of the window.
      :type lookback_window_seconds: float
      :param lookback_window_lag_seconds: Optional lag to offset the closest point for the window. If it is positive, we delay the start of the window. If it is negative, we are looking at the "future" rows in the history table.
      :type lookback_window_lag_seconds: float
      :param lookback_count: If the window is specified in terms of count, the start position of the window (0 is the current row).
      :type lookback_count: int
      :param lookback_until_position: Optional lag to offset the closest point for the window. If it is positive, we delay the start of the window by that many rows. If it is negative, we are looking at those many "future" rows in the history table.
      :type lookback_until_position: int
      :param new_feature_name: New name for the PiT feature.
      :type new_feature_name: str

      :returns: A feature group object with the newly added nested feature.
      :rtype: FeatureGroup


   .. py:method:: create_point_in_time_group(feature_group_id, group_name, window_key, aggregation_keys, history_table_name = None, history_window_key = None, history_aggregation_keys = None, lookback_window = None, lookback_window_lag = 0, lookback_count = None, lookback_until_position = 0)

      Create a Point-in-Time Group

      :param feature_group_id: The unique ID associated with the feature group to add the point in time group to.
      :type feature_group_id: str
      :param group_name: The name of the point in time group.
      :type group_name: str
      :param window_key: Name of feature to use for ordering the rows on the source table.
      :type window_key: str
      :param aggregation_keys: List of keys to perform on the source table for the window aggregation.
      :type aggregation_keys: list
      :param history_table_name: The table to use for aggregating, if not provided, the source table will be used.
      :type history_table_name: str
      :param history_window_key: Name of feature to use for ordering the rows on the history table. If not provided, the windowKey from the source table will be used.
      :type history_window_key: str
      :param history_aggregation_keys: List of keys to use for join the historical table and performing the window aggregation. If not provided, the aggregationKeys from the source table will be used. Must be the same length and order as the source table's aggregationKeys.
      :type history_aggregation_keys: list
      :param lookback_window: Number of seconds in the past from the current time for the start of the window. If 0, the lookback will include all rows.
      :type lookback_window: float
      :param lookback_window_lag: Optional lag to offset the closest point for the window. If it is positive, the start of the window is delayed. If it is negative, "future" rows in the history table are used.
      :type lookback_window_lag: float
      :param lookback_count: If window is specified in terms of count, the start position of the window (0 is the current row).
      :type lookback_count: int
      :param lookback_until_position: Optional lag to offset the closest point for the window. If it is positive, the start of the window is delayed by that many rows. If it is negative, those many "future" rows in the history table are used.
      :type lookback_until_position: int

      :returns: The feature group after the point in time group has been created.
      :rtype: FeatureGroup


   .. py:method:: generate_point_in_time_features(feature_group_id, group_name, columns, window_functions, prefix = None)

      Generates and adds PIT features given the selected columns to aggregate over, and the operations to include.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param group_name: Name of the point-in-time group.
      :type group_name: str
      :param columns: List of columns to generate point-in-time features for.
      :type columns: list
      :param window_functions: List of window functions to operate on.
      :type window_functions: list
      :param prefix: Prefix for generated features, defaults to group name
      :type prefix: str

      :returns: Feature group object with newly added point-in-time features.
      :rtype: FeatureGroup


   .. py:method:: update_point_in_time_group(feature_group_id, group_name, window_key = None, aggregation_keys = None, history_table_name = None, history_window_key = None, history_aggregation_keys = None, lookback_window = None, lookback_window_lag = None, lookback_count = None, lookback_until_position = None)

      Update Point-in-Time Group

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param group_name: The name of the point-in-time group.
      :type group_name: str
      :param window_key: Name of feature which contains the timestamp value for the point-in-time feature.
      :type window_key: str
      :param aggregation_keys: List of keys to use for joining the historical table and performing the window aggregation.
      :type aggregation_keys: list
      :param history_table_name: The table to use for aggregating, if not provided, the source table will be used.
      :type history_table_name: str
      :param history_window_key: Name of feature to use for ordering the rows on the history table. If not provided, the windowKey from the source table will be used.
      :type history_window_key: str
      :param history_aggregation_keys: List of keys to use for joining the historical table and performing the window aggregation. If not provided, the aggregationKeys from the source table will be used. Must be the same length and order as the source table's aggregationKeys.
      :type history_aggregation_keys: list
      :param lookback_window: Number of seconds in the past from the current time for the start of the window.
      :type lookback_window: float
      :param lookback_window_lag: Optional lag to offset the closest point for the window. If it is positive, the start of the window is delayed. If it is negative, future rows in the history table are looked at.
      :type lookback_window_lag: float
      :param lookback_count: If window is specified in terms of count, the start position of the window (0 is the current row).
      :type lookback_count: int
      :param lookback_until_position: Optional lag to offset the closest point for the window. If it is positive, the start of the window is delayed by that many rows. If it is negative, those many future rows in the history table are looked at.
      :type lookback_until_position: int

      :returns: The feature group after the update has been applied.
      :rtype: FeatureGroup


   .. py:method:: delete_point_in_time_group(feature_group_id, group_name)

      Delete point in time group

      :param feature_group_id: The unique identifier associated with the feature group.
      :type feature_group_id: str
      :param group_name: The name of the point in time group.
      :type group_name: str

      :returns: The feature group after the point in time group has been deleted.
      :rtype: FeatureGroup


   .. py:method:: create_point_in_time_group_feature(feature_group_id, group_name, name, expression)

      Create point in time group feature

      :param feature_group_id: A unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param group_name: The name of the point-in-time group.
      :type group_name: str
      :param name: The name of the feature to add to the point-in-time group.
      :type name: str
      :param expression: A SQL aggregate expression which can convert a sequence of rows into a scalar value.
      :type expression: str

      :returns: The feature group after the update has been applied.
      :rtype: FeatureGroup


   .. py:method:: update_point_in_time_group_feature(feature_group_id, group_name, name, expression)

      Update a feature's SQL expression in a point in time group

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param group_name: The name of the point-in-time group.
      :type group_name: str
      :param name: The name of the feature to add to the point-in-time group.
      :type name: str
      :param expression: SQL aggregate expression which can convert a sequence of rows into a scalar value.
      :type expression: str

      :returns: The feature group after the update has been applied.
      :rtype: FeatureGroup


   .. py:method:: set_feature_type(feature_group_id, feature, feature_type)

      Set the type of a feature in a feature group. Specify the feature group ID, feature name, and feature type, and the method will return the new column with the changes reflected.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param feature: The name of the feature.
      :type feature: str
      :param feature_type: The machine learning type of the data in the feature.
      :type feature_type: str

      :returns: The feature group after the data_type is applied.
      :rtype: Schema


   .. py:method:: invalidate_streaming_feature_group_data(feature_group_id, invalid_before_timestamp)

      Invalidates all streaming data with timestamp before invalidBeforeTimestamp

      :param feature_group_id: Unique string identifier of the streaming feature group to record data to
      :type feature_group_id: str
      :param invalid_before_timestamp: Unix timestamp; any data with a timestamp before this time will be invalidated
      :type invalid_before_timestamp: int


   .. py:method:: concatenate_feature_group_data(feature_group_id, source_feature_group_id, merge_type = 'UNION', replace_until_timestamp = None, skip_materialize = False)

      Concatenates data from one Feature Group to another. Feature Groups can be merged if their schemas are compatible, they have the special `updateTimestampKey` column, and (if set) the `primaryKey` column. The second operand in the concatenate operation will be appended to the first operand (merge target).

      :param feature_group_id: The destination Feature Group.
      :type feature_group_id: str
      :param source_feature_group_id: The Feature Group to concatenate with the destination Feature Group.
      :type source_feature_group_id: str
      :param merge_type: `UNION` or `INTERSECTION`.
      :type merge_type: str
      :param replace_until_timestamp: The UNIX timestamp to specify the point until which we will replace data from the source Feature Group.
      :type replace_until_timestamp: int
      :param skip_materialize: If `True`, will not materialize the concatenated Feature Group.
      :type skip_materialize: bool


   .. py:method:: remove_concatenation_config(feature_group_id)

      Removes the concatenation config on a destination feature group.

      :param feature_group_id: Unique identifier of the destination feature group to remove the concatenation configuration from.
      :type feature_group_id: str


   .. py:method:: set_feature_group_indexing_config(feature_group_id, primary_key = None, update_timestamp_key = None, lookup_keys = None)

      Sets various attributes of the feature group used for primary key, deployment lookups and streaming updates.

      :param feature_group_id: Unique string identifier for the feature group.
      :type feature_group_id: str
      :param primary_key: Name of the feature which defines the primary key of the feature group.
      :type primary_key: str
      :param update_timestamp_key: Name of the feature which defines the update timestamp of the feature group. Used in concatenation and primary key deduplication.
      :type update_timestamp_key: str
      :param lookup_keys: List of feature names which can be used in the lookup API to restrict the computation to a set of dataset rows. These feature names have to correspond to underlying dataset columns.
      :type lookup_keys: list


   .. py:method:: execute_async_feature_group_operation(query = None, fix_query_on_error = False)

      Starts the execution of fg operation

      :param query: The SQL to be executed.
      :type query: str
      :param fix_query_on_error: If enabled, SQL query is auto fixed if parsing fails.
      :type fix_query_on_error: bool

      :returns: A dict that contains the execution status
      :rtype: ExecuteFeatureGroupOperation


   .. py:method:: describe_async_feature_group_operation(feature_group_operation_run_id)

      Gets the status of the execution of fg operation

      :param feature_group_operation_run_id: The unique ID associated with the execution.
      :type feature_group_operation_run_id: str

      :returns: A dict that contains the execution status
      :rtype: ExecuteFeatureGroupOperation


   .. py:method:: update_feature_group(feature_group_id, description = None)

      Modify an existing Feature Group.

      :param feature_group_id: Unique identifier associated with the Feature Group.
      :type feature_group_id: str
      :param description: Description of the Feature Group.
      :type description: str

      :returns: Updated Feature Group object.
      :rtype: FeatureGroup


   .. py:method:: detach_feature_group_from_template(feature_group_id)

      Update a feature group to detach it from a template.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str

      :returns: The updated feature group.
      :rtype: FeatureGroup


   .. py:method:: update_feature_group_template_bindings(feature_group_id, template_bindings = None)

      Update the feature group template bindings for a template feature group.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param template_bindings: Values in these bindings override values set in the template.
      :type template_bindings: list

      :returns: Updated feature group.
      :rtype: FeatureGroup


   .. py:method:: update_feature_group_python_function_bindings(feature_group_id, python_function_bindings)

      Updates an existing Feature Group's Python function bindings from a user-provided Python Function. If a list of feature groups are supplied within the Python function bindings, we will provide DataFrames (Pandas in the case of Python) with the materialized feature groups for those input feature groups as arguments to the function.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param python_function_bindings: List of arguments to be supplied to the function as parameters in the format [{'name': 'function_argument', 'variable_type': 'FEATURE_GROUP', 'value': 'name_of_feature_group'}].
      :type python_function_bindings: list


   .. py:method:: update_feature_group_python_function(feature_group_id, python_function_name, python_function_bindings = [])

      Updates an existing Feature Group's python function from a user provided Python Function. If a list of feature groups are supplied within the python function

      bindings, we will provide as arguments to the function DataFrame's (pandas in the case of Python) with the materialized
      feature groups for those input feature groups.


      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param python_function_name: The name of the python function to be associated with the feature group.
      :type python_function_name: str
      :param python_function_bindings: List of arguments to be supplied to the function as parameters in the format [{'name': 'function_argument', 'variable_type': 'FEATURE_GROUP', 'value': 'name_of_feature_group'}].
      :type python_function_bindings: list


   .. py:method:: update_feature_group_sql_definition(feature_group_id, sql)

      Updates the SQL statement for a feature group.

      :param feature_group_id: The unique identifier associated with the feature group.
      :type feature_group_id: str
      :param sql: The input SQL statement for the feature group.
      :type sql: str

      :returns: The updated feature group.
      :rtype: FeatureGroup


   .. py:method:: update_dataset_feature_group_feature_expression(feature_group_id, feature_expression)

      Updates the SQL feature expression for a Dataset FeatureGroup's custom features

      :param feature_group_id: The unique identifier associated with the feature group.
      :type feature_group_id: str
      :param feature_expression: The input SQL statement for the feature group.
      :type feature_expression: str

      :returns: The updated feature group.
      :rtype: FeatureGroup


   .. py:method:: update_feature_group_function_definition(feature_group_id, function_source_code = None, function_name = None, input_feature_groups = None, cpu_size = None, memory = None, package_requirements = None, use_original_csv_names = False, python_function_bindings = None, use_gpu = None)

      Updates the function definition for a feature group

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param function_source_code: Contents of a valid source code file in a supported Feature Group specification language (currently only Python). The source code should contain a function called `function_name`. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type function_source_code: str
      :param function_name: Name of the function found in the source code that will be executed (on the optional inputs) to materialize this feature group.
      :type function_name: str
      :param input_feature_groups: List of feature groups that are supplied to the function as parameters. Each of the parameters are materialized DataFrames (same type as the functions return value).
      :type input_feature_groups: list
      :param cpu_size: Size of the CPU for the feature group function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the feature group function.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param use_original_csv_names: If set to `True`, feature group uses the original column names for input feature groups from CSV datasets.
      :type use_original_csv_names: bool
      :param python_function_bindings: List of arguments to be supplied to the function as parameters in the format [{'name': 'function_argument', 'variable_type': 'FEATURE_GROUP', 'value': 'name_of_feature_group'}].
      :type python_function_bindings: list
      :param use_gpu: Whether the feature group needs a gpu or not. Otherwise default to CPU.
      :type use_gpu: bool

      :returns: The updated feature group.
      :rtype: FeatureGroup


   .. py:method:: update_feature(feature_group_id, name, select_expression = None, new_name = None)

      Modifies an existing feature in a feature group.

      :param feature_group_id: Unique identifier of the feature group.
      :type feature_group_id: str
      :param name: Name of the feature to be updated.
      :type name: str
      :param select_expression: SQL statement for modifying the feature.
      :type select_expression: str
      :param new_name: New name of the feature.
      :type new_name: str

      :returns: Updated feature group object.
      :rtype: FeatureGroup


   .. py:method:: export_feature_group_version_to_file_connector(feature_group_version, location, export_file_format, overwrite = False)

      Export Feature group to File Connector.

      :param feature_group_version: Unique string identifier for the feature group instance to export.
      :type feature_group_version: str
      :param location: Cloud file location to export to.
      :type location: str
      :param export_file_format: Enum string specifying the file format to export to.
      :type export_file_format: str
      :param overwrite: If true and a file exists at this location, this process will overwrite the file.
      :type overwrite: bool

      :returns: The FeatureGroupExport instance.
      :rtype: FeatureGroupExport


   .. py:method:: export_feature_group_version_to_database_connector(feature_group_version, database_connector_id, object_name, write_mode, database_feature_mapping, id_column = None, additional_id_columns = None)

      Export Feature group to Database Connector.

      :param feature_group_version: Unique string identifier for the Feature Group instance to export.
      :type feature_group_version: str
      :param database_connector_id: Unique string identifier for the Database Connector to export to.
      :type database_connector_id: str
      :param object_name: Name of the database object to write to.
      :type object_name: str
      :param write_mode: Enum string indicating whether to use INSERT or UPSERT.
      :type write_mode: str
      :param database_feature_mapping: Key/value pair JSON object of "database connector column" -> "feature name" pairs.
      :type database_feature_mapping: dict
      :param id_column: Required if write_mode is UPSERT. Indicates which database column should be used as the lookup key.
      :type id_column: str
      :param additional_id_columns: For database connectors which support it, additional ID columns to use as a complex key for upserting.
      :type additional_id_columns: list

      :returns: The FeatureGroupExport instance.
      :rtype: FeatureGroupExport


   .. py:method:: export_feature_group_version_to_console(feature_group_version, export_file_format)

      Export Feature group to console.

      :param feature_group_version: Unique string identifier of the Feature Group instance to export.
      :type feature_group_version: str
      :param export_file_format: File format to export to.
      :type export_file_format: str

      :returns: The FeatureGroupExport instance.
      :rtype: FeatureGroupExport


   .. py:method:: set_feature_group_modifier_lock(feature_group_id, locked = True)

      Lock a feature group to prevent modification.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param locked: Whether to disable or enable feature group modification (True or False).
      :type locked: bool


   .. py:method:: add_user_to_feature_group_modifiers(feature_group_id, email)

      Adds a user to a feature group.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param email: The email address of the user to be added.
      :type email: str


   .. py:method:: add_organization_group_to_feature_group_modifiers(feature_group_id, organization_group_id)

      Add OrganizationGroup to a feature group modifiers list

      :param feature_group_id: Unique string identifier of the feature group.
      :type feature_group_id: str
      :param organization_group_id: Unique string identifier of the organization group.
      :type organization_group_id: str


   .. py:method:: remove_user_from_feature_group_modifiers(feature_group_id, email)

      Removes a user from a specified feature group.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param email: The email address of the user to be removed.
      :type email: str


   .. py:method:: remove_organization_group_from_feature_group_modifiers(feature_group_id, organization_group_id)

      Removes an OrganizationGroup from a feature group modifiers list

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param organization_group_id: The unique ID associated with the organization group.
      :type organization_group_id: str


   .. py:method:: delete_feature(feature_group_id, name)

      Removes a feature from the feature group.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param name: Name of the feature to be deleted.
      :type name: str

      :returns: Updated feature group object.
      :rtype: FeatureGroup


   .. py:method:: delete_feature_group(feature_group_id)

      Deletes a Feature Group.

      :param feature_group_id: Unique string identifier for the feature group to be removed.
      :type feature_group_id: str


   .. py:method:: create_feature_group_version(feature_group_id, variable_bindings = None)

      Creates a snapshot for a specified feature group.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str
      :param variable_bindings: Dictionary defining variable bindings that override parent feature group values.
      :type variable_bindings: dict

      :returns: A feature group version.
      :rtype: FeatureGroupVersion


   .. py:method:: create_feature_group_template(feature_group_id, name, template_sql, template_variables, description = None, template_bindings = None, should_attach_feature_group_to_template = False)

      Create a feature group template.

      :param feature_group_id: Unique identifier of the feature group this template was created from.
      :type feature_group_id: str
      :param name: User-friendly name for this feature group template.
      :type name: str
      :param template_sql: The template SQL that will be resolved by applying values from the template variables to generate SQL for a feature group.
      :type template_sql: str
      :param template_variables: The template variables for resolving the template.
      :type template_variables: list
      :param description: Description of this feature group template.
      :type description: str
      :param template_bindings: If the feature group will be attached to the newly created template, set these variable bindings on that feature group.
      :type template_bindings: list
      :param should_attach_feature_group_to_template: Set to `True` to convert the feature group to a template feature group and attach it to the newly created template.
      :type should_attach_feature_group_to_template: bool

      :returns: The created feature group template.
      :rtype: FeatureGroupTemplate


   .. py:method:: delete_feature_group_template(feature_group_template_id)

      Delete an existing feature group template.

      :param feature_group_template_id: Unique string identifier associated with the feature group template.
      :type feature_group_template_id: str


   .. py:method:: update_feature_group_template(feature_group_template_id, template_sql = None, template_variables = None, description = None, name = None)

      Update a feature group template.

      :param feature_group_template_id: Unique identifier of the feature group template to update.
      :type feature_group_template_id: str
      :param template_sql: If provided, the new value to use for the template SQL.
      :type template_sql: str
      :param template_variables: If provided, the new value to use for the template variables.
      :type template_variables: list
      :param description: Description of this feature group template.
      :type description: str
      :param name: User-friendly name for this feature group template.
      :type name: str

      :returns: The updated feature group template.
      :rtype: FeatureGroupTemplate


   .. py:method:: preview_feature_group_template_resolution(feature_group_template_id = None, template_bindings = None, template_sql = None, template_variables = None, should_validate = True)

      Resolve template sql using template variables and template bindings.

      :param feature_group_template_id: Unique string identifier. If specified, use this template, otherwise assume an empty template.
      :type feature_group_template_id: str
      :param template_bindings: Values to override the template variable values specified by the template.
      :type template_bindings: list
      :param template_sql: If specified, use this as the template SQL instead of the feature group template's SQL.
      :type template_sql: str
      :param template_variables: Template variables to use. If a template is provided, this overrides the template's template variables.
      :type template_variables: list
      :param should_validate: If true, validates the resolved SQL.
      :type should_validate: bool

      :returns: The resolved template
      :rtype: ResolvedFeatureGroupTemplate


   .. py:method:: cancel_upload(upload_id)

      Cancels an upload.

      :param upload_id: A unique string identifier for the upload.
      :type upload_id: str


   .. py:method:: upload_part(upload_id, part_number, part_data)

      Uploads part of a large dataset file from your bucket to our system. Our system currently supports parts of up to 5GB and full files of up to 5TB. Note that each part must be at least 5MB in size, unless it is the last part in the sequence of parts for the full file.

      :param upload_id: A unique identifier for this upload.
      :type upload_id: str
      :param part_number: The 1-indexed number denoting the position of the file part in the sequence of parts for the full file.
      :type part_number: int
      :param part_data: The multipart/form-data for the current part of the full file.
      :type part_data: io.TextIOBase

      :returns: The object 'UploadPart' which encapsulates the hash and the etag for the part that got uploaded.
      :rtype: UploadPart


   .. py:method:: mark_upload_complete(upload_id)

      Marks an upload process as complete.

      :param upload_id: A unique string identifier for the upload process.
      :type upload_id: str

      :returns: The upload object associated with the process, containing details of the file.
      :rtype: Upload


   .. py:method:: create_dataset_from_file_connector(table_name, location, file_format = None, refresh_schedule = None, csv_delimiter = None, filename_column = None, start_prefix = None, until_prefix = None, location_date_format = None, date_format_lookback_days = None, incremental = False, is_documentset = False, extract_bounding_boxes = False, document_processing_config = None, merge_file_schemas = False, reference_only_documentset = False, parsing_config = None)

      Creates a dataset from a file located in a cloud storage, such as Amazon AWS S3, using the specified dataset name and location.

      :param table_name: Organization-unique table name or the name of the feature group table to create using the source table.
      :type table_name: str
      :param location: The URI location format of the dataset source. The URI location format needs to be specified to match the `location_date_format` when `location_date_format` is specified. For example, Location = s3://bucket1/dir1/dir2/event_date=YYYY-MM-DD/* when `location_date_format` is specified. The URI location format needs to include both the `start_prefix` and `until_prefix` when both are specified. For example, Location s3://bucket1/dir1/* includes both s3://bucket1/dir1/dir2/event_date=2021-08-02/* and s3://bucket1/dir1/dir2/event_date=2021-08-08/*
      :type location: str
      :param file_format: The file format of the dataset.
      :type file_format: str
      :param refresh_schedule: The Cron time string format that describes a schedule to retrieve the latest version of the imported dataset. The time is specified in UTC.
      :type refresh_schedule: str
      :param csv_delimiter: If the file format is CSV, use a specific csv delimiter.
      :type csv_delimiter: str
      :param filename_column: Adds a new column to the dataset with the external URI path.
      :type filename_column: str
      :param start_prefix: The start prefix (inclusive) for a range based search on a cloud storage location URI.
      :type start_prefix: str
      :param until_prefix: The end prefix (exclusive) for a range based search on a cloud storage location URI.
      :type until_prefix: str
      :param location_date_format: The date format in which the data is partitioned in the cloud storage location. For example, if the data is partitioned as s3://bucket1/dir1/dir2/event_date=YYYY-MM-DD/dir4/filename.parquet, then the `location_date_format` is YYYY-MM-DD. This format needs to be consistent across all files within the specified location.
      :type location_date_format: str
      :param date_format_lookback_days: The number of days to look back from the current day for import locations that are date partitioned. For example, import date 2021-06-04 with `date_format_lookback_days` = 3 will retrieve data for all the dates in the range [2021-06-02, 2021-06-04].
      :type date_format_lookback_days: int
      :param incremental: Signifies if the dataset is an incremental dataset.
      :type incremental: bool
      :param is_documentset: Signifies if the dataset is docstore dataset. A docstore dataset contains documents like images, PDFs, audio files etc. or is tabular data with links to such files.
      :type is_documentset: bool
      :param extract_bounding_boxes: Signifies whether to extract bounding boxes out of the documents. Only valid if is_documentset if True.
      :type extract_bounding_boxes: bool
      :param document_processing_config: The document processing configuration. Only valid if is_documentset is True.
      :type document_processing_config: DocumentProcessingConfig
      :param merge_file_schemas: Signifies if the merge file schema policy is enabled. If is_documentset is True, this is also set to True by default.
      :type merge_file_schemas: bool
      :param reference_only_documentset: Signifies if the data reference only policy is enabled.
      :type reference_only_documentset: bool
      :param parsing_config: Custom config for dataset parsing.
      :type parsing_config: ParsingConfig

      :returns: The dataset created.
      :rtype: Dataset


   .. py:method:: create_dataset_version_from_file_connector(dataset_id, location = None, file_format = None, csv_delimiter = None, merge_file_schemas = None, parsing_config = None)

      Creates a new version of the specified dataset.

      :param dataset_id: Unique string identifier associated with the dataset.
      :type dataset_id: str
      :param location: External URI to import the dataset from. If not specified, the last location will be used.
      :type location: str
      :param file_format: File format to be used. If not specified, the service will try to detect the file format.
      :type file_format: str
      :param csv_delimiter: If the file format is CSV, use a specific CSV delimiter.
      :type csv_delimiter: str
      :param merge_file_schemas: Signifies if the merge file schema policy is enabled.
      :type merge_file_schemas: bool
      :param parsing_config: Custom config for dataset parsing.
      :type parsing_config: ParsingConfig

      :returns: The new Dataset Version created.
      :rtype: DatasetVersion


   .. py:method:: create_dataset_from_database_connector(table_name, database_connector_id, object_name = None, columns = None, query_arguments = None, refresh_schedule = None, sql_query = None, incremental = False, incremental_database_connector_config = None)

      Creates a dataset from a Database Connector.

      :param table_name: Organization-unique table name.
      :type table_name: str
      :param database_connector_id: Unique String Identifier of the Database Connector to import the dataset from.
      :type database_connector_id: str
      :param object_name: If applicable, the name/ID of the object in the service to query.
      :type object_name: str
      :param columns: The columns to query from the external service object.
      :type columns: str
      :param query_arguments: Additional query arguments to filter the data.
      :type query_arguments: str
      :param refresh_schedule: The Cron time string format that describes a schedule to retrieve the latest version of the imported dataset. The time is specified in UTC.
      :type refresh_schedule: str
      :param sql_query: The full SQL query to use when fetching data. If present, this parameter will override `object_name`, `columns`, `timestamp_column`, and `query_arguments`.
      :type sql_query: str
      :param incremental: Signifies if the dataset is an incremental dataset.
      :type incremental: bool
      :param incremental_database_connector_config: The config for incremental datasets. Only valid if incremental is True
      :type incremental_database_connector_config: IncrementalDatabaseConnectorConfig

      :returns: The created dataset.
      :rtype: Dataset


   .. py:method:: create_dataset_from_application_connector(table_name, application_connector_id, dataset_config = None, refresh_schedule = None)

      Creates a dataset from an Application Connector.

      :param table_name: Organization-unique table name.
      :type table_name: str
      :param application_connector_id: Unique string identifier of the application connector to download data from.
      :type application_connector_id: str
      :param dataset_config: Dataset config for the application connector.
      :type dataset_config: DatasetConfig
      :param refresh_schedule: Cron time string format that describes a schedule to retrieve the latest version of the imported dataset. The time is specified in UTC.
      :type refresh_schedule: str

      :returns: The created dataset.
      :rtype: Dataset


   .. py:method:: create_dataset_version_from_database_connector(dataset_id, object_name = None, columns = None, query_arguments = None, sql_query = None)

      Creates a new version of the specified dataset.

      :param dataset_id: The unique ID associated with the dataset.
      :type dataset_id: str
      :param object_name: The name/ID of the object in the service to query. If not specified, the last name will be used.
      :type object_name: str
      :param columns: The columns to query from the external service object. If not specified, the last columns will be used.
      :type columns: str
      :param query_arguments: Additional query arguments to filter the data. If not specified, the last arguments will be used.
      :type query_arguments: str
      :param sql_query: The full SQL query to use when fetching data. If present, this parameter will override object_name, columns, and query_arguments.
      :type sql_query: str

      :returns: The new Dataset Version created.
      :rtype: DatasetVersion


   .. py:method:: create_dataset_version_from_application_connector(dataset_id, dataset_config = None)

      Creates a new version of the specified dataset.

      :param dataset_id: The unique ID associated with the dataset.
      :type dataset_id: str
      :param dataset_config: Dataset config for the application connector. If any of the fields are not specified, the last values will be used.
      :type dataset_config: DatasetConfig

      :returns: The new Dataset Version created.
      :rtype: DatasetVersion


   .. py:method:: create_dataset_from_upload(table_name, file_format = None, csv_delimiter = None, is_documentset = False, extract_bounding_boxes = False, parsing_config = None, merge_file_schemas = False, document_processing_config = None)

      Creates a dataset and returns an upload ID that can be used to upload a file.

      :param table_name: Organization-unique table name for this dataset.
      :type table_name: str
      :param file_format: The file format of the dataset.
      :type file_format: str
      :param csv_delimiter: If the file format is CSV, use a specific CSV delimiter.
      :type csv_delimiter: str
      :param is_documentset: Signifies if the dataset is a docstore dataset. A docstore dataset contains documents like images, PDFs, audio files etc. or is tabular data with links to such files.
      :type is_documentset: bool
      :param extract_bounding_boxes: Signifies whether to extract bounding boxes out of the documents. Only valid if is_documentset if True.
      :type extract_bounding_boxes: bool
      :param parsing_config: Custom config for dataset parsing.
      :type parsing_config: ParsingConfig
      :param merge_file_schemas: Signifies whether to merge the schemas of all files in the dataset. If is_documentset is True, this is also set to True by default.
      :type merge_file_schemas: bool
      :param document_processing_config: The document processing configuration. Only valid if is_documentset is True.
      :type document_processing_config: DocumentProcessingConfig

      :returns: A reference to be used when uploading file parts.
      :rtype: Upload


   .. py:method:: create_dataset_version_from_upload(dataset_id, file_format = None)

      Creates a new version of the specified dataset using a local file upload.

      :param dataset_id: Unique string identifier associated with the dataset.
      :type dataset_id: str
      :param file_format: File format to be used. If not specified, the service will attempt to detect the file format.
      :type file_format: str

      :returns: Token to be used when uploading file parts.
      :rtype: Upload


   .. py:method:: create_streaming_dataset(table_name, primary_key = None, update_timestamp_key = None, lookup_keys = None)

      Creates a streaming dataset. Use a streaming dataset if your dataset is receiving information from multiple sources over an extended period of time.

      :param table_name: The feature group table name to create for this dataset.
      :type table_name: str
      :param primary_key: The optional primary key column name for the dataset.
      :type primary_key: str
      :param update_timestamp_key: Name of the feature which defines the update timestamp of the feature group. Used in concatenation and primary key deduplication. Only relevant if lookup keys are set.
      :type update_timestamp_key: str
      :param lookup_keys: List of feature names which can be used in the lookup API to restrict the computation to a set of dataset rows. These feature names have to correspond to underlying dataset columns.
      :type lookup_keys: list

      :returns: The streaming dataset created.
      :rtype: Dataset


   .. py:method:: snapshot_streaming_data(dataset_id)

      Snapshots the current data in the streaming dataset.

      :param dataset_id: The unique ID associated with the dataset.
      :type dataset_id: str

      :returns: The new Dataset Version created by taking a snapshot of the current data in the streaming dataset.
      :rtype: DatasetVersion


   .. py:method:: set_dataset_column_data_type(dataset_id, column, data_type)

      Set a Dataset's column type.

      :param dataset_id: The unique ID associated with the dataset.
      :type dataset_id: str
      :param column: The name of the column.
      :type column: str
      :param data_type: The type of the data in the column. Note: Some ColumnMappings may restrict the options or explicitly set the DataType.
      :type data_type: str

      :returns: The dataset and schema after the data type has been set.
      :rtype: Dataset


   .. py:method:: create_dataset_from_streaming_connector(table_name, streaming_connector_id, streaming_args = None, refresh_schedule = None)

      Creates a dataset from a Streaming Connector

      :param table_name: Organization-unique table name
      :type table_name: str
      :param streaming_connector_id: Unique String Identifier for the Streaming Connector to import the dataset from
      :type streaming_connector_id: str
      :param streaming_args: Dictionary of arguments to read data from the streaming connector
      :type streaming_args: dict
      :param refresh_schedule: Cron time string format that describes a schedule to retrieve the latest version of the imported dataset. Time is specified in UTC.
      :type refresh_schedule: str

      :returns: The created dataset.
      :rtype: Dataset


   .. py:method:: set_streaming_retention_policy(dataset_id, retention_hours = None, retention_row_count = None, ignore_records_before_timestamp = None)

      Sets the streaming retention policy.

      :param dataset_id: Unique string identifier for the streaming dataset.
      :type dataset_id: str
      :param retention_hours: Number of hours to retain streamed data in memory.
      :type retention_hours: int
      :param retention_row_count: Number of rows to retain streamed data in memory.
      :type retention_row_count: int
      :param ignore_records_before_timestamp: The Unix timestamp (in seconds) to use as a cutoff to ignore all entries sent before it
      :type ignore_records_before_timestamp: int


   .. py:method:: rename_database_connector(database_connector_id, name)

      Renames a Database Connector

      :param database_connector_id: The unique identifier for the database connector.
      :type database_connector_id: str
      :param name: The new name for the Database Connector.
      :type name: str


   .. py:method:: rename_application_connector(application_connector_id, name)

      Renames a Application Connector

      :param application_connector_id: The unique identifier for the application connector.
      :type application_connector_id: str
      :param name: A new name for the application connector.
      :type name: str


   .. py:method:: verify_database_connector(database_connector_id)

      Checks if Abacus.AI can access the specified database.

      :param database_connector_id: Unique string identifier for the database connector.
      :type database_connector_id: str


   .. py:method:: verify_file_connector(bucket)

      Checks to see if Abacus.AI can access the given bucket.

      :param bucket: The bucket to test.
      :type bucket: str

      :returns: The result of the verification.
      :rtype: FileConnectorVerification


   .. py:method:: delete_database_connector(database_connector_id)

      Delete a database connector.

      :param database_connector_id: The unique identifier for the database connector.
      :type database_connector_id: str


   .. py:method:: delete_application_connector(application_connector_id)

      Delete an application connector.

      :param application_connector_id: The unique identifier for the application connector.
      :type application_connector_id: str


   .. py:method:: delete_file_connector(bucket)

      Deletes a file connector

      :param bucket: The fully qualified URI of the bucket to remove.
      :type bucket: str


   .. py:method:: verify_application_connector(application_connector_id)

      Checks if Abacus.AI can access the application using the provided application connector ID.

      :param application_connector_id: Unique string identifier for the application connector.
      :type application_connector_id: str


   .. py:method:: set_azure_blob_connection_string(bucket, connection_string)

      Authenticates the specified Azure Blob Storage bucket using an authenticated Connection String.

      :param bucket: The fully qualified Azure Blob Storage Bucket URI.
      :type bucket: str
      :param connection_string: The Connection String Abacus.AI should use to authenticate when accessing this bucket.
      :type connection_string: str

      :returns: An object with the roleArn and verification status for the specified bucket.
      :rtype: FileConnectorVerification


   .. py:method:: verify_streaming_connector(streaming_connector_id)

      Checks to see if Abacus.AI can access the streaming connector.

      :param streaming_connector_id: Unique string identifier for the streaming connector to be checked for Abacus.AI access.
      :type streaming_connector_id: str


   .. py:method:: rename_streaming_connector(streaming_connector_id, name)

      Renames a Streaming Connector

      :param streaming_connector_id: The unique identifier for the streaming connector.
      :type streaming_connector_id: str
      :param name: A new name for the streaming connector.
      :type name: str


   .. py:method:: delete_streaming_connector(streaming_connector_id)

      Delete a streaming connector.

      :param streaming_connector_id: The unique identifier for the streaming connector.
      :type streaming_connector_id: str


   .. py:method:: create_streaming_token()

      Creates a streaming token for the specified project. Streaming tokens are used to authenticate requests when appending data to streaming datasets.

      :returns: The generated streaming token.
      :rtype: StreamingAuthToken


   .. py:method:: delete_streaming_token(streaming_token)

      Deletes the specified streaming token.

      :param streaming_token: The streaming token to delete.
      :type streaming_token: str


   .. py:method:: delete_dataset(dataset_id)

      Deletes the specified dataset from the organization.

      :param dataset_id: Unique string identifier of the dataset to delete.
      :type dataset_id: str


   .. py:method:: extract_document_data(document = None, doc_id = None, document_processing_config = None)

      Extracts data from a document.

      :param document: The document to extract data from. One of document or doc_id must be provided.
      :type document: io.TextIOBase
      :param doc_id: A unique Docstore string identifier for the document. One of document or doc_id must be provided.
      :type doc_id: str
      :param document_processing_config: The document processing configuration.
      :type document_processing_config: DocumentProcessingConfig

      :returns: The extracted document data.
      :rtype: DocumentData


   .. py:method:: get_training_config_options(project_id, feature_group_ids = None, for_retrain = False, current_training_config = None)

      Retrieves the full initial description of the model training configuration options available for the specified project. The configuration options available are determined by the use case associated with the specified project. Refer to the [Use Case Documentation]({USE_CASES_URL}) for more information on use cases and use case-specific configuration options.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_ids: The feature group IDs to be used for training.
      :type feature_group_ids: list
      :param for_retrain: Whether the training config options are used for retraining.
      :type for_retrain: bool
      :param current_training_config: The current state of the training config, with some options set, which shall be used to get new options after refresh. This is `None` by default initially.
      :type current_training_config: TrainingConfig

      :returns: An array of options that can be specified when training a model in this project.
      :rtype: list[TrainingConfigOptions]


   .. py:method:: create_train_test_data_split_feature_group(project_id, training_config, feature_group_ids)

      Get the train and test data split without training the model. Only supported for models with custom algorithms.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param training_config: The training config used to influence how the split is calculated.
      :type training_config: TrainingConfig
      :param feature_group_ids: List of feature group IDs provided by the user, including the required one for data split and others to influence how to split.
      :type feature_group_ids: list

      :returns: The feature group containing the training data and folds information.
      :rtype: FeatureGroup


   .. py:method:: train_model(project_id, name = None, training_config = None, feature_group_ids = None, refresh_schedule = None, custom_algorithms = None, custom_algorithms_only = False, custom_algorithm_configs = None, builtin_algorithms = None, cpu_size = None, memory = None, algorithm_training_configs = None)

      Create a new model and start its training in the given project.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param name: The name of the model. Defaults to "<Project Name> Model".
      :type name: str
      :param training_config: The training config used to train this model.
      :type training_config: TrainingConfig
      :param feature_group_ids: List of feature group IDs provided by the user to train the model on.
      :type feature_group_ids: list
      :param refresh_schedule: A cron-style string that describes a schedule in UTC to automatically retrain the created model.
      :type refresh_schedule: str
      :param custom_algorithms: List of user-defined algorithms to train. If not set, the default enabled custom algorithms will be used.
      :type custom_algorithms: list
      :param custom_algorithms_only: Whether to only run custom algorithms.
      :type custom_algorithms_only: bool
      :param custom_algorithm_configs: Configs for each user-defined algorithm; key is the algorithm name, value is the config serialized to JSON.
      :type custom_algorithm_configs: dict
      :param builtin_algorithms: List of algorithm names or algorithm IDs of the builtin algorithms provided by Abacus.AI to train. If not set, all applicable builtin algorithms will be used.
      :type builtin_algorithms: list
      :param cpu_size: Size of the CPU for the user-defined algorithms during training.
      :type cpu_size: str
      :param memory: Memory (in GB) for the user-defined algorithms during training.
      :type memory: int
      :param algorithm_training_configs: List of algorithm specifc training configs that will be part of the model training AutoML run.
      :type algorithm_training_configs: list

      :returns: The new model which is being trained.
      :rtype: Model


   .. py:method:: create_model_from_python(project_id, function_source_code, train_function_name, training_input_tables, predict_function_name = None, predict_many_function_name = None, initialize_function_name = None, name = None, cpu_size = None, memory = None, training_config = None, exclusive_run = False, package_requirements = None, use_gpu = False, is_thread_safe = None)

      Initializes a new Model from user-provided Python code. If a list of input feature groups is supplied, they will be provided as arguments to the train and predict functions with the materialized feature groups for those input feature groups.

      This method expects `functionSourceCode` to be a valid language source file which contains the functions named `trainFunctionName` and `predictFunctionName`. `trainFunctionName` returns the ModelVersion that is the result of training the model using `trainFunctionName` and `predictFunctionName` has no well-defined return type, as it returns the prediction made by the `predictFunctionName`, which can be anything.


      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param function_source_code: Contents of a valid Python source code file. The source code should contain the functions named `trainFunctionName` and `predictFunctionName`. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type function_source_code: str
      :param train_function_name: Name of the function found in the source code that will be executed to train the model. It is not executed when this function is run.
      :type train_function_name: str
      :param training_input_tables: List of feature groups that are supplied to the train function as parameters. Each of the parameters are materialized Dataframes (same type as the functions return value).
      :type training_input_tables: list
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions through the model. It is not executed when this function is run.
      :type predict_function_name: str
      :param predict_many_function_name: Name of the function found in the source code that will be executed for batch prediction of the model. It is not executed when this function is run.
      :type predict_many_function_name: str
      :param initialize_function_name: Name of the function found in the source code to initialize the trained model before using it to make predictions using the model
      :type initialize_function_name: str
      :param name: The name you want your model to have. Defaults to "<Project Name> Model"
      :type name: str
      :param cpu_size: Size of the CPU for the model training function
      :type cpu_size: str
      :param memory: Memory (in GB) for the model training function
      :type memory: int
      :param training_config: Training configuration
      :type training_config: TrainingConfig
      :param exclusive_run: Decides if this model will be run exclusively or along with other Abacus.ai algorithms
      :type exclusive_run: bool
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: list
      :param use_gpu: Whether this model needs gpu
      :type use_gpu: bool
      :param is_thread_safe: Whether this model is thread safe
      :type is_thread_safe: bool

      :returns: The new model, which has not been trained.
      :rtype: Model


   .. py:method:: rename_model(model_id, name)

      Renames a model

      :param model_id: Unique identifier of the model to rename.
      :type model_id: str
      :param name: The new name to assign to the model.
      :type name: str


   .. py:method:: update_python_model(model_id, function_source_code = None, train_function_name = None, predict_function_name = None, predict_many_function_name = None, initialize_function_name = None, training_input_tables = None, cpu_size = None, memory = None, package_requirements = None, use_gpu = None, is_thread_safe = None)

      Updates an existing Python Model using user-provided Python code. If a list of input feature groups is supplied, they will be provided as arguments to the `train` and `predict` functions with the materialized feature groups for those input feature groups.

      This method expects `functionSourceCode` to be a valid language source file which contains the functions named `trainFunctionName` and `predictFunctionName`. `trainFunctionName` returns the ModelVersion that is the result of training the model using `trainFunctionName`. `predictFunctionName` has no well-defined return type, as it returns the prediction made by the `predictFunctionName`, which can be anything.


      :param model_id: The unique ID associated with the Python model to be changed.
      :type model_id: str
      :param function_source_code: Contents of a valid Python source code file. The source code should contain the functions named `trainFunctionName` and `predictFunctionName`. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type function_source_code: str
      :param train_function_name: Name of the function found in the source code that will be executed to train the model. It is not executed when this function is run.
      :type train_function_name: str
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions through the model. It is not executed when this function is run.
      :type predict_function_name: str
      :param predict_many_function_name: Name of the function found in the source code that will be executed to run batch predictions through the model. It is not executed when this function is run.
      :type predict_many_function_name: str
      :param initialize_function_name: Name of the function found in the source code to initialize the trained model before using it to make predictions using the model.
      :type initialize_function_name: str
      :param training_input_tables: List of feature groups that are supplied to the `train` function as parameters. Each of the parameters are materialized DataFrames (same type as the functions return value).
      :type training_input_tables: list
      :param cpu_size: Size of the CPU for the model training function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the model training function.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: `['numpy==1.2.3', 'pandas>=1.4.0']`.
      :type package_requirements: list
      :param use_gpu: Whether this model needs gpu
      :type use_gpu: bool
      :param is_thread_safe: Whether this model is thread safe
      :type is_thread_safe: bool

      :returns: The updated model.
      :rtype: Model


   .. py:method:: update_python_model_zip(model_id, train_function_name = None, predict_function_name = None, predict_many_function_name = None, train_module_name = None, predict_module_name = None, training_input_tables = None, cpu_size = None, memory = None, package_requirements = None, use_gpu = None)

      Updates an existing Python Model using a provided zip file. If a list of input feature groups are supplied, they will be provided as arguments to the train and predict functions with the materialized feature groups for those input feature groups.

      This method expects `trainModuleName` and `predictModuleName` to be valid language source files which contain the functions named `trainFunctionName` and `predictFunctionName`, respectively. `trainFunctionName` returns the ModelVersion that is the result of training the model using `trainFunctionName`, and `predictFunctionName` has no well-defined return type, as it returns the prediction made by the `predictFunctionName`, which can be anything.


      :param model_id: The unique ID associated with the Python model to be changed.
      :type model_id: str
      :param train_function_name: Name of the function found in the train module that will be executed to train the model. It is not executed when this function is run.
      :type train_function_name: str
      :param predict_function_name: Name of the function found in the predict module that will be executed to run predictions through the model. It is not executed when this function is run.
      :type predict_function_name: str
      :param predict_many_function_name: Name of the function found in the predict module that will be executed to run batch predictions through the model. It is not executed when this function is run.
      :type predict_many_function_name: str
      :param train_module_name: Full path of the module that contains the train function from the root of the zip.
      :type train_module_name: str
      :param predict_module_name: Full path of the module that contains the predict function from the root of the zip.
      :type predict_module_name: str
      :param training_input_tables: List of feature groups that are supplied to the train function as parameters. Each of the parameters are materialized Dataframes (same type as the function's return value).
      :type training_input_tables: list
      :param cpu_size: Size of the CPU for the model training function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the model training function.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param use_gpu: Whether this model needs gpu
      :type use_gpu: bool

      :returns: The updated model.
      :rtype: Upload


   .. py:method:: update_python_model_git(model_id, application_connector_id = None, branch_name = None, python_root = None, train_function_name = None, predict_function_name = None, predict_many_function_name = None, train_module_name = None, predict_module_name = None, training_input_tables = None, cpu_size = None, memory = None, use_gpu = None)

      Updates an existing Python model using an existing Git application connector. If a list of input feature groups are supplied, these will be provided as arguments to the train and predict functions with the materialized feature groups for those input feature groups.

      This method expects `trainModuleName` and `predictModuleName` to be valid language source files which contain the functions named `trainFunctionName` and `predictFunctionName`, respectively. `trainFunctionName` returns the `ModelVersion` that is the result of training the model using `trainFunctionName`, and `predictFunctionName` has no well-defined return type, as it returns the prediction made by the `predictFunctionName`, which can be anything.


      :param model_id: The unique ID associated with the Python model to be changed.
      :type model_id: str
      :param application_connector_id: The unique ID associated with the Git application connector.
      :type application_connector_id: str
      :param branch_name: Name of the branch in the Git repository to be used for training.
      :type branch_name: str
      :param python_root: Path from the top level of the Git repository to the directory containing the Python source code. If not provided, the default is the root of the Git repository.
      :type python_root: str
      :param train_function_name: Name of the function found in train module that will be executed to train the model. It is not executed when this function is run.
      :type train_function_name: str
      :param predict_function_name: Name of the function found in the predict module that will be executed to run predictions through model. It is not executed when this function is run.
      :type predict_function_name: str
      :param predict_many_function_name: Name of the function found in the predict module that will be executed to run batch predictions through model. It is not executed when this function is run.
      :type predict_many_function_name: str
      :param train_module_name: Full path of the module that contains the train function from the root of the zip.
      :type train_module_name: str
      :param predict_module_name: Full path of the module that contains the predict function from the root of the zip.
      :type predict_module_name: str
      :param training_input_tables: List of feature groups that are supplied to the train function as parameters. Each of the parameters are materialized Dataframes (same type as the functions return value).
      :type training_input_tables: list
      :param cpu_size: Size of the CPU for the model training function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the model training function.
      :type memory: int
      :param use_gpu: Whether this model needs gpu
      :type use_gpu: bool

      :returns: The updated model.
      :rtype: Model


   .. py:method:: set_model_training_config(model_id, training_config, feature_group_ids = None)

      Edits the default model training config

      :param model_id: A unique string identifier of the model to update.
      :type model_id: str
      :param training_config: The training config used to train this model.
      :type training_config: TrainingConfig
      :param feature_group_ids: The list of feature groups used as input to the model.
      :type feature_group_ids: list

      :returns: The model object corresponding to the updated training config.
      :rtype: Model


   .. py:method:: set_model_objective(model_version, metric = None)

      Sets the best model for all model instances of the model based on the specified metric, and updates the training configuration to use the specified metric for any future model versions.

      If metric is set to None, then just use the default selection


      :param model_version: The model version to set as the best model.
      :type model_version: str
      :param metric: The metric to use to determine the best model.
      :type metric: str


   .. py:method:: set_model_prediction_params(model_id, prediction_config)

      Sets the model prediction config for the model

      :param model_id: Unique string identifier of the model to update.
      :type model_id: str
      :param prediction_config: Prediction configuration for the model.
      :type prediction_config: dict

      :returns: Model object after the prediction configuration is applied.
      :rtype: Model


   .. py:method:: retrain_model(model_id, deployment_ids = None, feature_group_ids = None, custom_algorithms = None, builtin_algorithms = None, custom_algorithm_configs = None, cpu_size = None, memory = None, training_config = None, algorithm_training_configs = None)

      Retrains the specified model, with an option to choose the deployments to which the retraining will be deployed.

      :param model_id: Unique string identifier of the model to retrain.
      :type model_id: str
      :param deployment_ids: List of unique string identifiers of deployments to automatically deploy to.
      :type deployment_ids: list
      :param feature_group_ids: List of feature group IDs provided by the user to train the model on.
      :type feature_group_ids: list
      :param custom_algorithms: List of user-defined algorithms to train. If not set, will honor the runs from the last time and applicable new custom algorithms.
      :type custom_algorithms: list
      :param builtin_algorithms: List of algorithm names or algorithm IDs of Abacus.AI built-in algorithms to train. If not set, will honor the runs from the last time and applicable new built-in algorithms.
      :type builtin_algorithms: list
      :param custom_algorithm_configs: User-defined training configs for each custom algorithm.
      :type custom_algorithm_configs: dict
      :param cpu_size: Size of the CPU for the user-defined algorithms during training.
      :type cpu_size: str
      :param memory: Memory (in GB) for the user-defined algorithms during training.
      :type memory: int
      :param training_config: The training config used to train this model.
      :type training_config: TrainingConfig
      :param algorithm_training_configs: List of algorithm specifc training configs that will be part of the model training AutoML run.
      :type algorithm_training_configs: list

      :returns: The model that is being retrained.
      :rtype: Model


   .. py:method:: delete_model(model_id)

      Deletes the specified model and all its versions. Models which are currently used in deployments cannot be deleted.

      :param model_id: Unique string identifier of the model to delete.
      :type model_id: str


   .. py:method:: delete_model_version(model_version)

      Deletes the specified model version. Model versions which are currently used in deployments cannot be deleted.

      :param model_version: The unique identifier of the model version to delete.
      :type model_version: str


   .. py:method:: export_model_artifact_as_feature_group(model_version, table_name, artifact_type = None)

      Exports metric artifact data for a model as a feature group.

      :param model_version: Unique string identifier for the version of the model.
      :type model_version: str
      :param table_name: Name of the feature group table to create.
      :type table_name: str
      :param artifact_type: eval artifact type to export.
      :type artifact_type: EvalArtifactType

      :returns: The created feature group.
      :rtype: FeatureGroup


   .. py:method:: set_default_model_algorithm(model_id = None, algorithm = None, data_cluster_type = None)

      Sets the model's algorithm to default for all new deployments

      :param model_id: Unique identifier of the model to set.
      :type model_id: str
      :param algorithm: Algorithm to pin in the model.
      :type algorithm: str
      :param data_cluster_type: Data cluster type to set the lead model for.
      :type data_cluster_type: str


   .. py:method:: get_custom_train_function_info(project_id, feature_group_names_for_training = None, training_data_parameter_name_override = None, training_config = None, custom_algorithm_config = None)

      Returns information about how to call the custom train function.

      :param project_id: The unique version ID of the project.
      :type project_id: str
      :param feature_group_names_for_training: A list of feature group table names to be used for training.
      :type feature_group_names_for_training: list
      :param training_data_parameter_name_override: Override from feature group type to parameter name in the train function.
      :type training_data_parameter_name_override: dict
      :param training_config: Training config for the options supported by the Abacus.ai platform.
      :type training_config: TrainingConfig
      :param custom_algorithm_config: User-defined config that can be serialized by JSON.
      :type custom_algorithm_config: dict

      :returns: Information about how to call the customer-provided train function.
      :rtype: CustomTrainFunctionInfo


   .. py:method:: export_custom_model_version(model_version, output_location, algorithm = None)

      Bundle custom model artifacts to a zip file, and export to the specified location.

      :param model_version: A unique string identifier for the model version.
      :type model_version: str
      :param output_location: Location to export the model artifacts results. For example, s3://a-bucket/
      :type output_location: str
      :param algorithm: The algorithm to be exported. Optional if there's only one custom algorithm in the model version.
      :type algorithm: str

      :returns: Object describing the export and its status.
      :rtype: ModelArtifactsExport


   .. py:method:: create_model_monitor(project_id, prediction_feature_group_id, training_feature_group_id = None, name = None, refresh_schedule = None, target_value = None, target_value_bias = None, target_value_performance = None, feature_mappings = None, model_id = None, training_feature_mappings = None, feature_group_base_monitor_config = None, feature_group_comparison_monitor_config = None)

      Runs a model monitor for the specified project.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param prediction_feature_group_id: The unique ID of the prediction data feature group.
      :type prediction_feature_group_id: str
      :param training_feature_group_id: The unique ID of the training data feature group.
      :type training_feature_group_id: str
      :param name: The name you want your model monitor to have. Defaults to "<Project Name> Model Monitor".
      :type name: str
      :param refresh_schedule: A cron-style string that describes a schedule in UTC to automatically retrain the created model monitor.
      :type refresh_schedule: str
      :param target_value: A target positive value for the label to compute bias and PR/AUC for performance page.
      :type target_value: str
      :param target_value_bias: A target positive value for the label to compute bias.
      :type target_value_bias: str
      :param target_value_performance: A target positive value for the label to compute PR curve/AUC for performance page.
      :type target_value_performance: str
      :param feature_mappings: A JSON map to override features for prediction_feature_group, where keys are column names and the values are feature data use types.
      :type feature_mappings: dict
      :param model_id: The unique ID of the model.
      :type model_id: str
      :param training_feature_mappings: A JSON map to override features for training_fature_group, where keys are column names and the values are feature data use types.
      :type training_feature_mappings: dict
      :param feature_group_base_monitor_config: Selection strategy for the feature_group 1 with the feature group version if selected.
      :type feature_group_base_monitor_config: dict
      :param feature_group_comparison_monitor_config: Selection strategy for the feature_group 1 with the feature group version if selected.
      :type feature_group_comparison_monitor_config: dict

      :returns: The new model monitor that was created.
      :rtype: ModelMonitor


   .. py:method:: rerun_model_monitor(model_monitor_id)

      Re-runs the specified model monitor.

      :param model_monitor_id: Unique string identifier of the model monitor to re-run.
      :type model_monitor_id: str

      :returns: The model monitor that is being re-run.
      :rtype: ModelMonitor


   .. py:method:: rename_model_monitor(model_monitor_id, name)

      Renames a model monitor

      :param model_monitor_id: Unique identifier of the model monitor to rename.
      :type model_monitor_id: str
      :param name: The new name to apply to the model monitor.
      :type name: str


   .. py:method:: delete_model_monitor(model_monitor_id)

      Deletes the specified Model Monitor and all its versions.

      :param model_monitor_id: Unique identifier of the Model Monitor to delete.
      :type model_monitor_id: str


   .. py:method:: delete_model_monitor_version(model_monitor_version)

      Deletes the specified model monitor version.

      :param model_monitor_version: Unique identifier of the model monitor version to delete.
      :type model_monitor_version: str


   .. py:method:: create_vision_drift_monitor(project_id, prediction_feature_group_id, training_feature_group_id, name, feature_mappings, training_feature_mappings, target_value_performance = None, refresh_schedule = None)

      Runs a vision drift monitor for the specified project.

      :param project_id: Unique string identifier of the project.
      :type project_id: str
      :param prediction_feature_group_id: Unique string identifier of the prediction data feature group.
      :type prediction_feature_group_id: str
      :param training_feature_group_id: Unique string identifier of the training data feature group.
      :type training_feature_group_id: str
      :param name: The name you want your model monitor to have. Defaults to "<Project Name> Model Monitor".
      :type name: str
      :param feature_mappings: A JSON map to override features for prediction_feature_group, where keys are column names and the values are feature data use types.
      :type feature_mappings: dict
      :param training_feature_mappings: A JSON map to override features for training_feature_group, where keys are column names and the values are feature data use types.
      :type training_feature_mappings: dict
      :param target_value_performance: A target positive value for the label to compute precision-recall curve/area under curve for performance page.
      :type target_value_performance: str
      :param refresh_schedule: A cron-style string that describes a schedule in UTC to automatically rerun the created vision drift monitor.
      :type refresh_schedule: str

      :returns: The new model monitor that was created.
      :rtype: ModelMonitor


   .. py:method:: create_nlp_drift_monitor(project_id, prediction_feature_group_id, training_feature_group_id, name, feature_mappings, training_feature_mappings, target_value_performance = None, refresh_schedule = None)

      Runs an NLP drift monitor for the specified project.

      :param project_id: Unique string identifier of the project.
      :type project_id: str
      :param prediction_feature_group_id: Unique string identifier of the prediction data feature group.
      :type prediction_feature_group_id: str
      :param training_feature_group_id: Unique string identifier of the training data feature group.
      :type training_feature_group_id: str
      :param name: The name you want your model monitor to have. Defaults to "<Project Name> Model Monitor".
      :type name: str
      :param feature_mappings: A JSON map to override features for prediction_feature_group, where keys are column names and the values are feature data use types.
      :type feature_mappings: dict
      :param training_feature_mappings: A JSON map to override features for training_feature_group, where keys are column names and the values are feature data use types.
      :type training_feature_mappings: dict
      :param target_value_performance: A target positive value for the label to compute precision-recall curve/area under curve for performance page.
      :type target_value_performance: str
      :param refresh_schedule: A cron-style string that describes a schedule in UTC to automatically rerun the created nlp drift monitor.
      :type refresh_schedule: str

      :returns: The new model monitor that was created.
      :rtype: ModelMonitor


   .. py:method:: create_forecasting_monitor(project_id, name, prediction_feature_group_id, training_feature_group_id, training_forecast_config, prediction_forecast_config, forecast_frequency = None, refresh_schedule = None)

      Runs a forecasting monitor for the specified project.

      :param project_id: Unique string identifier of the project.
      :type project_id: str
      :param name: The name you want your model monitor to have. Defaults to "<Project Name> Model Monitor".
      :type name: str
      :param prediction_feature_group_id: Unique string identifier of the prediction data feature group.
      :type prediction_feature_group_id: str
      :param training_feature_group_id: Unique string identifier of the training data feature group.
      :type training_feature_group_id: str
      :param training_forecast_config: The configuration for the training data.
      :type training_forecast_config: ForecastingMonitorConfig
      :param prediction_forecast_config: The configuration for the prediction data.
      :type prediction_forecast_config: ForecastingMonitorConfig
      :param forecast_frequency: The frequency of the forecast. Defaults to the frequency of the prediction data.
      :type forecast_frequency: str
      :param refresh_schedule: A cron-style string that describes a schedule in UTC to automatically rerun the created forecasting monitor.
      :type refresh_schedule: str

      :returns: The new model monitor that was created.
      :rtype: ModelMonitor


   .. py:method:: create_eda(project_id, feature_group_id, name, refresh_schedule = None, include_collinearity = False, include_data_consistency = False, collinearity_keys = None, primary_keys = None, data_consistency_test_config = None, data_consistency_reference_config = None, feature_mappings = None, forecast_frequency = None)

      Run an Exploratory Data Analysis (EDA) for the specified project.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_id: The unique ID of the prediction data feature group.
      :type feature_group_id: str
      :param name: The name you want your model monitor to have. Defaults to "<Project Name> EDA".
      :type name: str
      :param refresh_schedule: A cron-style string that describes a schedule in UTC to automatically retrain the created EDA.
      :type refresh_schedule: str
      :param include_collinearity: Set to True if the EDA type is collinearity.
      :type include_collinearity: bool
      :param include_data_consistency: Set to True if the EDA type is data consistency.
      :type include_data_consistency: bool
      :param collinearity_keys: List of features to use for collinearity
      :type collinearity_keys: list
      :param primary_keys: List of features that corresponds to the primary keys or item ids for the given feature group for Data Consistency analysis or Forecasting analysis respectively.
      :type primary_keys: list
      :param data_consistency_test_config: Test feature group version selection strategy for Data Consistency EDA type.
      :type data_consistency_test_config: dict
      :param data_consistency_reference_config: Reference feature group version selection strategy for Data Consistency EDA type.
      :type data_consistency_reference_config: dict
      :param feature_mappings: A JSON map to override features for the given feature_group, where keys are column names and the values are feature data use types. (In forecasting, used to set the timestamp column and target value)
      :type feature_mappings: dict
      :param forecast_frequency: The frequency of the data. It can be either HOURLY, DAILY, WEEKLY, MONTHLY, QUARTERLY, YEARLY.
      :type forecast_frequency: str

      :returns: The new EDA object that was created.
      :rtype: Eda


   .. py:method:: rerun_eda(eda_id)

      Reruns the specified EDA object.

      :param eda_id: Unique string identifier of the EDA object to rerun.
      :type eda_id: str

      :returns: The EDA object that is being rerun.
      :rtype: Eda


   .. py:method:: rename_eda(eda_id, name)

      Renames an EDA

      :param eda_id: Unique string identifier of the EDA to rename.
      :type eda_id: str
      :param name: The new name to apply to the model monitor.
      :type name: str


   .. py:method:: delete_eda(eda_id)

      Deletes the specified EDA and all its versions.

      :param eda_id: Unique string identifier of the EDA to delete.
      :type eda_id: str


   .. py:method:: delete_eda_version(eda_version)

      Deletes the specified EDA version.

      :param eda_version: Unique string identifier of the EDA version to delete.
      :type eda_version: str


   .. py:method:: create_holdout_analysis(name, model_id, feature_group_ids, model_version = None, algorithm = None)

      Create a holdout analysis for a model

      :param name: Name of the holdout analysis
      :type name: str
      :param model_id: ID of the model to create a holdout analysis for
      :type model_id: str
      :param feature_group_ids: List of feature group IDs to use for the holdout analysis
      :type feature_group_ids: list
      :param model_version: (optional) Version of the model to use for the holdout analysis
      :type model_version: str
      :param algorithm: (optional) ID of algorithm to use for the holdout analysis
      :type algorithm: str

      :returns: The created holdout analysis
      :rtype: HoldoutAnalysis


   .. py:method:: rerun_holdout_analysis(holdout_analysis_id, model_version = None, algorithm = None)

      Rerun a holdout analysis. A different model version and algorithm can be specified which should be under the same model.

      :param holdout_analysis_id: ID of the holdout analysis to rerun
      :type holdout_analysis_id: str
      :param model_version: (optional) Version of the model to use for the holdout analysis
      :type model_version: str
      :param algorithm: (optional) ID of algorithm to use for the holdout analysis
      :type algorithm: str

      :returns: The created holdout analysis version
      :rtype: HoldoutAnalysisVersion


   .. py:method:: create_monitor_alert(project_id, model_monitor_id, alert_name, condition_config, action_config)

      Create a monitor alert for the given conditions and monitor

      :param project_id: Unique string identifier for the project.
      :type project_id: str
      :param model_monitor_id: Unique string identifier for the model monitor created under the project.
      :type model_monitor_id: str
      :param alert_name: Name of the alert.
      :type alert_name: str
      :param condition_config: Condition to run the actions for the alert.
      :type condition_config: AlertConditionConfig
      :param action_config: Configuration for the action of the alert.
      :type action_config: AlertActionConfig

      :returns: Object describing the monitor alert.
      :rtype: MonitorAlert


   .. py:method:: update_monitor_alert(monitor_alert_id, alert_name = None, condition_config = None, action_config = None)

      Update monitor alert

      :param monitor_alert_id: Unique identifier of the monitor alert.
      :type monitor_alert_id: str
      :param alert_name: Name of the alert.
      :type alert_name: str
      :param condition_config: Condition to run the actions for the alert.
      :type condition_config: AlertConditionConfig
      :param action_config: Configuration for the action of the alert.
      :type action_config: AlertActionConfig

      :returns: Object describing the monitor alert.
      :rtype: MonitorAlert


   .. py:method:: run_monitor_alert(monitor_alert_id)

      Reruns a given monitor alert from latest monitor instance

      :param monitor_alert_id: Unique identifier of a monitor alert.
      :type monitor_alert_id: str

      :returns: Object describing the monitor alert.
      :rtype: MonitorAlert


   .. py:method:: delete_monitor_alert(monitor_alert_id)

      Delets a monitor alert

      :param monitor_alert_id: The unique string identifier of the alert to delete.
      :type monitor_alert_id: str


   .. py:method:: create_prediction_operator(name, project_id, source_code = None, predict_function_name = None, initialize_function_name = None, feature_group_ids = None, cpu_size = None, memory = None, package_requirements = None, use_gpu = False)

      Create a new prediction operator.

      :param name: Name of the prediction operator.
      :type name: str
      :param project_id: The unique ID of the associated project.
      :type project_id: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the function `predictFunctionName`, and the function 'initializeFunctionName' if defined.
      :type source_code: str
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions.
      :type predict_function_name: str
      :param initialize_function_name: Name of the optional initialize function found in the source code. This function will generate anything used by predictions, based on input feature groups.
      :type initialize_function_name: str
      :param feature_group_ids: List of feature groups that are supplied to the initialize function as parameters. Each of the parameters are materialized Dataframes. The order should match the initialize function's parameters.
      :type feature_group_ids: list
      :param cpu_size: Size of the CPU for the prediction operator.
      :type cpu_size: str
      :param memory: Memory (in GB) for the  prediction operator.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: list
      :param use_gpu: Whether this prediction operator needs gpu.
      :type use_gpu: bool

      :returns: The created prediction operator object.
      :rtype: PredictionOperator


   .. py:method:: update_prediction_operator(prediction_operator_id, name = None, feature_group_ids = None, source_code = None, initialize_function_name = None, predict_function_name = None, cpu_size = None, memory = None, package_requirements = None, use_gpu = None)

      Update an existing prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str
      :param name: Name of the prediction operator.
      :type name: str
      :param feature_group_ids: List of feature groups that are supplied to the initialize function as parameters. Each of the parameters are materialized Dataframes. The order should match the initialize function's parameters.
      :type feature_group_ids: list
      :param source_code: Contents of a valid Python source code file. The source code should contain the function `predictFunctionName`, and the function 'initializeFunctionName' if defined.
      :type source_code: str
      :param initialize_function_name: Name of the optional initialize function found in the source code. This function will generate anything used by predictions, based on input feature groups.
      :type initialize_function_name: str
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions.
      :type predict_function_name: str
      :param cpu_size: Size of the CPU for the prediction operator.
      :type cpu_size: str
      :param memory: Memory (in GB) for the  prediction operator.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: list
      :param use_gpu: Whether this prediction operator needs gpu.
      :type use_gpu: bool

      :returns: The updated prediction operator object.
      :rtype: PredictionOperator


   .. py:method:: delete_prediction_operator(prediction_operator_id)

      Delete an existing prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str


   .. py:method:: deploy_prediction_operator(prediction_operator_id, auto_deploy = True)

      Deploy the prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str
      :param auto_deploy: Flag to enable the automatic deployment when a new prediction operator version is created.
      :type auto_deploy: bool

      :returns: The created deployment object.
      :rtype: Deployment


   .. py:method:: create_prediction_operator_version(prediction_operator_id)

      Create a new version of the prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str

      :returns: The created prediction operator version object.
      :rtype: PredictionOperatorVersion


   .. py:method:: delete_prediction_operator_version(prediction_operator_version)

      Delete a prediction operator version.

      :param prediction_operator_version: The unique ID of the prediction operator version.
      :type prediction_operator_version: str


   .. py:method:: create_deployment(name = None, model_id = None, model_version = None, algorithm = None, feature_group_id = None, project_id = None, description = None, calls_per_second = None, auto_deploy = True, start = True, enable_batch_streaming_updates = False, skip_metrics_check = False, model_deployment_config = None, deployment_config = None)

      Creates a deployment with the specified name and description for the specified model or feature group.

      A Deployment makes the trained model or feature group available for prediction requests.


      :param name: The name of the deployment.
      :type name: str
      :param model_id: The unique ID associated with the model.
      :type model_id: str
      :param model_version: The unique ID associated with the model version to deploy.
      :type model_version: str
      :param algorithm: The unique ID associated with the algorithm to deploy.
      :type algorithm: str
      :param feature_group_id: The unique ID associated with a feature group.
      :type feature_group_id: str
      :param project_id: The unique ID associated with a project.
      :type project_id: str
      :param description: The description for the deployment.
      :type description: str
      :param calls_per_second: The number of calls per second the deployment can handle.
      :type calls_per_second: int
      :param auto_deploy: Flag to enable the automatic deployment when a new Model Version finishes training.
      :type auto_deploy: bool
      :param start: If true, will start the deployment; otherwise will create offline
      :type start: bool
      :param enable_batch_streaming_updates: Flag to enable marking the feature group deployment to have a background process cache streamed in rows for quicker lookup.
      :type enable_batch_streaming_updates: bool
      :param skip_metrics_check: Flag to skip metric regression with this current deployment
      :type skip_metrics_check: bool
      :param model_deployment_config: The deployment config for model to deploy
      :type model_deployment_config: dict
      :param deployment_config: Additional parameters specific to different use_cases
      :type deployment_config: dict

      :returns: The new model or feature group deployment.
      :rtype: Deployment


   .. py:method:: create_deployment_token(project_id, name = None)

      Creates a deployment token for the specified project.

      Deployment tokens are used to authenticate requests to the prediction APIs and are scoped to the project level.


      :param project_id: The unique string identifier associated with the project.
      :type project_id: str
      :param name: The name of the deployment token.
      :type name: str

      :returns: The deployment token.
      :rtype: DeploymentAuthToken


   .. py:method:: update_deployment(deployment_id, description = None, auto_deploy = None, skip_metrics_check = None)

      Updates a deployment's properties.

      :param deployment_id: Unique identifier of the deployment to update.
      :type deployment_id: str
      :param description: The new description for the deployment.
      :type description: str
      :param auto_deploy: Flag to enable the automatic deployment when a new Model Version finishes training.
      :type auto_deploy: bool
      :param skip_metrics_check: Flag to skip metric regression with this current deployment. This field is only relevant when auto_deploy is on
      :type skip_metrics_check: bool


   .. py:method:: rename_deployment(deployment_id, name)

      Updates a deployment's name

      :param deployment_id: Unique string identifier for the deployment to update.
      :type deployment_id: str
      :param name: The new deployment name.
      :type name: str


   .. py:method:: set_auto_deployment(deployment_id, enable = None)

      Enable or disable auto deployment for the specified deployment.

      When a model is scheduled to retrain, deployments with auto deployment enabled will be marked to automatically promote the new model version. After the newly trained model completes, a check on its metrics in comparison to the currently deployed model version will be performed. If the metrics are comparable or better, the newly trained model version is automatically promoted. If not, it will be marked as a failed model version promotion with an error indicating poor metrics performance.


      :param deployment_id: The unique ID associated with the deployment.
      :type deployment_id: str
      :param enable: Enable or disable the autoDeploy property of the deployment.
      :type enable: bool


   .. py:method:: set_deployment_model_version(deployment_id, model_version, algorithm = None, model_deployment_config = None)

      Promotes a model version and/or algorithm to be the active served deployment version

      :param deployment_id: A unique identifier for the deployment.
      :type deployment_id: str
      :param model_version: A unique identifier for the model version.
      :type model_version: str
      :param algorithm: The algorithm to use for the model version. If not specified, the algorithm will be inferred from the model version.
      :type algorithm: str
      :param model_deployment_config: The deployment configuration for the model to deploy.
      :type model_deployment_config: dict


   .. py:method:: set_deployment_feature_group_version(deployment_id, feature_group_version)

      Promotes a feature group version to be served in the deployment.

      :param deployment_id: Unique string identifier for the deployment.
      :type deployment_id: str
      :param feature_group_version: Unique string identifier for the feature group version.
      :type feature_group_version: str


   .. py:method:: set_deployment_prediction_operator_version(deployment_id, prediction_operator_version)

      Promotes a prediction operator version to be served in the deployment.

      :param deployment_id: Unique string identifier for the deployment.
      :type deployment_id: str
      :param prediction_operator_version: Unique string identifier for the prediction operator version.
      :type prediction_operator_version: str


   .. py:method:: start_deployment(deployment_id)

      Restarts the specified deployment that was previously suspended.

      :param deployment_id: A unique string identifier associated with the deployment.
      :type deployment_id: str


   .. py:method:: stop_deployment(deployment_id)

      Stops the specified deployment.

      :param deployment_id: Unique string identifier of the deployment to be stopped.
      :type deployment_id: str


   .. py:method:: delete_deployment(deployment_id)

      Deletes the specified deployment. The deployment's models will not be affected. Note that the deployments are not recoverable after they are deleted.

      :param deployment_id: Unique string identifier of the deployment to delete.
      :type deployment_id: str


   .. py:method:: delete_deployment_token(deployment_token)

      Deletes the specified deployment token.

      :param deployment_token: The deployment token to delete.
      :type deployment_token: str


   .. py:method:: set_deployment_feature_group_export_file_connector_output(deployment_id, file_format = None, output_location = None)

      Sets the export output for the Feature Group Deployment to be a file connector.

      :param deployment_id: The ID of the deployment for which the export type is set.
      :type deployment_id: str
      :param file_format: The type of export output, either CSV or JSON.
      :type file_format: str
      :param output_location: The file connector (cloud) location where the output should be exported.
      :type output_location: str


   .. py:method:: set_deployment_feature_group_export_database_connector_output(deployment_id, database_connector_id, object_name, write_mode, database_feature_mapping, id_column = None, additional_id_columns = None)

      Sets the export output for the Feature Group Deployment to a Database connector.

      :param deployment_id: The ID of the deployment for which the export type is set.
      :type deployment_id: str
      :param database_connector_id: The unique string identifier of the database connector used.
      :type database_connector_id: str
      :param object_name: The object of the database connector to write to.
      :type object_name: str
      :param write_mode: The write mode to use when writing to the database connector, either UPSERT or INSERT.
      :type write_mode: str
      :param database_feature_mapping: The column/feature pairs mapping the features to the database columns.
      :type database_feature_mapping: dict
      :param id_column: The id column to use as the upsert key.
      :type id_column: str
      :param additional_id_columns: For database connectors which support it, a list of additional ID columns to use as a complex key for upserting.
      :type additional_id_columns: list


   .. py:method:: remove_deployment_feature_group_export_output(deployment_id)

      Removes the export type that is set for the Feature Group Deployment

      :param deployment_id: The ID of the deployment for which the export type is set.
      :type deployment_id: str


   .. py:method:: create_deployment_alert(deployment_id, alert_name, condition_config, action_config)

      Create a deployment alert for the given conditions.

      Only support batch prediction usage now.


      :param deployment_id: Unique string identifier for the deployment.
      :type deployment_id: str
      :param alert_name: Name of the alert.
      :type alert_name: str
      :param condition_config: Condition to run the actions for the alert.
      :type condition_config: AlertConditionConfig
      :param action_config: Configuration for the action of the alert.
      :type action_config: AlertActionConfig

      :returns: Object describing the deployment alert.
      :rtype: MonitorAlert


   .. py:method:: create_refresh_policy(name, cron, refresh_type, project_id = None, dataset_ids = [], feature_group_id = None, model_ids = [], deployment_ids = [], batch_prediction_ids = [], prediction_metric_ids = [], model_monitor_ids = [], notebook_id = None, prediction_operator_id = None, feature_group_export_config = None)

      Creates a refresh policy with a particular cron pattern and refresh type. The cron is specified in UTC time.

      A refresh policy allows for the scheduling of a set of actions at regular intervals. This can be useful for periodically updating data that needs to be re-imported into the project for retraining.


      :param name: The name of the refresh policy.
      :type name: str
      :param cron: A cron-like string specifying the frequency of the refresh policy in UTC time.
      :type cron: str
      :param refresh_type: The refresh type used to determine what is being refreshed, such as a single dataset, dataset and model, or more.
      :type refresh_type: str
      :param project_id: Optionally, a project ID can be specified so that all datasets, models, deployments, batch predictions, prediction metrics, model monitrs, and notebooks are captured at the instant the policy was created.
      :type project_id: str
      :param dataset_ids: Comma-separated list of dataset IDs.
      :type dataset_ids: list
      :param feature_group_id: Feature Group ID associated with refresh policy.
      :type feature_group_id: str
      :param model_ids: Comma-separated list of model IDs.
      :type model_ids: list
      :param deployment_ids: Comma-separated list of deployment IDs.
      :type deployment_ids: list
      :param batch_prediction_ids: Comma-separated list of batch prediction IDs.
      :type batch_prediction_ids: list
      :param prediction_metric_ids: Comma-separated list of prediction metric IDs.
      :type prediction_metric_ids: list
      :param model_monitor_ids: Comma-separated list of model monitor IDs.
      :type model_monitor_ids: list
      :param notebook_id: Notebook ID associated with refresh policy.
      :type notebook_id: str
      :param prediction_operator_id: Prediction Operator ID associated with refresh policy.
      :type prediction_operator_id: str
      :param feature_group_export_config: Feature group export configuration.
      :type feature_group_export_config: FeatureGroupExportConfig

      :returns: The created refresh policy.
      :rtype: RefreshPolicy


   .. py:method:: delete_refresh_policy(refresh_policy_id)

      Delete a refresh policy.

      :param refresh_policy_id: Unique string identifier associated with the refresh policy to delete.
      :type refresh_policy_id: str


   .. py:method:: pause_refresh_policy(refresh_policy_id)

      Pauses a refresh policy

      :param refresh_policy_id: Unique identifier associated with the refresh policy to be paused.
      :type refresh_policy_id: str


   .. py:method:: resume_refresh_policy(refresh_policy_id)

      Resumes a refresh policy

      :param refresh_policy_id: The unique ID associated with this refresh policy.
      :type refresh_policy_id: str


   .. py:method:: run_refresh_policy(refresh_policy_id)

      Force a run of the refresh policy.

      :param refresh_policy_id: Unique string identifier associated with the refresh policy to be run.
      :type refresh_policy_id: str


   .. py:method:: update_refresh_policy(refresh_policy_id, name = None, cron = None, feature_group_export_config = None)

      Update the name or cron string of a refresh policy

      :param refresh_policy_id: Unique string identifier associated with the refresh policy.
      :type refresh_policy_id: str
      :param name: Name of the refresh policy to be updated.
      :type name: str
      :param cron: Cron string describing the schedule from the refresh policy to be updated.
      :type cron: str
      :param feature_group_export_config: Feature group export configuration to update a feature group refresh policy.
      :type feature_group_export_config: FeatureGroupExportConfig

      :returns: Updated refresh policy.
      :rtype: RefreshPolicy


   .. py:method:: lookup_features(deployment_token, deployment_id, query_data, limit_results = None, result_columns = None)

      Returns the feature group deployed in the feature store project.

      :param deployment_token: A deployment token used to authenticate access to created deployments. This token only authorizes predictions on deployments in this project, so it can be safely embedded inside an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the key is the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the value is the unique value of the same entity.
      :type query_data: dict
      :param limit_results: If provided, will limit the number of results to the value specified.
      :type limit_results: int
      :param result_columns: If provided, will limit the columns present in each result to the columns specified in this list.
      :type result_columns: list


   .. py:method:: predict(deployment_token, deployment_id, query_data)

      Returns a prediction for Predictive Modeling

      :param deployment_token: A deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, and is safe to embed in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the key is the column name (e.g. a column with name 'user_id' in the dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed, and the value is the unique value of the same entity.
      :type query_data: dict


   .. py:method:: predict_multiple(deployment_token, deployment_id, query_data)

      Returns a list of predictions for predictive modeling.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, and is safe to embed in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A list of dictionaries, where the 'key' is the column name (e.g. a column with name 'user_id' in the dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed, and the 'value' is the unique value of the same entity.
      :type query_data: list


   .. py:method:: predict_from_datasets(deployment_token, deployment_id, query_data)

      Returns a list of predictions for Predictive Modeling.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the 'key' is the source dataset name, and the 'value' is a list of records corresponding to the dataset rows.
      :type query_data: dict


   .. py:method:: predict_lead(deployment_token, deployment_id, query_data, explain_predictions = False, explainer_type = None)

      Returns the probability of a user being a lead based on their interaction with the service/product and their own attributes (e.g. income, assets, credit score, etc.). Note that the inputs to this method, wherever applicable, should be the column names in the dataset mapped to the column mappings in our system (e.g. column 'user_id' mapped to mapping 'LEAD_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary containing user attributes and/or user's interaction data with the product/service (e.g. number of clicks, items in cart, etc.).
      :type query_data: dict
      :param explain_predictions: Will explain predictions for leads
      :type explain_predictions: bool
      :param explainer_type: Type of explainer to use for explanations
      :type explainer_type: str


   .. py:method:: predict_churn(deployment_token, deployment_id, query_data)

      Returns the probability of a user to churn out in response to their interactions with the item/product/service. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'churn_result' mapped to mapping 'CHURNED_YN' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where the 'key' will be the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the 'value' will be the unique value of the same entity.
      :type query_data: dict


   .. py:method:: predict_takeover(deployment_token, deployment_id, query_data)

      Returns a probability for each class label associated with the types of fraud or a 'yes' or 'no' type label for the possibility of fraud. Note that the inputs to this method, wherever applicable, will be the column names in the dataset mapped to the column mappings in our system (e.g., column 'account_name' mapped to mapping 'ACCOUNT_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary containing account activity characteristics (e.g., login id, login duration, login type, IP address, etc.).
      :type query_data: dict


   .. py:method:: predict_fraud(deployment_token, deployment_id, query_data)

      Returns the probability of a transaction performed under a specific account being fraudulent or not. Note that the inputs to this method, wherever applicable, should be the column names in your dataset mapped to the column mappings in our system (e.g. column 'account_number' mapped to the mapping 'ACCOUNT_ID' in our system).

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary containing transaction attributes (e.g. credit card type, transaction location, transaction amount, etc.).
      :type query_data: dict


   .. py:method:: predict_class(deployment_token, deployment_id, query_data, threshold = None, threshold_class = None, thresholds = None, explain_predictions = False, fixed_features = None, nested = None, explainer_type = None)

      Returns a classification prediction

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the 'Key' is the column name (e.g. a column with the name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the 'Value' is the unique value of the same entity.
      :type query_data: dict
      :param threshold: A float value that is applied on the popular class label.
      :type threshold: float
      :param threshold_class: The label upon which the threshold is added (binary labels only).
      :type threshold_class: str
      :param thresholds: Maps labels to thresholds (multi-label classification only). Defaults to F1 optimal threshold if computed for the given class, else uses 0.5.
      :type thresholds: list
      :param explain_predictions: If True, returns the SHAP explanations for all input features.
      :type explain_predictions: bool
      :param fixed_features: A set of input features to treat as constant for explanations - only honored when the explainer type is KERNEL_EXPLAINER
      :type fixed_features: list
      :param nested: If specified generates prediction delta for each index of the specified nested feature.
      :type nested: str
      :param explainer_type: The type of explainer to use.
      :type explainer_type: str


   .. py:method:: predict_target(deployment_token, deployment_id, query_data, explain_predictions = False, fixed_features = None, nested = None, explainer_type = None)

      Returns a prediction from a classification or regression model. Optionally, includes explanations.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the 'key' is the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the 'value' is the unique value of the same entity.
      :type query_data: dict
      :param explain_predictions: If true, returns the SHAP explanations for all input features.
      :type explain_predictions: bool
      :param fixed_features: Set of input features to treat as constant for explanations - only honored when the explainer type is KERNEL_EXPLAINER
      :type fixed_features: list
      :param nested: If specified, generates prediction delta for each index of the specified nested feature.
      :type nested: str
      :param explainer_type: The type of explainer to use.
      :type explainer_type: str


   .. py:method:: get_anomalies(deployment_token, deployment_id, threshold = None, histogram = False)

      Returns a list of anomalies from the training dataset.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param threshold: The threshold score of what is an anomaly. Valid values are between 0.8 and 0.99.
      :type threshold: float
      :param histogram: If True, will return a histogram of the distribution of all points.
      :type histogram: bool


   .. py:method:: is_anomaly(deployment_token, deployment_id, query_data = None)

      Returns a list of anomaly attributes based on login information for a specified account. Note that the inputs to this method, wherever applicable, should be the column names in the dataset mapped to the column mappings in our system (e.g. column 'account_name' mapped to mapping 'ACCOUNT_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: The input data for the prediction.
      :type query_data: dict


   .. py:method:: get_event_anomaly_score(deployment_token, deployment_id, query_data = None)

      Returns an anomaly score for an event.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: The input data for the prediction.
      :type query_data: dict


   .. py:method:: get_forecast(deployment_token, deployment_id, query_data, future_data = None, num_predictions = None, prediction_start = None, explain_predictions = False, explainer_type = None, get_item_data = False)

      Returns a list of forecasts for a given entity under the specified project deployment. Note that the inputs to the deployed model will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'holiday_yn' mapped to mapping 'FUTURE' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where 'Key' will be the column name (e.g. a column with name 'store_id' in your dataset) mapped to the column mapping ITEM_ID that uniquely identifies the entity against which forecasting is performed and 'Value' will be the unique value of the same entity.
      :type query_data: dict
      :param future_data: This will be a list of values known ahead of time that are relevant for forecasting (e.g. State Holidays, National Holidays, etc.). Each element is a dictionary, where the key and the value both will be of type 'str'. For example future data entered for a Store may be [{"Holiday":"No", "Promo":"Yes", "Date": "2015-07-31 00:00:00"}].
      :type future_data: list
      :param num_predictions: The number of timestamps to predict in the future.
      :type num_predictions: int
      :param prediction_start: The start date for predictions (e.g., "2015-08-01T00:00:00" as input for mid-night of 2015-08-01).
      :type prediction_start: str
      :param explain_predictions: Will explain predictions for forecasting
      :type explain_predictions: bool
      :param explainer_type: Type of explainer to use for explanations
      :type explainer_type: str
      :param get_item_data: Will return the data corresponding to items in query
      :type get_item_data: bool


   .. py:method:: get_k_nearest(deployment_token, deployment_id, vector, k = None, distance = None, include_score = False, catalog_id = None)

      Returns the k nearest neighbors for the provided embedding vector.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param vector: Input vector to perform the k nearest neighbors with.
      :type vector: list
      :param k: Overrideable number of items to return.
      :type k: int
      :param distance: Specify the distance function to use when finding nearest neighbors.
      :type distance: str
      :param include_score: If True, will return the score alongside the resulting embedding value.
      :type include_score: bool
      :param catalog_id: An optional parameter honored only for embeddings that provide a catalog id
      :type catalog_id: str


   .. py:method:: get_multiple_k_nearest(deployment_token, deployment_id, queries)

      Returns the k nearest neighbors for the queries provided.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param queries: List of mappings of format {"catalogId": "cat0", "vectors": [...], "k": 20, "distance": "euclidean"}. See `getKNearest` for additional information about the supported parameters.
      :type queries: list


   .. py:method:: get_labels(deployment_token, deployment_id, query_data)

      Returns a list of scored labels for a document.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: Dictionary where key is "Content" and value is the text from which entities are to be extracted.
      :type query_data: dict


   .. py:method:: get_entities_from_pdf(deployment_token, deployment_id, pdf = None, doc_id = None, return_extracted_features = False, verbose = False, save_extracted_features = None)

      Extracts text from the provided PDF and returns a list of recognized labels and their scores.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param pdf: (Optional) The pdf to predict on. One of pdf or docId must be specified.
      :type pdf: io.TextIOBase
      :param doc_id: (Optional) The pdf to predict on. One of pdf or docId must be specified.
      :type doc_id: str
      :param return_extracted_features: (Optional) If True, will return all extracted features (e.g. all tokens in a page) from the PDF. Default is False.
      :type return_extracted_features: bool
      :param verbose: (Optional) If True, will return all the extracted tokens probabilities for all the trained labels. Default is False.
      :type verbose: bool
      :param save_extracted_features: (Optional) If True, will save extracted features (i.e. page tokens) so that they can be fetched using the prediction docId. Default is False.
      :type save_extracted_features: bool


   .. py:method:: get_recommendations(deployment_token, deployment_id, query_data, num_items = None, page = None, exclude_item_ids = None, score_field = None, scaling_factors = None, restrict_items = None, exclude_items = None, explore_fraction = None, diversity_attribute_name = None, diversity_max_results_per_value = None)

      Returns a list of recommendations for a given user under the specified project deployment. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'time' mapped to mapping 'TIMESTAMP' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where 'Key' will be the column name (e.g. a column with name 'user_name' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the user against which recommendations are made and 'Value' will be the unique value of the same item. For example, if you have the column name 'user_name' mapped to the column mapping 'USER_ID', then the query must have the exact same column name (user_name) as key and the name of the user (John Doe) as value.
      :type query_data: dict
      :param num_items: The number of items to recommend on one page. By default, it is set to 50 items per page.
      :type num_items: int
      :param page: The page number to be displayed. For example, let's say that the num_items is set to 10 with the total recommendations list size of 50 recommended items, then an input value of 2 in the 'page' variable will display a list of items that rank from 11th to 20th.
      :type page: int
      :param exclude_item_ids: [DEPRECATED]
      :type exclude_item_ids: list
      :param score_field: The relative item scores are returned in a separate field named with the same name as the key (score_field) for this argument.
      :type score_field: str
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted.  Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there's a type of item that might be less popular but you want to promote it or there's an item that always comes up and you want to demote it.
      :type scaling_factors: list
      :param restrict_items: It allows you to restrict the recommendations to certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", "value3", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1", "value3", ...]" to which to restrict the recommendations to. Let's take an example where the input to restrict_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. This input will restrict the recommendations to SUVs and Sedans. This type of restriction is particularly useful if there's a list of items that you know is of use in some particular scenario and you want to restrict the recommendations only to that list.
      :type restrict_items: list
      :param exclude_items: It allows you to exclude certain items from the list of recommendations. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" to exclude from the recommendations. Let's take an example where the input to exclude_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. The resulting recommendation list will exclude all SUVs and Sedans. This is
      :type exclude_items: list
      :param explore_fraction: Explore fraction.
      :type explore_fraction: float
      :param diversity_attribute_name: item attribute column name which is used to ensure diversity of prediction results.
      :type diversity_attribute_name: str
      :param diversity_max_results_per_value: maximum number of results per value of diversity_attribute_name.
      :type diversity_max_results_per_value: int


   .. py:method:: get_personalized_ranking(deployment_token, deployment_id, query_data, preserve_ranks = None, preserve_unknown_items = False, scaling_factors = None)

      Returns a list of items with personalized promotions for a given user under the specified project deployment. Note that the inputs to this method, wherever applicable, should be the column names in the dataset mapped to the column mappings in our system (e.g. column 'item_code' mapped to mapping 'ITEM_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This should be a dictionary with two key-value pairs. The first pair represents a 'Key' where the column name (e.g. a column with name 'user_id' in the dataset) mapped to the column mapping USER_ID uniquely identifies the user against whom a prediction is made and a 'Value' which is the identifier value for that user. The second pair will have a 'Key' which will be the name of the column name (e.g. movie_name) mapped to ITEM_ID (unique item identifier) and a 'Value' which will be a list of identifiers that uniquely identifies those items.
      :type query_data: dict
      :param preserve_ranks: List of dictionaries of format {"column": "col0", "values": ["value0, value1"]}, where the ranks of items in query_data is preserved for all the items in "col0" with values, "value0" and "value1". This option is useful when the desired items are being recommended in the desired order and the ranks for those items need to be kept unchanged during recommendation generation.
      :type preserve_ranks: list
      :param preserve_unknown_items: If true, any items that are unknown to the model, will not be reranked, and the original position in the query will be preserved.
      :type preserve_unknown_items: bool
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted. Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there's a type of item that might be less popular but you want to promote it or there's an item that always comes up and you want to demote it.
      :type scaling_factors: list


   .. py:method:: get_ranked_items(deployment_token, deployment_id, query_data, preserve_ranks = None, preserve_unknown_items = False, score_field = None, scaling_factors = None, diversity_attribute_name = None, diversity_max_results_per_value = None)

      Returns a list of re-ranked items for a selected user when a list of items is required to be reranked according to the user's preferences. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'item_code' mapped to mapping 'ITEM_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary with two key-value pairs. The first pair represents a 'Key' where the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID uniquely identifies the user against whom a prediction is made and a 'Value' which is the identifier value for that user. The second pair will have a 'Key' which will be the name of the column name (e.g. movie_name) mapped to ITEM_ID (unique item identifier) and a 'Value' which will be a list of identifiers that uniquely identifies those items.
      :type query_data: dict
      :param preserve_ranks: List of dictionaries of format {"column": "col0", "values": ["value0, value1"]}, where the ranks of items in query_data is preserved for all the items in "col0" with values, "value0" and "value1". This option is useful when the desired items are being recommended in the desired order and the ranks for those items need to be kept unchanged during recommendation generation.
      :type preserve_ranks: list
      :param preserve_unknown_items: If true, any items that are unknown to the model, will not be reranked, and the original position in the query will be preserved
      :type preserve_unknown_items: bool
      :param score_field: The relative item scores are returned in a separate field named with the same name as the key (score_field) for this argument.
      :type score_field: str
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted. Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there is a type of item that might be less popular but you want to promote it or there is an item that always comes up and you want to demote it.
      :type scaling_factors: list
      :param diversity_attribute_name: item attribute column name which is used to ensure diversity of prediction results.
      :type diversity_attribute_name: str
      :param diversity_max_results_per_value: maximum number of results per value of diversity_attribute_name.
      :type diversity_max_results_per_value: int


   .. py:method:: get_related_items(deployment_token, deployment_id, query_data, num_items = None, page = None, scaling_factors = None, restrict_items = None, exclude_items = None)

      Returns a list of related items for a given item under the specified project deployment. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'item_code' mapped to mapping 'ITEM_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where the 'key' will be the column name (e.g. a column with name 'user_name' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the user against which related items are determined and the 'value' will be the unique value of the same item. For example, if you have the column name 'user_name' mapped to the column mapping 'USER_ID', then the query must have the exact same column name (user_name) as key and the name of the user (John Doe) as value.
      :type query_data: dict
      :param num_items: The number of items to recommend on one page. By default, it is set to 50 items per page.
      :type num_items: int
      :param page: The page number to be displayed. For example, let's say that the num_items is set to 10 with the total recommendations list size of 50 recommended items, then an input value of 2 in the 'page' variable will display a list of items that rank from 11th to 20th.
      :type page: int
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted.  Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there's a type of item that might be less popular but you want to promote it or there's an item that always comes up and you want to demote it.
      :type scaling_factors: list
      :param restrict_items: It allows you to restrict the recommendations to certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", "value3", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1", "value3", ...]" to which to restrict the recommendations to. Let's take an example where the input to restrict_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. This input will restrict the recommendations to SUVs and Sedans. This type of restriction is particularly useful if there's a list of items that you know is of use in some particular scenario and you want to restrict the recommendations only to that list.
      :type restrict_items: list
      :param exclude_items: It allows you to exclude certain items from the list of recommendations. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" to exclude from the recommendations. Let's take an example where the input to exclude_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. The resulting recommendation list will exclude all SUVs and Sedans. This is particularly useful if there's a list of items that you know is of no use in some particular scenario and you don't want to show those items present in that list.
      :type exclude_items: list


   .. py:method:: get_chat_response(deployment_token, deployment_id, messages, llm_name = None, num_completion_tokens = None, system_message = None, temperature = 0.0, filter_key_values = None, search_score_cutoff = None, chat_config = None, ignore_documents = False)

      Return a chat response which continues the conversation based on the input messages and search results.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param messages: A list of chronologically ordered messages, starting with a user message and alternating sources. A message is a dict with attributes:     is_user (bool): Whether the message is from the user.      text (str): The message's text.
      :type messages: list
      :param llm_name: Name of the specific LLM backend to use to power the chat experience
      :type llm_name: str
      :param num_completion_tokens: Default for maximum number of tokens for chat answers
      :type num_completion_tokens: int
      :param system_message: The generative LLM system message
      :type system_message: str
      :param temperature: The generative LLM temperature
      :type temperature: float
      :param filter_key_values: A dictionary mapping column names to a list of values to restrict the retrieved search results.
      :type filter_key_values: dict
      :param search_score_cutoff: Cutoff for the document retriever score. Matching search results below this score will be ignored.
      :type search_score_cutoff: float
      :param chat_config: A dictionary specifying the query chat config override.
      :type chat_config: dict
      :param ignore_documents: If True, will ignore any documents and search results, and only use the messages to generate a response.
      :type ignore_documents: bool


   .. py:method:: get_conversation_response(deployment_id, message, deployment_conversation_id = None, external_session_id = None, llm_name = None, num_completion_tokens = None, system_message = None, temperature = 0.0, filter_key_values = None, search_score_cutoff = None, chat_config = None, ignore_documents = False)

      Return a conversation response which continues the conversation based on the input message and deployment conversation id (if exists).

      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param message: A message from the user
      :type message: str
      :param deployment_conversation_id: The unique identifier of a deployment conversation to continue. If not specified, a new one will be created.
      :type deployment_conversation_id: str
      :param external_session_id: The user supplied unique identifier of a deployment conversation to continue. If specified, we will use this instead of a internal deployment conversation id.
      :type external_session_id: str
      :param llm_name: Name of the specific LLM backend to use to power the chat experience
      :type llm_name: str
      :param num_completion_tokens: Default for maximum number of tokens for chat answers
      :type num_completion_tokens: int
      :param system_message: The generative LLM system message
      :type system_message: str
      :param temperature: The generative LLM temperature
      :type temperature: float
      :param filter_key_values: A dictionary mapping column names to a list of values to restrict the retrived search results.
      :type filter_key_values: dict
      :param search_score_cutoff: Cutoff for the document retriever score. Matching search results below this score will be ignored.
      :type search_score_cutoff: float
      :param chat_config: A dictionary specifiying the query chat config override.
      :type chat_config: dict
      :param ignore_documents: If True, will ignore any documents and search results, and only use the message and past conversation to generate a response.
      :type ignore_documents: bool


   .. py:method:: get_search_results(deployment_token, deployment_id, query_data, num = 15)

      Return the most relevant search results to the search query from the uploaded documents.

      :param deployment_token: A token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it can be securely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the key is "Content" and the value is the text from which entities are to be extracted.
      :type query_data: dict
      :param num: Number of search results to return.
      :type num: int


   .. py:method:: get_sentiment(deployment_token, deployment_id, document)

      Predicts sentiment on a document

      :param deployment_token: A token used to authenticate access to deployments created in this project. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for a deployment created under this project.
      :type deployment_id: str
      :param document: The document to be analyzed for sentiment.
      :type document: str


   .. py:method:: get_entailment(deployment_token, deployment_id, document)

      Predicts the classification of the document

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param document: The document to be classified.
      :type document: str


   .. py:method:: get_classification(deployment_token, deployment_id, document)

      Predicts the classification of the document

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param document: The document to be classified.
      :type document: str


   .. py:method:: get_summary(deployment_token, deployment_id, query_data)

      Returns a JSON of the predicted summary for the given document. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'text' mapped to mapping 'DOCUMENT' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: Raw data dictionary containing the required document data - must have a key 'document' corresponding to a DOCUMENT type text as value.
      :type query_data: dict


   .. py:method:: predict_language(deployment_token, deployment_id, query_data)

      Predicts the language of the text

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments within this project, making it safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: The input string to detect.
      :type query_data: str


   .. py:method:: get_assignments(deployment_token, deployment_id, query_data, forced_assignments = None, solve_time_limit_seconds = None, include_all_assignments = False)

      Get all positive assignments that match a query.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: Specifies the set of assignments being requested. The value for the key can be: 1. A simple scalar value, which is matched exactly 2. A list of values, which matches any element in the list 3. A dictionary with keys lower_in/lower_ex and upper_in/upper_ex, which matches values in an inclusive/exclusive range
      :type query_data: dict
      :param forced_assignments: Set of assignments to force and resolve before returning query results.
      :type forced_assignments: dict
      :param solve_time_limit_seconds: Maximum time in seconds to spend solving the query.
      :type solve_time_limit_seconds: float
      :param include_all_assignments: If True, will return all assignments, including assignments with value 0. Default is False.
      :type include_all_assignments: bool


   .. py:method:: get_alternative_assignments(deployment_token, deployment_id, query_data, add_constraints = None, solve_time_limit_seconds = None)

      Get alternative positive assignments for given query. Optimal assignments are ignored and the alternative assignments are returned instead.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: Specifies the set of assignments being requested. The value for the key can be: 1. A simple scalar value, which is matched exactly 2. A list of values, which matches any element in the list 3. A dictionary with keys lower_in/lower_ex and upper_in/upper_ex, which matches values in an inclusive/exclusive range
      :type query_data: dict
      :param add_constraints: List of constraints dict to apply to the query. The constraint dict should have the following keys: 1. query (dict): Specifies the set of assignments involved in the constraint. The format is same as query_data. 2. operator (str): Constraint operator '=' or '<=' or '>='. 3. constant (int): Constraint RHS constant value.
      :type add_constraints: list
      :param solve_time_limit_seconds: Maximum time in seconds to spend solving the query.
      :type solve_time_limit_seconds: float


   .. py:method:: check_constraints(deployment_token, deployment_id, query_data)

      Check for any constraints violated by the overrides.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: Assignment overrides to the solution.
      :type query_data: dict


   .. py:method:: predict_with_binary_data(deployment_token, deployment_id, blob)

      Make predictions for a given blob, e.g. image, audio

      :param deployment_token: A token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param blob: The multipart/form-data of the data.
      :type blob: io.TextIOBase


   .. py:method:: describe_image(deployment_token, deployment_id, image, categories, top_n = None)

      Describe the similarity between an image and a list of categories.

      :param deployment_token: Authentication token to access created deployments. This token is only authorized to predict on deployments in the current project, and can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: Unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param image: Image to describe.
      :type image: io.TextIOBase
      :param categories: List of candidate categories to compare with the image.
      :type categories: list
      :param top_n: Return the N most similar categories.
      :type top_n: int


   .. py:method:: get_text_from_document(deployment_token, deployment_id, document = None, adjust_doc_orientation = False, return_detected_images = False, save_predicted_pdf = False, save_extracted_features = False)

      Generate text from a document

      :param deployment_token: Authentication token to access created deployments. This token is only authorized to predict on deployments in the current project, and can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: Unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param document: Input document which can be an image, pdf, or word document (Some formats might not be supported yet)
      :type document: io.TextIOBase
      :param adjust_doc_orientation: (Optional) whether to detect the document page orientation and rotate it if needed.
      :type adjust_doc_orientation: bool
      :param return_detected_images: whether the detected images should be saved in docstore or not (if true, adds a docstore id to the response (may not be available for some algorithms))
      :type return_detected_images: bool
      :param save_predicted_pdf: (Optional) If True, will save the predicted pdf bytes so that they can be fetched using the prediction docId. Default is False.
      :type save_predicted_pdf: bool
      :param save_extracted_features: (Optional) If True, will save extracted features (i.e. page tokens) so that they can be fetched using the prediction docId. Default is False.
      :type save_extracted_features: bool


   .. py:method:: transcribe_audio(deployment_token, deployment_id, audio)

      Transcribe the audio

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to make predictions on deployments in this project, so it can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param audio: The audio to transcribe.
      :type audio: io.TextIOBase


   .. py:method:: classify_image(deployment_token, deployment_id, image = None, doc_id = None)

      Classify an image.

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier to a deployment created under the project.
      :type deployment_id: str
      :param image: The binary data of the image to classify. One of image or doc_id must be specified.
      :type image: io.TextIOBase
      :param doc_id: The document ID of the image. One of image or doc_id must be specified.
      :type doc_id: str


   .. py:method:: classify_pdf(deployment_token, deployment_id, pdf = None)

      Returns a classification prediction from a PDF

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param pdf: (Optional) The pdf to predict on. One of pdf or docId must be specified.
      :type pdf: io.TextIOBase


   .. py:method:: get_cluster(deployment_token, deployment_id, query_data)

      Predicts the cluster for given data.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where each 'key' represents a column name and its corresponding 'value' represents the value of that column. For Timeseries Clustering, the 'key' should be ITEM_ID, and its value should represent a unique item ID that needs clustering.
      :type query_data: dict


   .. py:method:: get_objects_from_image(deployment_token, deployment_id, image)

      Classify an image.

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier to a deployment created under the project.
      :type deployment_id: str
      :param image: The binary data of the image to detect objects from.
      :type image: io.TextIOBase


   .. py:method:: score_image(deployment_token, deployment_id, image)

      Score on image.

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier to a deployment created under the project.
      :type deployment_id: str
      :param image: The binary data of the image to get the score.
      :type image: io.TextIOBase


   .. py:method:: transfer_style(deployment_token, deployment_id, source_image, style_image)

      Change the source image to adopt the visual style from the style image.

      :param deployment_token: A token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param source_image: The source image to apply the makeup.
      :type source_image: io.TextIOBase
      :param style_image: The image that has the style as a reference.
      :type style_image: io.TextIOBase


   .. py:method:: generate_image(deployment_token, deployment_id, query_data)

      Generate an image from text prompt.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: Specifies the text prompt. For example, {'prompt': 'a cat'}
      :type query_data: dict


   .. py:method:: execute_agent(deployment_token, deployment_id, arguments = None, keyword_arguments = None)

      Executes a deployed AI agent function using the arguments as keyword arguments to the agent execute function.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param arguments: Positional arguments to the agent execute function.
      :type arguments: list
      :param keyword_arguments: A dictionary where each 'key' represents the paramter name and its corresponding 'value' represents the value of that parameter for the agent execute function.
      :type keyword_arguments: dict


   .. py:method:: execute_conversation_agent(deployment_token, deployment_id, arguments = None, keyword_arguments = None, deployment_conversation_id = None, external_session_id = None, regenerate = False, doc_infos = None)

      Executes a deployed AI agent function using the arguments as keyword arguments to the agent execute function.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param arguments: Positional arguments to the agent execute function.
      :type arguments: list
      :param keyword_arguments: A dictionary where each 'key' represents the paramter name and its corresponding 'value' represents the value of that parameter for the agent execute function.
      :type keyword_arguments: dict
      :param deployment_conversation_id: A unique string identifier for the deployment conversation used for the conversation.
      :type deployment_conversation_id: str
      :param external_session_id: A unique string identifier for the session used for the conversation. If both deployment_conversation_id and external_session_id are not provided, a new session will be created.
      :type external_session_id: str
      :param regenerate: If True, will regenerate the response from the last query.
      :type regenerate: bool
      :param doc_infos: An optional list of documents use for the conversation. A keyword 'doc_id' is expected to be present in each document for retrieving contents from docstore.
      :type doc_infos: list


   .. py:method:: lookup_matches(deployment_token, deployment_id, data = None, filters = None, num = None, result_columns = None, max_words = None, num_retrieval_margin_words = None, max_words_per_chunk = None, score_multiplier_column = None)

      Lookup document retrievers and return the matching documents from the document retriever deployed with given query.

      Original documents are splitted into chunks and stored in the document retriever. This lookup function will return the relevant chunks
      from the document retriever. The returned chunks could be expanded to include more words from the original documents and merged if they
      are overlapping, and permitted by the settings provided. The returned chunks are sorted by relevance.


      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments within this project, making it safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param data: The query to search for.
      :type data: str
      :param filters: A dictionary mapping column names to a list of values to restrict the retrieved search results.
      :type filters: dict
      :param num: If provided, will limit the number of results to the value specified.
      :type num: int
      :param result_columns: If provided, will limit the column properties present in each result to those specified in this list.
      :type result_columns: list
      :param max_words: If provided, will limit the total number of words in the results to the value specified.
      :type max_words: int
      :param num_retrieval_margin_words: If provided, will add this number of words from left and right of the returned chunks.
      :type num_retrieval_margin_words: int
      :param max_words_per_chunk: If provided, will limit the number of words in each chunk to the value specified. If the value provided is smaller than the actual size of chunk on disk, which is determined during document retriever creation, the actual size of chunk will be used. I.e, chunks looked up from document retrievers will not be split into smaller chunks during lookup due to this setting.
      :type max_words_per_chunk: int
      :param score_multiplier_column: If provided, will use the values in this column to modify the relevance score of the returned chunks. Values in this column must be numeric.
      :type score_multiplier_column: str

      :returns: The relevant documentation results found from the document retriever.
      :rtype: list[DocumentRetrieverLookupResult]


   .. py:method:: create_batch_prediction(deployment_id, table_name = None, name = None, global_prediction_args = None, explanations = False, output_format = None, output_location = None, database_connector_id = None, database_output_config = None, refresh_schedule = None, csv_input_prefix = None, csv_prediction_prefix = None, csv_explanations_prefix = None, output_includes_metadata = None, result_input_columns = None, input_feature_groups = None)

      Creates a batch prediction job description for the given deployment.

      :param deployment_id: Unique string identifier for the deployment.
      :type deployment_id: str
      :param table_name: Name of the feature group table to write the results of the batch prediction. Can only be specified if outputLocation and databaseConnectorId are not specified. If tableName is specified, the outputType will be enforced as CSV.
      :type table_name: str
      :param name: Name of the batch prediction job.
      :type name: str
      :param global_prediction_args: Batch Prediction args specific to problem type.
      :type global_prediction_args: BatchPredictionArgs
      :param explanations: If true, SHAP explanations will be provided for each prediction, if supported by the use case.
      :type explanations: bool
      :param output_format: Format of the batch prediction output (CSV or JSON).
      :type output_format: str
      :param output_location: Location to write the prediction results. Otherwise, results will be stored in Abacus.AI.
      :type output_location: str
      :param database_connector_id: Unique identifier of a Database Connection to write predictions to. Cannot be specified in conjunction with outputLocation.
      :type database_connector_id: str
      :param database_output_config: Key-value pair of columns/values to write to the database connector. Only available if databaseConnectorId is specified.
      :type database_output_config: dict
      :param refresh_schedule: Cron-style string that describes a schedule in UTC to automatically run the batch prediction.
      :type refresh_schedule: str
      :param csv_input_prefix: Prefix to prepend to the input columns, only applies when output format is CSV.
      :type csv_input_prefix: str
      :param csv_prediction_prefix: Prefix to prepend to the prediction columns, only applies when output format is CSV.
      :type csv_prediction_prefix: str
      :param csv_explanations_prefix: Prefix to prepend to the explanation columns, only applies when output format is CSV.
      :type csv_explanations_prefix: str
      :param output_includes_metadata: If true, output will contain columns including prediction start time, batch prediction version, and model version.
      :type output_includes_metadata: bool
      :param result_input_columns: If present, will limit result files or feature groups to only include columns present in this list.
      :type result_input_columns: list
      :param input_feature_groups: A dict of {'<feature_group_type>': '<feature_group_id>'} which overrides the default input data of that type for the Batch Prediction. Default input data is the training data that was used for training the deployed model.
      :type input_feature_groups: dict

      :returns: The batch prediction description.
      :rtype: BatchPrediction


   .. py:method:: start_batch_prediction(batch_prediction_id)

      Creates a new batch prediction version job for a given batch prediction job description.

      :param batch_prediction_id: The unique identifier of the batch prediction to create a new version of.
      :type batch_prediction_id: str

      :returns: The batch prediction version started by this method call.
      :rtype: BatchPredictionVersion


   .. py:method:: update_batch_prediction(batch_prediction_id, deployment_id = None, global_prediction_args = None, explanations = None, output_format = None, csv_input_prefix = None, csv_prediction_prefix = None, csv_explanations_prefix = None, output_includes_metadata = None, result_input_columns = None, name = None)

      Update a batch prediction job description.

      :param batch_prediction_id: Unique identifier of the batch prediction.
      :type batch_prediction_id: str
      :param deployment_id: Unique identifier of the deployment.
      :type deployment_id: str
      :param global_prediction_args: Batch Prediction args specific to problem type.
      :type global_prediction_args: BatchPredictionArgs
      :param explanations: If True, SHAP explanations for each prediction will be provided, if supported by the use case.
      :type explanations: bool
      :param output_format: If specified, sets the format of the batch prediction output (CSV or JSON).
      :type output_format: str
      :param csv_input_prefix: Prefix to prepend to the input columns, only applies when output format is CSV.
      :type csv_input_prefix: str
      :param csv_prediction_prefix: Prefix to prepend to the prediction columns, only applies when output format is CSV.
      :type csv_prediction_prefix: str
      :param csv_explanations_prefix: Prefix to prepend to the explanation columns, only applies when output format is CSV.
      :type csv_explanations_prefix: str
      :param output_includes_metadata: If True, output will contain columns including prediction start time, batch prediction version, and model version.
      :type output_includes_metadata: bool
      :param result_input_columns: If present, will limit result files or feature groups to only include columns present in this list.
      :type result_input_columns: list
      :param name: If present, will rename the batch prediction.
      :type name: str

      :returns: The batch prediction.
      :rtype: BatchPrediction


   .. py:method:: set_batch_prediction_file_connector_output(batch_prediction_id, output_format = None, output_location = None)

      Updates the file connector output configuration of the batch prediction

      :param batch_prediction_id: The unique identifier of the batch prediction.
      :type batch_prediction_id: str
      :param output_format: The format of the batch prediction output (CSV or JSON). If not specified, the default format will be used.
      :type output_format: str
      :param output_location: The location to write the prediction results. If not specified, results will be stored in Abacus.AI.
      :type output_location: str

      :returns: The batch prediction description.
      :rtype: BatchPrediction


   .. py:method:: set_batch_prediction_database_connector_output(batch_prediction_id, database_connector_id = None, database_output_config = None)

      Updates the database connector output configuration of the batch prediction

      :param batch_prediction_id: Unique string identifier of the batch prediction.
      :type batch_prediction_id: str
      :param database_connector_id: Unique string identifier of an Database Connection to write predictions to.
      :type database_connector_id: str
      :param database_output_config: Key-value pair of columns/values to write to the database connector.
      :type database_output_config: dict

      :returns: Description of the batch prediction.
      :rtype: BatchPrediction


   .. py:method:: set_batch_prediction_feature_group_output(batch_prediction_id, table_name)

      Creates a feature group and sets it as the batch prediction output.

      :param batch_prediction_id: Unique string identifier of the batch prediction.
      :type batch_prediction_id: str
      :param table_name: Name of the feature group table to create.
      :type table_name: str

      :returns: Batch prediction after the output has been applied.
      :rtype: BatchPrediction


   .. py:method:: set_batch_prediction_output_to_console(batch_prediction_id)

      Sets the batch prediction output to the console, clearing both the file connector and database connector configurations.

      :param batch_prediction_id: The unique identifier of the batch prediction.
      :type batch_prediction_id: str

      :returns: The batch prediction description.
      :rtype: BatchPrediction


   .. py:method:: set_batch_prediction_feature_group(batch_prediction_id, feature_group_type, feature_group_id = None)

      Sets the batch prediction input feature group.

      :param batch_prediction_id: Unique identifier of the batch prediction.
      :type batch_prediction_id: str
      :param feature_group_type: Enum string representing the feature group type to set. The type is based on the use case under which the feature group is being created (e.g. Catalog Attributes for personalized recommendation use case).
      :type feature_group_type: str
      :param feature_group_id: Unique identifier of the feature group to set as input to the batch prediction.
      :type feature_group_id: str

      :returns: Description of the batch prediction.
      :rtype: BatchPrediction


   .. py:method:: set_batch_prediction_dataset_remap(batch_prediction_id, dataset_id_remap)

      For the purpose of this batch prediction, will swap out datasets in the training feature groups

      :param batch_prediction_id: Unique string identifier of the batch prediction.
      :type batch_prediction_id: str
      :param dataset_id_remap: Key/value pairs of dataset ids to be replaced during the batch prediction.
      :type dataset_id_remap: dict

      :returns: Batch prediction object.
      :rtype: BatchPrediction


   .. py:method:: delete_batch_prediction(batch_prediction_id)

      Deletes a batch prediction and associated data, such as associated monitors.

      :param batch_prediction_id: Unique string identifier of the batch prediction.
      :type batch_prediction_id: str


   .. py:method:: add_user_item_interaction(streaming_token, dataset_id, timestamp, user_id, item_id, event_type, additional_attributes)

      Adds a user-item interaction record (data row) to a streaming dataset.

      :param streaming_token: The streaming token for authenticating requests to the dataset.
      :type streaming_token: str
      :param dataset_id: The unique string identifier for the streaming dataset to record data to.
      :type dataset_id: str
      :param timestamp: The Unix timestamp of the event.
      :type timestamp: int
      :param user_id: The unique identifier for the user.
      :type user_id: str
      :param item_id: The unique identifier for the items.
      :type item_id: list
      :param event_type: The event type.
      :type event_type: str
      :param additional_attributes: Attributes of the user interaction.
      :type additional_attributes: dict


   .. py:method:: upsert_user_attributes(streaming_token, dataset_id, user_id, user_attributes)

      Adds a user attribute record (data row) to a streaming dataset.

      Either the streaming dataset ID or the project ID is required.


      :param streaming_token: The streaming token for authenticating requests to the dataset.
      :type streaming_token: str
      :param dataset_id: The unique string identifier for the streaming dataset to record data to.
      :type dataset_id: str
      :param user_id: The unique identifier for the user.
      :type user_id: str
      :param user_attributes: Attributes of the user interaction.
      :type user_attributes: dict


   .. py:method:: upsert_item_attributes(streaming_token, dataset_id, item_id, item_attributes)

      Adds an item attributes record (data row) to a streaming dataset.

      Either the streaming dataset ID or the project ID is required.


      :param streaming_token: The streaming token for authenticating requests to the dataset.
      :type streaming_token: str
      :param dataset_id: The unique string identifier for the streaming dataset to record data to.
      :type dataset_id: str
      :param item_id: The unique identifier for the item.
      :type item_id: str
      :param item_attributes: Attributes of the item interaction.
      :type item_attributes: dict


   .. py:method:: add_multiple_user_item_interactions(streaming_token, dataset_id, interactions)

      Adds a user-item interaction record (data row) to a streaming dataset.

      :param streaming_token: The streaming token for authenticating requests to the dataset.
      :type streaming_token: str
      :param dataset_id: The unique string identifier of the streaming dataset to record data to.
      :type dataset_id: str
      :param interactions: List of interactions, each interaction of format {'userId': userId, 'timestamp': timestamp, 'itemId': itemId, 'eventType': eventType, 'additionalAttributes': {'attribute1': 'abc', 'attribute2': 123}}.
      :type interactions: list


   .. py:method:: upsert_multiple_user_attributes(streaming_token, dataset_id, upserts)

      Adds multiple user attributes records (data rows) to a streaming dataset.

      :param streaming_token: The streaming token for authenticating requests to the dataset.
      :type streaming_token: str
      :param dataset_id: A unique string identifier for the streaming dataset to record data to.
      :type dataset_id: str
      :param upserts: List of upserts, each upsert of format {'userId': userId, 'userAttributes': {'attribute1': 'abc', 'attribute2': 123}}.
      :type upserts: list


   .. py:method:: upsert_multiple_item_attributes(streaming_token, dataset_id, upserts)

      Adds multiple item attributes records (data rows) to a streaming dataset.

      :param streaming_token: The streaming token for authenticating requests to the dataset.
      :type streaming_token: str
      :param dataset_id: A unique string identifier for the streaming dataset to record data to.
      :type dataset_id: str
      :param upserts: A list of upserts, each upsert of format {'itemId': itemId, 'itemAttributes': {'attribute1': 'abc', 'attribute2': 123}}.
      :type upserts: list


   .. py:method:: upsert_item_embeddings(streaming_token, model_id, item_id, vector, catalog_id = None)

      Upserts an embedding vector for an item id for a model_id.

      :param streaming_token: The streaming token for authenticating requests to the model.
      :type streaming_token: str
      :param model_id: A unique string identifier for the model to upsert item embeddings to.
      :type model_id: str
      :param item_id: The item id for which its embeddings will be upserted.
      :type item_id: str
      :param vector: The embedding vector.
      :type vector: list
      :param catalog_id: The name of the catalog in the model to update.
      :type catalog_id: str


   .. py:method:: delete_item_embeddings(streaming_token, model_id, item_ids, catalog_id = None)

      Deletes KNN embeddings for a list of item IDs for a given model ID.

      :param streaming_token: The streaming token for authenticating requests to the model.
      :type streaming_token: str
      :param model_id: A unique string identifier for the model from which to delete item embeddings.
      :type model_id: str
      :param item_ids: A list of item IDs whose embeddings will be deleted.
      :type item_ids: list
      :param catalog_id: An optional name to specify which catalog in a model to update.
      :type catalog_id: str


   .. py:method:: upsert_multiple_item_embeddings(streaming_token, model_id, upserts, catalog_id = None)

      Upserts a knn embedding for multiple item ids for a model_id.

      :param streaming_token: The streaming token for authenticating requests to the model.
      :type streaming_token: str
      :param model_id: The unique string identifier of the model to upsert item embeddings to.
      :type model_id: str
      :param upserts: A list of dictionaries of the form {'itemId': ..., 'vector': [...]} for each upsert.
      :type upserts: list
      :param catalog_id: Name of the catalog in the model to update.
      :type catalog_id: str


   .. py:method:: append_data(feature_group_id, streaming_token, data)

      Appends new data into the feature group for a given lookup key recordId.

      :param feature_group_id: Unique string identifier for the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: The streaming token for authenticating requests.
      :type streaming_token: str
      :param data: The data to record as a JSON object.
      :type data: dict


   .. py:method:: upsert_multiple_data(feature_group_id, streaming_token, data)

      Update new data into the feature group for a given lookup key recordId if the recordId is found; otherwise, insert new data into the feature group.

      :param feature_group_id: Unique string identifier for the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: The streaming token for authenticating requests.
      :type streaming_token: str
      :param data: The data to record, as a list of JSON objects.
      :type data: list


   .. py:method:: append_multiple_data(feature_group_id, streaming_token, data)

      Appends new data into the feature group for a given lookup key recordId.

      :param feature_group_id: Unique string identifier of the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: Streaming token for authenticating requests.
      :type streaming_token: str
      :param data: Data to record, as a list of JSON objects.
      :type data: list


   .. py:method:: upsert_data(feature_group_id, streaming_token = None, data = None)

      Update new data into the feature group for a given lookup key record ID if the record ID is found; otherwise, insert new data into the feature group.

      :param feature_group_id: A unique string identifier of the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: Optional streaming token for authenticating requests if upserting to streaming FG.
      :type streaming_token: str
      :param data: The data to record, in JSON format.
      :type data: dict

      :returns: The feature group row that was upserted.
      :rtype: FeatureGroupRow


   .. py:method:: upsert_online_data(feature_group_id, data)

      Update new data into the feature group for a given lookup key record ID if the record ID is found; otherwise, insert new data into the feature group.

      :param feature_group_id: A unique string identifier of the online feature group to record data to.
      :type feature_group_id: str
      :param data: The data to record, in JSON format.
      :type data: dict

      :returns: The feature group row that was upserted.
      :rtype: FeatureGroupRow


   .. py:method:: delete_data(feature_group_id, primary_key)

      Deletes a row from the feature group given the primary key

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param primary_key: The primary key value for which to delete the feature group row
      :type primary_key: str


   .. py:method:: add_feature_group_document(feature_group_id, document)

      Adds a document to the feature group.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param document: The multipart/form-data of the document to add to the feature group.
      :type document: io.TextIOBase

      :returns: The feature group document that was added.
      :rtype: FeatureGroupDocument


   .. py:method:: describe_feature_group_row_process_by_key(deployment_id, primary_key_value)

      Gets the feature group row process.

      :param deployment_id: The deployment id
      :type deployment_id: str
      :param primary_key_value: The primary key value
      :type primary_key_value: str

      :returns: An object representing the feature group row process
      :rtype: FeatureGroupRowProcess


   .. py:method:: list_feature_group_row_processes(deployment_id, limit = None, status = None)

      Gets a list of feature group row processes.

      :param deployment_id: The deployment id for the process
      :type deployment_id: str
      :param limit: The maximum number of processes to return. Defaults to None.
      :type limit: int
      :param status: The status of the processes to return. Defaults to None.
      :type status: str

      :returns: A list of object representing the feature group row process
      :rtype: list[FeatureGroupRowProcess]


   .. py:method:: get_feature_group_row_process_summary(deployment_id)

      Gets a summary of the statuses of the individual feature group processes.

      :param deployment_id: The deployment id for the process
      :type deployment_id: str

      :returns: An object representing the summary of the statuses of the individual feature group processes
      :rtype: FeatureGroupRowProcessSummary


   .. py:method:: reset_feature_group_row_process_by_key(deployment_id, primary_key_value)

      Resets a feature group row process so that it can be reprocessed

      :param deployment_id: The deployment id
      :type deployment_id: str
      :param primary_key_value: The primary key value
      :type primary_key_value: str

      :returns: An object representing the feature group row process.
      :rtype: FeatureGroupRowProcess


   .. py:method:: get_feature_group_row_process_logs_by_key(deployment_id, primary_key_value)

      Gets the logs for a feature group row process

      :param deployment_id: The deployment id
      :type deployment_id: str
      :param primary_key_value: The primary key value
      :type primary_key_value: str

      :returns: An object representing the logs for the feature group row process
      :rtype: FeatureGroupRowProcessLogs


   .. py:method:: create_python_function(name, source_code = None, function_name = None, function_variable_mappings = None, package_requirements = None, function_type = 'FEATURE_GROUP')

      Creates a custom Python function that is reusable.

      :param name: The name to identify the Python function. Must be a valid Python identifier.
      :type name: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the transform feature group functions. A list of allowed imports and system libraries for each language is specified in the user functions documentation section.
      :type source_code: str
      :param function_name: The name of the Python function.
      :type function_name: str
      :param function_variable_mappings: List of Python function arguments.
      :type function_variable_mappings: list
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param function_type: Type of Python function to create. Default is FEATURE_GROUP, but can also be PLOTLY_FIG.
      :type function_type: str

      :returns: The Python function that can be used (e.g. for feature group transform).
      :rtype: PythonFunction


   .. py:method:: update_python_function(name, source_code = None, function_name = None, function_variable_mappings = None, package_requirements = None)

      Update custom python function with user inputs for the given python function.

      :param name: The name to identify the Python function. Must be a valid Python identifier.
      :type name: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the transform feature group functions. A list of allowed imports and system libraries for each language is specified in the user functions documentation section.
      :type source_code: str
      :param function_name: The name of the Python function within `source_code`.
      :type function_name: str
      :param function_variable_mappings: List of arguments required by `function_name`.
      :type function_variable_mappings: list
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list

      :returns: The Python function object.
      :rtype: PythonFunction


   .. py:method:: delete_python_function(name)

      Removes an existing Python function.

      :param name: The name to identify the Python function. Must be a valid Python identifier.
      :type name: str


   .. py:method:: create_pipeline(pipeline_name, project_id = None, cron = None, is_prod = None)

      Creates a pipeline for executing multiple steps.

      :param pipeline_name: The name of the pipeline, which should be unique to the organization.
      :type pipeline_name: str
      :param project_id: A unique string identifier for the pipeline.
      :type project_id: str
      :param cron: A cron-like string specifying the frequency of pipeline reruns.
      :type cron: str
      :param is_prod: Whether the pipeline is a production pipeline or not.
      :type is_prod: bool

      :returns: An object that describes a Pipeline.
      :rtype: Pipeline


   .. py:method:: describe_pipeline(pipeline_id)

      Describes a given pipeline.

      :param pipeline_id: The ID of the pipeline to describe.
      :type pipeline_id: str

      :returns: An object describing a Pipeline
      :rtype: Pipeline


   .. py:method:: describe_pipeline_by_name(pipeline_name)

      Describes a given pipeline.

      :param pipeline_name: The name of the pipeline to describe.
      :type pipeline_name: str

      :returns: An object describing a Pipeline
      :rtype: Pipeline


   .. py:method:: update_pipeline(pipeline_id, project_id = None, pipeline_variable_mappings = None, cron = None, is_prod = None)

      Updates a pipeline for executing multiple steps.

      :param pipeline_id: The ID of the pipeline to update.
      :type pipeline_id: str
      :param project_id: A unique string identifier for the pipeline.
      :type project_id: str
      :param pipeline_variable_mappings: List of Python function arguments for the pipeline.
      :type pipeline_variable_mappings: list
      :param cron: A cron-like string specifying the frequency of the scheduled pipeline runs.
      :type cron: str
      :param is_prod: Whether the pipeline is a production pipeline or not.
      :type is_prod: bool

      :returns: An object that describes a Pipeline.
      :rtype: Pipeline


   .. py:method:: rename_pipeline(pipeline_id, pipeline_name)

      Renames a pipeline.

      :param pipeline_id: The ID of the pipeline to rename.
      :type pipeline_id: str
      :param pipeline_name: The new name of the pipeline.
      :type pipeline_name: str

      :returns: An object that describes a Pipeline.
      :rtype: Pipeline


   .. py:method:: delete_pipeline(pipeline_id)

      Deletes a pipeline.

      :param pipeline_id: The ID of the pipeline to delete.
      :type pipeline_id: str


   .. py:method:: list_pipeline_versions(pipeline_id, limit = 200)

      Lists the pipeline versions for a specified pipeline

      :param pipeline_id: The ID of the pipeline to list versions for.
      :type pipeline_id: str
      :param limit: The maximum number of pipeline versions to return.
      :type limit: int

      :returns: A list of pipeline versions.
      :rtype: list[PipelineVersion]


   .. py:method:: run_pipeline(pipeline_id, pipeline_variable_mappings = None)

      Runs a specified pipeline with the arguments provided.

      :param pipeline_id: The ID of the pipeline to run.
      :type pipeline_id: str
      :param pipeline_variable_mappings: List of Python function arguments for the pipeline.
      :type pipeline_variable_mappings: list

      :returns: The object describing the pipeline
      :rtype: PipelineVersion


   .. py:method:: reset_pipeline_version(pipeline_version, steps = None, include_downstream_steps = True)

      Reruns a pipeline version for the given steps and downstream steps if specified.

      :param pipeline_version: The id of the pipeline version.
      :type pipeline_version: str
      :param steps: List of pipeline step names to rerun.
      :type steps: list
      :param include_downstream_steps: Whether to rerun downstream steps from the steps you have passed
      :type include_downstream_steps: bool

      :returns: Object describing the pipeline version
      :rtype: PipelineVersion


   .. py:method:: create_pipeline_step(pipeline_id, step_name, function_name = None, source_code = None, step_input_mappings = None, output_variable_mappings = None, step_dependencies = None, package_requirements = None, cpu_size = None, memory = None)

      Creates a step in a given pipeline.

      :param pipeline_id: The ID of the pipeline to run.
      :type pipeline_id: str
      :param step_name: The name of the step.
      :type step_name: str
      :param function_name: The name of the Python function.
      :type function_name: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the transform feature group functions. A list of allowed imports and system libraries for each language is specified in the user functions documentation section.
      :type source_code: str
      :param step_input_mappings: List of Python function arguments.
      :type step_input_mappings: list
      :param output_variable_mappings: List of Python function ouputs.
      :type output_variable_mappings: list
      :param step_dependencies: List of step names this step depends on.
      :type step_dependencies: list
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param cpu_size: Size of the CPU for the step function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the step function.
      :type memory: int

      :returns: Object describing the pipeline.
      :rtype: Pipeline


   .. py:method:: delete_pipeline_step(pipeline_step_id)

      Deletes a step from a pipeline.

      :param pipeline_step_id: The ID of the pipeline step.
      :type pipeline_step_id: str


   .. py:method:: update_pipeline_step(pipeline_step_id, function_name = None, source_code = None, step_input_mappings = None, output_variable_mappings = None, step_dependencies = None, package_requirements = None, cpu_size = None, memory = None)

      Creates a step in a given pipeline.

      :param pipeline_step_id: The ID of the pipeline_step to update.
      :type pipeline_step_id: str
      :param function_name: The name of the Python function.
      :type function_name: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the transform feature group functions. A list of allowed imports and system libraries for each language is specified in the user functions documentation section.
      :type source_code: str
      :param step_input_mappings: List of Python function arguments.
      :type step_input_mappings: list
      :param output_variable_mappings: List of Python function ouputs.
      :type output_variable_mappings: list
      :param step_dependencies: List of step names this step depends on.
      :type step_dependencies: list
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param cpu_size: Size of the CPU for the step function.
      :type cpu_size: str
      :param memory: Memory (in GB) for the step function.
      :type memory: int

      :returns: Object describing the pipeline.
      :rtype: PipelineStep


   .. py:method:: rename_pipeline_step(pipeline_step_id, step_name)

      Renames a step in a given pipeline.

      :param pipeline_step_id: The ID of the pipeline_step to update.
      :type pipeline_step_id: str
      :param step_name: The name of the step.
      :type step_name: str

      :returns: Object describing the pipeline.
      :rtype: PipelineStep


   .. py:method:: unset_pipeline_refresh_schedule(pipeline_id)

      Deletes the refresh schedule for a given pipeline.

      :param pipeline_id: The id of the pipeline.
      :type pipeline_id: str

      :returns: Object describing the pipeline.
      :rtype: Pipeline


   .. py:method:: pause_pipeline_refresh_schedule(pipeline_id)

      Pauses the refresh schedule for a given pipeline.

      :param pipeline_id: The id of the pipeline.
      :type pipeline_id: str

      :returns: Object describing the pipeline.
      :rtype: Pipeline


   .. py:method:: resume_pipeline_refresh_schedule(pipeline_id)

      Resumes the refresh schedule for a given pipeline.

      :param pipeline_id: The id of the pipeline.
      :type pipeline_id: str

      :returns: Object describing the pipeline.
      :rtype: Pipeline


   .. py:method:: skip_pending_pipeline_version_steps(pipeline_version)

      Skips pending steps in a pipeline version.

      :param pipeline_version: The id of the pipeline version.
      :type pipeline_version: str

      :returns: Object describing the pipeline version
      :rtype: PipelineVersion


   .. py:method:: create_graph_dashboard(project_id, name, python_function_ids = None)

      Create a plot dashboard given selected python plots

      :param project_id: A unique string identifier for the plot dashboard.
      :type project_id: str
      :param name: The name of the dashboard.
      :type name: str
      :param python_function_ids: A list of unique string identifiers for the python functions to be used in the graph dashboard.
      :type python_function_ids: list

      :returns: An object describing the graph dashboard.
      :rtype: GraphDashboard


   .. py:method:: delete_graph_dashboard(graph_dashboard_id)

      Deletes a graph dashboard

      :param graph_dashboard_id: Unique string identifier for the graph dashboard to be deleted.
      :type graph_dashboard_id: str


   .. py:method:: update_graph_dashboard(graph_dashboard_id, name = None, python_function_ids = None)

      Updates a graph dashboard

      :param graph_dashboard_id: Unique string identifier for the graph dashboard to update.
      :type graph_dashboard_id: str
      :param name: Name of the dashboard.
      :type name: str
      :param python_function_ids: List of unique string identifiers for the Python functions to be used in the graph dashboard.
      :type python_function_ids: list

      :returns: An object describing the graph dashboard.
      :rtype: GraphDashboard


   .. py:method:: add_graph_to_dashboard(python_function_id, graph_dashboard_id, function_variable_mappings = None, name = None)

      Add a python plot function to a dashboard

      :param python_function_id: Unique string identifier for the Python function.
      :type python_function_id: str
      :param graph_dashboard_id: Unique string identifier for the graph dashboard to update.
      :type graph_dashboard_id: str
      :param function_variable_mappings: List of arguments to be supplied to the function as parameters, in the format [{'name': 'function_argument', 'variable_type': 'FEATURE_GROUP', 'value': 'name_of_feature_group'}].
      :type function_variable_mappings: dict
      :param name: Name of the added python plot
      :type name: str

      :returns: An object describing the graph dashboard.
      :rtype: GraphDashboard


   .. py:method:: update_graph_to_dashboard(graph_reference_id, function_variable_mappings = None, name = None)

      Update a python plot function to a dashboard

      :param graph_reference_id: A unique string identifier for the graph reference.
      :type graph_reference_id: str
      :param function_variable_mappings: A list of arguments to be supplied to the Python function as parameters in the format [{'name': 'function_argument', 'variable_type': 'FEATURE_GROUP', 'value': 'name_of_feature_group'}].
      :type function_variable_mappings: list
      :param name: The updated name for the graph
      :type name: str

      :returns: An object describing the graph dashboard.
      :rtype: GraphDashboard


   .. py:method:: delete_graph_from_dashboard(graph_reference_id)

      Deletes a python plot function from a dashboard

      :param graph_reference_id: Unique String Identifier for the graph
      :type graph_reference_id: str


   .. py:method:: create_algorithm(name, problem_type, source_code = None, training_data_parameter_names_mapping = None, training_config_parameter_name = None, train_function_name = None, predict_function_name = None, predict_many_function_name = None, initialize_function_name = None, config_options = None, is_default_enabled = False, project_id = None, use_gpu = False, package_requirements = None)

      Creates a custom algorithm that is re-usable for model training.

      :param name: The name to identify the algorithm; only uppercase letters, numbers, and underscores are allowed.
      :type name: str
      :param problem_type: The type of problem this algorithm will work on.
      :type problem_type: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the train/predict/predict_many/initialize functions. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type source_code: str
      :param training_data_parameter_names_mapping: The mapping from feature group types to training data parameter names in the train function.
      :type training_data_parameter_names_mapping: dict
      :param training_config_parameter_name: The train config parameter name in the train function.
      :type training_config_parameter_name: str
      :param train_function_name: Name of the function found in the source code that will be executed to train the model. It is not executed when this function is run.
      :type train_function_name: str
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions through the model. It is not executed when this function is run.
      :type predict_function_name: str
      :param predict_many_function_name: Name of the function found in the source code that will be executed for batch prediction of the model. It is not executed when this function is run.
      :type predict_many_function_name: str
      :param initialize_function_name: Name of the function found in the source code to initialize the trained model before using it to make predictions using the model.
      :type initialize_function_name: str
      :param config_options: Map dataset types and configs to train function parameter names.
      :type config_options: dict
      :param is_default_enabled: Whether to train with the algorithm by default.
      :type is_default_enabled: bool
      :param project_id: The unique version ID of the project.
      :type project_id: str
      :param use_gpu: Whether this algorithm needs to run on GPU.
      :type use_gpu: bool
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list

      :returns: The new custom model that can be used for training.
      :rtype: Algorithm


   .. py:method:: delete_algorithm(algorithm)

      Deletes the specified customer algorithm.

      :param algorithm: The name of the algorithm to delete.
      :type algorithm: str


   .. py:method:: update_algorithm(algorithm, source_code = None, training_data_parameter_names_mapping = None, training_config_parameter_name = None, train_function_name = None, predict_function_name = None, predict_many_function_name = None, initialize_function_name = None, config_options = None, is_default_enabled = None, use_gpu = None, package_requirements = None)

      Update a custom algorithm for the given algorithm name. If source code is provided, all function names for the source code must also be provided.

      :param algorithm: The name to identify the algorithm. Only uppercase letters, numbers, and underscores are allowed.
      :type algorithm: str
      :param source_code: Contents of a valid Python source code file. The source code should contain the train/predict/predict_many/initialize functions. A list of allowed imports and system libraries for each language is specified in the user functions documentation section.
      :type source_code: str
      :param training_data_parameter_names_mapping: The mapping from feature group types to training data parameter names in the train function.
      :type training_data_parameter_names_mapping: dict
      :param training_config_parameter_name: The train config parameter name in the train function.
      :type training_config_parameter_name: str
      :param train_function_name: Name of the function found in the source code that will be executed to train the model. It is not executed when this function is run.
      :type train_function_name: str
      :param predict_function_name: Name of the function found in the source code that will be executed to run predictions through the model. It is not executed when this function is run.
      :type predict_function_name: str
      :param predict_many_function_name: Name of the function found in the source code that will be executed for batch prediction of the model. It is not executed when this function is run.
      :type predict_many_function_name: str
      :param initialize_function_name: Name of the function found in the source code to initialize the trained model before using it to make predictions using the model.
      :type initialize_function_name: str
      :param config_options: Map dataset types and configs to train function parameter names.
      :type config_options: dict
      :param is_default_enabled: Whether to train with the algorithm by default.
      :type is_default_enabled: bool
      :param use_gpu: Whether this algorithm needs to run on GPU.
      :type use_gpu: bool
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list

      :returns: The new custom model can be used for training.
      :rtype: Algorithm


   .. py:method:: list_builtin_algorithms(project_id, feature_group_ids, training_config = None)

      Return list of built-in algorithms based on given input data and training config.

      :param project_id: Unique string identifier associated with the project.
      :type project_id: str
      :param feature_group_ids: List of feature group IDs specifying input data.
      :type feature_group_ids: list
      :param training_config: The training config to be used for model training.
      :type training_config: TrainingConfig

      :returns: List of applicable builtin algorithms.
      :rtype: list[Algorithm]


   .. py:method:: create_custom_loss_function_with_source_code(name, loss_function_type, loss_function_name, loss_function_source_code)

      Registers a new custom loss function which can be used as an objective function during model training.

      :param name: A name for the loss, unique per organization. Must be 50 characters or fewer, and can contain only underscores, numbers, and uppercase alphabets.
      :type name: str
      :param loss_function_type: The category of problems that this loss would be applicable to, e.g. REGRESSION_DL_TF, CLASSIFICATION_DL_TF, etc.
      :type loss_function_type: str
      :param loss_function_name: The name of the function whose full source code is passed in loss_function_source_code.
      :type loss_function_name: str
      :param loss_function_source_code: Python source code string of the function.
      :type loss_function_source_code: str

      :returns: A description of the registered custom loss function.
      :rtype: CustomLossFunction


   .. py:method:: update_custom_loss_function_with_source_code(name, loss_function_name, loss_function_source_code)

      Updates a previously registered custom loss function with a new function implementation.

      :param name: Name of the registered custom loss.
      :type name: str
      :param loss_function_name: Name of the function whose full source code is passed in loss_function_source_code.
      :type loss_function_name: str
      :param loss_function_source_code: Python source code string of the function.
      :type loss_function_source_code: str

      :returns: A description of the updated custom loss function.
      :rtype: CustomLossFunction


   .. py:method:: delete_custom_loss_function(name)

      Deletes a previously registered custom loss function.

      :param name: The name of the custom loss function to be deleted.
      :type name: str


   .. py:method:: create_custom_metric(name, problem_type, custom_metric_function_name = None, source_code = None)

      Registers a new custom metric which can be used as an evaluation metric for the trained model.

      :param name: A unique name for the metric, with a limit of 50 characters. Only underscores, numbers, and uppercase alphabets are allowed.
      :type name: str
      :param problem_type: The problem type that this metric would be applicable to, e.g. REGRESSION, FORECASTING, etc.
      :type problem_type: str
      :param custom_metric_function_name: The name of the function whose full source code is passed in source_code.
      :type custom_metric_function_name: str
      :param source_code: The full source code of the custom metric function. This is required if custom_metric_function_name is passed.
      :type source_code: str

      :returns: The newly created custom metric.
      :rtype: CustomMetric


   .. py:method:: update_custom_metric(name, custom_metric_function_name, source_code)

      Updates a previously registered custom metric with a new function implementation.

      :param name: Name of the registered custom metric.
      :type name: str
      :param custom_metric_function_name: Name of the function whose full source code is passed in `source_code`.
      :type custom_metric_function_name: str
      :param source_code: Python source code string of the function.
      :type source_code: str

      :returns: A description of the updated custom metric.
      :rtype: CustomMetric


   .. py:method:: delete_custom_metric(name)

      Deletes a previously registered custom metric.

      :param name: The name of the custom metric to be deleted.
      :type name: str


   .. py:method:: create_module(name, source_code = None)

      Creates a module that's re-usable in customer's code, e.g. python function, bring your own algorithm and etc.

      :param name: The name to identify the module, only lower case letters and underscore allowed.
      :type name: str
      :param source_code: Contents of a valid python source code file.
      :type source_code: str

      :returns: The new module
      :rtype: Module


   .. py:method:: delete_module(name)

      Deletes the specified customer module.

      :param name: The name of the custom module to delete.
      :type name: str


   .. py:method:: update_module(name, source_code = None)

      Update the module.

      :param name: The name to identify the module.
      :type name: str
      :param source_code: Contents of a valid python source code file.
      :type source_code: str

      :returns: The updated module.
      :rtype: Module


   .. py:method:: create_organization_secret(secret_key, value)

      Creates a secret which can be accessed in functions and notebooks.

      :param secret_key: The secret key.
      :type secret_key: str
      :param value: The secret value.
      :type value: str

      :returns: The created secret.
      :rtype: OrganizationSecret


   .. py:method:: delete_organization_secret(secret_key)

      Deletes a secret.

      :param secret_key: The secret key.
      :type secret_key: str


   .. py:method:: update_organization_secret(secret_key, value)

      Updates a secret.

      :param secret_key: The secret key.
      :type secret_key: str
      :param value: The secret value.
      :type value: str

      :returns: The updated secret.
      :rtype: OrganizationSecret


   .. py:method:: set_natural_language_explanation(short_explanation, long_explanation, feature_group_id = None, feature_group_version = None, model_id = None)

      Saves the natural language explanation of an artifact with given ID. The artifact can be - Feature Group or Feature Group Version

      :param short_explanation: succinct explanation of the artifact with given ID
      :type short_explanation: str
      :param long_explanation: verbose explanation of the artifact with given ID
      :type long_explanation: str
      :param feature_group_id: A unique string identifier associated with the Feature Group.
      :type feature_group_id: str
      :param feature_group_version: A unique string identifier associated with the Feature Group Version.
      :type feature_group_version: str
      :param model_id: A unique string identifier associated with the Model.
      :type model_id: str


   .. py:method:: create_chat_session(project_id = None, name = None)

      Creates a chat session with Data Science Co-pilot.

      :param project_id: The unique project identifier this chat session belongs to
      :type project_id: str
      :param name: The name of the chat session. Defaults to the project name.
      :type name: str

      :returns: The chat session with Data Science Co-pilot
      :rtype: ChatSession


   .. py:method:: delete_chat_message(chat_session_id, message_index)

      Deletes a message in a chat session and its associated response.

      :param chat_session_id: Unique ID of the chat session.
      :type chat_session_id: str
      :param message_index: The index of the chat message within the UI.
      :type message_index: int


   .. py:method:: export_chat_session(chat_session_id)

      Exports a chat session to an HTML file

      :param chat_session_id: Unique ID of the chat session.
      :type chat_session_id: str


   .. py:method:: rename_chat_session(chat_session_id, name)

      Renames a chat session with Data Science Co-pilot.

      :param chat_session_id: Unique ID of the chat session.
      :type chat_session_id: str
      :param name: The new name of the chat session.
      :type name: str


   .. py:method:: suggest_abacus_apis(query, verbosity = 1, limit = 5)

      Suggests several Abacus APIs that are most relevant to the supplied natural language query.

      :param query: The natural language query to find Abacus APIs for
      :type query: str
      :param verbosity: The verbosity level of the suggested Abacus APIs. Ranges from 0 to 2, with 0 being the least verbose and 2 being the most verbose.
      :type verbosity: int
      :param limit: The maximum number of APIs to return
      :type limit: int

      :returns: A list of suggested Abacus APIs
      :rtype: list[AbacusApi]


   .. py:method:: create_deployment_conversation(deployment_id, name, deployment_token = None)

      Creates a deployment conversation.

      :param deployment_id: The deployment this conversation belongs to.
      :type deployment_id: str
      :param name: The name of the conversation.
      :type name: str
      :param deployment_token: The deployment token to authenticate access to the deployment. This is required if not logged in.
      :type deployment_token: str

      :returns: The deployment conversation.
      :rtype: DeploymentConversation


   .. py:method:: delete_deployment_conversation(deployment_conversation_id, deployment_id = None, deployment_token = None)

      Delete a Deployment Conversation.

      :param deployment_conversation_id: A unique string identifier associated with the deployment conversation.
      :type deployment_conversation_id: str
      :param deployment_id: The deployment this conversation belongs to. This is required if not logged in.
      :type deployment_id: str
      :param deployment_token: The deployment token to authenticate access to the deployment. This is required if not logged in.
      :type deployment_token: str


   .. py:method:: clear_deployment_conversation(deployment_conversation_id = None, external_session_id = None, deployment_id = None, deployment_token = None, user_message_indices = None)

      Clear the message history of a Deployment Conversation.

      :param deployment_conversation_id: A unique string identifier associated with the deployment conversation.
      :type deployment_conversation_id: str
      :param external_session_id: The external session id associated with the deployment conversation.
      :type external_session_id: str
      :param deployment_id: The deployment this conversation belongs to. This is required if not logged in.
      :type deployment_id: str
      :param deployment_token: The deployment token to authenticate access to the deployment. This is required if not logged in.
      :type deployment_token: str
      :param user_message_indices: Optional list of user message indices to clear. The associated bot response will also be cleared. If not provided, all messages will be cleared.
      :type user_message_indices: list


   .. py:method:: set_deployment_conversation_feedback(deployment_conversation_id, message_index, is_useful = None, is_not_useful = None, feedback = None, feedback_type = None, deployment_id = None, deployment_token = None)

      Sets a deployment conversation message as useful or not useful

      :param deployment_conversation_id: A unique string identifier associated with the deployment conversation.
      :type deployment_conversation_id: str
      :param message_index: The index of the deployment conversation message
      :type message_index: int
      :param is_useful: If the message is useful. If true, the message is useful. If false, clear the useful flag.
      :type is_useful: bool
      :param is_not_useful: If the message is not useful. If true, the message is not useful. If set to false, clear the useful flag.
      :type is_not_useful: bool
      :param feedback: Optional feedback on why the message is useful or not useful
      :type feedback: str
      :param feedback_type: Optional feedback type
      :type feedback_type: str
      :param deployment_id: The deployment this conversation belongs to. This is required if not logged in.
      :type deployment_id: str
      :param deployment_token: The deployment token to authenticate access to the deployment. This is required if not logged in.
      :type deployment_token: str


   .. py:method:: rename_deployment_conversation(deployment_conversation_id, name, deployment_id = None, deployment_token = None)

      Rename a Deployment Conversation.

      :param deployment_conversation_id: A unique string identifier associated with the deployment conversation.
      :type deployment_conversation_id: str
      :param name: The new name of the conversation.
      :type name: str
      :param deployment_id: The deployment this conversation belongs to. This is required if not logged in.
      :type deployment_id: str
      :param deployment_token: The deployment token to authenticate access to the deployment. This is required if not logged in.
      :type deployment_token: str


   .. py:method:: create_app_user_group(name)

      Creates a new App User Group. This User Group is used to have permissions to access the external chatbots.

      :param name: The name of the App User Group.
      :type name: str

      :returns: The App User Group.
      :rtype: AppUserGroup


   .. py:method:: delete_app_user_group(user_group_id)

      Deletes an App User Group.

      :param user_group_id: The ID of the App User Group.
      :type user_group_id: str


   .. py:method:: invite_user_to_app_user_group(email, user_group_id)

      Invite a user to an App User Group. This method will send the specified email address an invitation link to join a specific user group.

      This will allow them to use any chatbots that this user group has access to.


      :param email: The email address to invite to your user group.
      :type email: str
      :param user_group_id: The ID of the App User Group to invite the user to.
      :type user_group_id: str


   .. py:method:: add_users_to_app_user_group(user_group_id, user_emails)

      Adds users to a App User Group.

      :param user_group_id: The ID of the App User Group.
      :type user_group_id: str
      :param user_emails: The emails of the users to add to the App User Group.
      :type user_emails: list


   .. py:method:: remove_users_from_app_user_group(user_group_id, user_emails)

      Removes users from an App User Group.

      :param user_group_id: The ID of the App User Group.
      :type user_group_id: str
      :param user_emails: The emails of the users to remove from the App User Group.
      :type user_emails: list


   .. py:method:: add_app_user_group_to_external_application(user_group_id, external_application_id)

      Adds a permission for an App User Group to access an External Application.

      :param user_group_id: The ID of the App User Group.
      :type user_group_id: str
      :param external_application_id: The ID of the External Application.
      :type external_application_id: str


   .. py:method:: remove_app_user_group_from_external_application(user_group_id, external_application_id)

      Removes a permission for an App User Group to access an External Application.

      :param user_group_id: The ID of the App User Group.
      :type user_group_id: str
      :param external_application_id: The ID of the External Application.
      :type external_application_id: str


   .. py:method:: create_external_application(deployment_id, name = None, logo = None, theme = None)

      Creates a new External Application from an existing ChatLLM Deployment.

      :param deployment_id: The ID of the deployment to use.
      :type deployment_id: str
      :param name: The name of the External Application. If not provided, the name of the deployment will be used.
      :type name: str
      :param logo: The logo to be displayed.
      :type logo: str
      :param theme: The visual theme of the External Application.
      :type theme: dict

      :returns: The newly created External Application.
      :rtype: ExternalApplication


   .. py:method:: update_external_application(external_application_id, name = None, theme = None)

      Updates an External Application.

      :param external_application_id: The ID of the External Application.
      :type external_application_id: str
      :param name: The name of the External Application.
      :type name: str
      :param theme: The visual theme of the External Application.
      :type theme: dict

      :returns: The updated External Application.
      :rtype: ExternalApplication


   .. py:method:: delete_external_application(external_application_id)

      Deletes an External Application.

      :param external_application_id: The ID of the External Application.
      :type external_application_id: str


   .. py:method:: create_agent(project_id, function_source_code, agent_function_name, name = None, memory = None, package_requirements = None, description = None, enable_binary_input = False)

      Creates a new AI agent.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param function_source_code: The contents of a valid Python source code file. The source code should contain a function named `agentFunctionName`. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type function_source_code: str
      :param agent_function_name: The name of the function found in the source code that will be executed when the agent is deployed.
      :type agent_function_name: str
      :param name: The name you want your agent to have, defaults to "<Project Name> Agent".
      :type name: str
      :param memory: The memory allocation (in GB) for the agent.
      :type memory: int
      :param package_requirements: A list of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0'].
      :type package_requirements: list
      :param description: A description of the agent, including its purpose and instructions.
      :type description: str
      :param enable_binary_input: If True, the agent will be able to accept binary data as inputs.
      :type enable_binary_input: bool

      :returns: The new agent
      :rtype: Agent


   .. py:method:: update_agent(model_id, function_source_code = None, agent_function_name = None, memory = None, package_requirements = None, description = None, enable_binary_input = False)

      Updates an existing AI Agent using user-provided Python code. A new version of the agent will be created and published.

      :param model_id: The unique ID associated with the AI Agent to be changed.
      :type model_id: str
      :param function_source_code: Contents of a valid Python source code file. The source code should contain the functions named `agentFunctionName`. A list of allowed import and system libraries for each language is specified in the user functions documentation section.
      :type function_source_code: str
      :param agent_function_name: Name of the function found in the source code that will be executed to the agent when it is deployed.
      :type agent_function_name: str
      :param memory: Memory (in GB) for the agent.
      :type memory: int
      :param package_requirements: List of package requirement strings. For example: ['numpy==1.2.3', 'pandas>=1.4.0']
      :type package_requirements: list
      :param description: A description of the agent, including its purpose and instructions.
      :type description: str
      :param enable_binary_input: If True, the agent will be able to accept binary data as inputs.
      :type enable_binary_input: bool

      :returns: The updated agent
      :rtype: Agent


   .. py:method:: evaluate_prompt(prompt = None, system_message = None, llm_name = None, max_tokens = None, temperature = 0.0, messages = None, response_type = None, json_response_schema = None)

      Generate response to the prompt using the specified model.

      :param prompt: Prompt to use for generation.
      :type prompt: str
      :param system_message: System prompt for models that support it.
      :type system_message: str
      :param llm_name: Name of the underlying LLM to be used for generation. Default is auto selection.
      :type llm_name: LLMName
      :param max_tokens: Maximum number of tokens to generate. If set, the model will just stop generating after this token limit is reached.
      :type max_tokens: int
      :param temperature: Temperature to use for generation. Higher temperature makes more non-deterministic responses, a value of zero makes mostly deterministic reponses. Default is 0.0. A range of 0.0 - 2.0 is allowed.
      :type temperature: float
      :param messages: A list of messages to use as conversation history. For completion models like OPENAI_GPT3_5_TEXT and PALM_TEXT this should not be set. A message is a dict with attributes: is_user (bool): Whether the message is from the user. text (str): The message's text.
      :type messages: list
      :param response_type: Specifies the type of response to request from the LLM. One of 'text' and 'json'. If set to 'json', the LLM will respond with a json formatted string whose schema can be specified `json_response_schema`. Defaults to 'text'
      :type response_type: str
      :param json_response_schema: A dictionary specifying the keys/schema/parameters which LLM should adhere to in its response when `response_type` is 'json'. Each parameter is mapped to a dict with the following info - type (str) (required): Data type of the parameter description (str) (required): Description of the parameter is_required (bool) (optional): Whether the parameter is required or not.     Example:     json_response_schema={         'title': {'type': 'string', 'description': 'Article title', 'is_required': true},         'body': {'type': 'string', 'description': 'Article body'},     }
      :type json_response_schema: dict

      :returns: The response from the model, raw text and parsed components.
      :rtype: LlmResponse


   .. py:method:: render_feature_groups_for_llm(feature_group_ids, token_budget = None, include_definition = True)

      Encode feature groups as language model inputs.

      :param feature_group_ids: List of feature groups to be encoded.
      :type feature_group_ids: list
      :param token_budget: Enforce a given budget for each encoded feature group.
      :type token_budget: int
      :param include_definition: Include the definition of the feature group in the encoding.
      :type include_definition: bool

      :returns: LLM input object comprising of information about the feature groups with given IDs.
      :rtype: LlmInput


   .. py:method:: generate_code_for_data_query_using_llm(query, feature_group_ids = None, external_database_schemas = None, prompt_context = None, llm_name = None, temperature = None, sql_dialect = 'Spark')

      Execute a data query using a large language model in an async fashion.

      :param query: The natural language query to execute. The query is converted to a SQL query using the language model.
      :type query: str
      :param feature_group_ids: A list of feature group IDs that the query should be executed against.
      :type feature_group_ids: list
      :param external_database_schemas: A list of schmeas from external database that the query should be executed against.
      :type external_database_schemas: list
      :param prompt_context: The context message used to construct the prompt for the language model. If not provide, a default context message is used.
      :type prompt_context: str
      :param llm_name: The name of the language model to use. If not provided, the default language model is used.
      :type llm_name: LLMName
      :param temperature: The temperature to use for the language model if supported. If not provided, the default temperature is used.
      :type temperature: float
      :param sql_dialect: The dialect of sql to generate sql for. The default is Spark.
      :type sql_dialect: str

      :returns: The generated SQL code.
      :rtype: LlmGeneratedCode


   .. py:method:: extract_data_using_llm(field_descriptors, document_id = None, document_text = None, llm_name = None)

      Extract fields from a document using a large language model.

      :param field_descriptors: A list of fields to extract from the document.
      :type field_descriptors: list
      :param document_id: The ID of the document to query.
      :type document_id: str
      :param document_text: The text of the document to query. Only used if document_id is not provided.
      :type document_text: str
      :param llm_name: The name of the language model to use. If not provided, the default language model is used.
      :type llm_name: LLMName

      :returns: The response from the document query.
      :rtype: ExtractedFields


   .. py:method:: execute_agent_with_binary_data(deployment_token, deployment_id, arguments = None, keyword_arguments = None, deployment_conversation_id = None, external_session_id = None, blobs = None)

      Executes a deployed AI agent function with binary data as inputs.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param arguments: Positional arguments to the agent execute function.
      :type arguments: list
      :param keyword_arguments: A dictionary where each 'key' represents the parameter name and its corresponding 'value' represents the value of that parameter for the agent execute function.
      :type keyword_arguments: dict
      :param deployment_conversation_id: A unique string identifier for the deployment conversation used for the conversation.
      :type deployment_conversation_id: str
      :param external_session_id: A unique string identifier for the session used for the conversation. If both deployment_conversation_id and external_session_id are not provided, a new session will be created.
      :type external_session_id: str
      :param blobs: A dictionary of binary data to use as inputs to the agent execute function.
      :type blobs: None

      :returns: The result of the agent execution
      :rtype: AgentDataExecutionResult


   .. py:method:: construct_agent_conversation_messages_for_llm(current_message = None, current_doc_ids = None, include_history = True, include_document_contents = True, deployment_conversation_id = None, external_session_id = None, max_document_words = None)

      Returns conversation history in a format for LLM calls.

      :param current_message: Current turn message from user.
      :type current_message: str
      :param current_doc_ids: Document IDs associated with the current turn message from user.
      :type current_doc_ids: list
      :param include_history: If true, include the conversation history in the generated messages.
      :type include_history: bool
      :param include_document_contents: If true, include contents from uploaded documents in the generated messages.
      :type include_document_contents: bool
      :param deployment_conversation_id: Unique ID of the conversation. One of deployment_conversation_id or external_session_id must be provided.
      :type deployment_conversation_id: str
      :param external_session_id: External session ID of the conversation.
      :type external_session_id: str
      :param max_document_words: Maximum number of words to include in the generated message from each uploaded document.
      :type max_document_words: int

      :returns: A list of ChatMessage that represents the conversation.
      :rtype: ChatMessage


   .. py:method:: create_document_retriever(project_id, name, feature_group_id, document_retriever_config = None)

      Returns a document retriever that stores embeddings for document chunks in a feature group.

      Document columns in the feature group are broken into chunks. For cases with multiple document columns, chunks from all columns are combined together to form a single chunk.


      :param project_id: The ID of project that the vector store is created in.
      :type project_id: str
      :param name: The name of the vector store.
      :type name: str
      :param feature_group_id: The ID of the feature group that the vector store is associated with.
      :type feature_group_id: str
      :param document_retriever_config: The configuration, including chunk_size and chunk_overlap_fraction, for document retrieval.
      :type document_retriever_config: DocumentRetrieverConfig

      :returns: The newly created document retriever.
      :rtype: DocumentRetriever


   .. py:method:: rename_document_retriever(document_retriever_id, name)

      Updates an existing document retriever.

      :param document_retriever_id: The unique ID associated with the document retriever.
      :type document_retriever_id: str
      :param name: The name to update the document retriever with.
      :type name: str

      :returns: The updated document retriever.
      :rtype: DocumentRetriever


   .. py:method:: create_document_retriever_version(document_retriever_id, feature_group_id = None, document_retriever_config = None)

      Creates a document retriever version from the latest version of the feature group that the document retriever associated with.

      :param document_retriever_id: The unique ID associated with the document retriever to create version with.
      :type document_retriever_id: str
      :param feature_group_id: The ID of the feature group to update the document retriever with.
      :type feature_group_id: str
      :param document_retriever_config: The configuration, including chunk_size and chunk_overlap_fraction, for document retrieval.
      :type document_retriever_config: DocumentRetrieverConfig

      :returns: The newly created document retriever version.
      :rtype: DocumentRetrieverVersion


   .. py:method:: delete_document_retriever(vector_store_id)

      Delete a Document Retriever.

      :param vector_store_id: A unique string identifier associated with the document retriever.
      :type vector_store_id: str


   .. py:method:: get_document_snippet(document_retriever_id, document_id, start_word_index = None, end_word_index = None)

      Get a snippet from documents in the document retriever.

      :param document_retriever_id: A unique string identifier associated with the document retriever.
      :type document_retriever_id: str
      :param document_id: The ID of the document to retrieve the snippet from.
      :type document_id: str
      :param start_word_index: If provided, will start the snippet at the index (of words in the document) specified.
      :type start_word_index: int
      :param end_word_index: If provided, will end the snippet at the index of (of words in the document) specified.
      :type end_word_index: int

      :returns: The documentation snippet found from the document retriever.
      :rtype: DocumentRetrieverLookupResult


   .. py:method:: restart_document_retriever(document_retriever_id)

      Restart the document retriever if it is stopped.

      :param document_retriever_id: A unique string identifier associated with the document retriever.
      :type document_retriever_id: str


   .. py:method:: get_relevant_snippets(doc_ids = None, blobs = None, query = None, document_retriever_config = None, honor_sentence_boundary = True, num_retrieval_margin_words = None, max_words_per_snippet = None, max_snippets_per_document = None, start_word_index = None, end_word_index = None, including_bounding_boxes = False)

      Get relevant snippets from documents with respect to the query. Document retrievers may be created on-the-fly to perform lookup.

      :param doc_ids: A list of document store IDs to retrieve the snippets from.
      :type doc_ids: list
      :param blobs: A dictionary mapping document names to the blob data.
      :type blobs: io.TextIOBase
      :param query: The query that the documents are relevant to.
      :type query: str
      :param document_retriever_config: If provided, used to configure the retrieval steps like chunking for embeddings.
      :type document_retriever_config: DocumentRetrieverConfig
      :param honor_sentence_boundary: If provided, will honor sentence boundary when returning the snippets.
      :type honor_sentence_boundary: bool
      :param num_retrieval_margin_words: If provided, will add this number of words from left and right of the returned snippets.
      :type num_retrieval_margin_words: int
      :param max_words_per_snippet: If provided, will limit the number of words in each snippet to the value specified.
      :type max_words_per_snippet: int
      :param max_snippets_per_document: If provided, will limit the number of snippets retrieved from each document to the value specified.
      :type max_snippets_per_document: int
      :param start_word_index: If provided, will start the snippet at the index (of words in the document) specified.
      :type start_word_index: int
      :param end_word_index: If provided, will end the snippet at the index of (of words in the document) specified.
      :type end_word_index: int
      :param including_bounding_boxes: If true, will include the bounding boxes of the snippets if they are available.
      :type including_bounding_boxes: bool

      :returns: The snippets found from the documents.
      :rtype: list[DocumentRetrieverLookupResult]



.. py:exception:: ApiException(message, http_status, exception = None, request_id = None)


   Bases: :py:obj:`Exception`

   Default ApiException raised by APIs

   :param message: The error message
   :type message: str
   :param http_status: The https status code raised by the server
   :type http_status: int
   :param exception: The exception class raised by the server
   :type exception: str
   :param request_id: The request id
   :type request_id: str

   .. py:method:: __str__()

      Return str(self).



.. py:class:: ClientOptions(exception_on_404 = True, server = DEFAULT_SERVER)


   Options for configuring the ApiClient

   :param exception_on_404: If true, will raise an exception on a 404 from the server, else will return None.
   :type exception_on_404: bool
   :param server: The default server endpoint to use for API requests
   :type server: str


.. py:class:: ReadOnlyClient(api_key = None, server = None, client_options = None, skip_version_check = False)


   Bases: :py:obj:`BaseApiClient`

   Abacus.AI Read Only API Client. Only contains GET methods

   :param api_key: The api key to use as authentication to the server
   :type api_key: str
   :param server: The base server url to use to send API requets to
   :type server: str
   :param client_options: Optional API client configurations
   :type client_options: ClientOptions
   :param skip_version_check: If true, will skip checking the server's current API version on initializing the client
   :type skip_version_check: bool

   .. py:method:: list_api_keys()

      Lists all of the user's API keys

      :returns: List of API Keys for the current user's organization.
      :rtype: list[ApiKey]


   .. py:method:: list_organization_users()

      Retrieves a list of all users in the organization, including pending users who have been invited.

      :returns: An array of all the users in the organization.
      :rtype: list[User]


   .. py:method:: describe_user()

      Retrieve the current user's information, such as their name, email address, and admin status.

      :returns: An object containing information about the current user.
      :rtype: User


   .. py:method:: list_organization_groups()

      Lists all Organizations Groups

      :returns: A list of all the organization groups within this organization.
      :rtype: list[OrganizationGroup]


   .. py:method:: describe_organization_group(organization_group_id)

      Returns the specific organization group passed in by the user.

      :param organization_group_id: The unique identifier of the organization group to be described.
      :type organization_group_id: str

      :returns: Information about a specific organization group.
      :rtype: OrganizationGroup


   .. py:method:: describe_webhook(webhook_id)

      Describe the webhook with a given ID.

      :param webhook_id: Unique string identifier of the target webhook.
      :type webhook_id: str

      :returns: The webhook with the given ID.
      :rtype: Webhook


   .. py:method:: list_deployment_webhooks(deployment_id)

      List all the webhooks attached to a given deployment.

      :param deployment_id: Unique identifier of the target deployment.
      :type deployment_id: str

      :returns: List of the webhooks attached to the given deployment ID.
      :rtype: list[Webhook]


   .. py:method:: list_use_cases()

      Retrieves a list of all use cases with descriptions. Use the given mappings to specify a use case when needed.

      :returns: A list of `UseCase` objects describing all the use cases addressed by the platform. For details, please refer to.
      :rtype: list[UseCase]


   .. py:method:: describe_problem_type(problem_type)

      Describes a problem type

      :param problem_type: The problem type to get details on
      :type problem_type: str

      :returns: The problem type requirements
      :rtype: ProblemType


   .. py:method:: describe_use_case_requirements(use_case)

      This API call returns the feature requirements for a specified use case.

      :param use_case: This contains the Enum String for the use case whose dataset requirements are needed.
      :type use_case: str

      :returns: The feature requirements of the use case are returned, including all the feature groups required for the use case along with their descriptions and feature mapping details.
      :rtype: list[UseCaseRequirements]


   .. py:method:: describe_project(project_id)

      Returns a description of a project.

      :param project_id: A unique string identifier for the project.
      :type project_id: str

      :returns: The description of the project.
      :rtype: Project


   .. py:method:: list_projects(limit = 100, start_after_id = None)

      Retrieves a list of all projects in the current organization.

      :param limit: The maximum length of the list of projects.
      :type limit: int
      :param start_after_id: The ID of the project after which the list starts.
      :type start_after_id: str

      :returns: A list of all projects in the Organization the user is currently logged in to.
      :rtype: list[Project]


   .. py:method:: get_project_feature_group_config(feature_group_id, project_id)

      Gets a feature group's project config

      :param feature_group_id: Unique string identifier for the feature group.
      :type feature_group_id: str
      :param project_id: Unique string identifier for the project.
      :type project_id: str

      :returns: The feature group's project configuration.
      :rtype: ProjectConfig


   .. py:method:: validate_project(project_id, feature_group_ids = None)

      Validates that the specified project has all required feature group types for its use case and that all required feature columns are set.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_ids: The list of feature group IDs to validate.
      :type feature_group_ids: list

      :returns: The project validation. If the specified project is missing required columns or feature groups, the response includes an array of objects for each missing required feature group and the missing required features in each feature group.
      :rtype: ProjectValidation


   .. py:method:: infer_feature_mappings(project_id, feature_group_id)

      Infer the feature mappings for the feature group in the project based on the problem type.

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str

      :returns: A dict that contains the inferred feature mappings.
      :rtype: InferredFeatureMappings


   .. py:method:: verify_and_describe_annotation(feature_group_id, feature_name = None, doc_id = None, feature_group_row_identifier = None)

      Get the latest annotation entry for a given feature group, feature, and document along with verification information.

      :param feature_group_id: The ID of the feature group the annotation is on.
      :type feature_group_id: str
      :param feature_name: The name of the feature the annotation is on.
      :type feature_name: str
      :param doc_id: The ID of the primary document the annotation is on. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type doc_id: str
      :param feature_group_row_identifier: The key value of the feature group row the annotation is on (cast to string). Usually the feature group's primary / identifier key value. At least one of the doc_id or feature_group_row_identifier must be provided in order to identify the correct annotation.
      :type feature_group_row_identifier: str

      :returns: The latest annotation entry for the given feature group, feature, document, and/or annotation key value. Includes the verification information.
      :rtype: AnnotationEntry


   .. py:method:: get_annotations_status(feature_group_id, feature_name = None, check_for_materialization = False)

      Get the status of the annotations for a given feature group and feature.

      :param feature_group_id: The ID of the feature group the annotation is on.
      :type feature_group_id: str
      :param feature_name: The name of the feature the annotation is on.
      :type feature_name: str
      :param check_for_materialization: If True, check if the feature group needs to be materialized before using for annotations.
      :type check_for_materialization: bool

      :returns: The status of the annotations for the given feature group and feature.
      :rtype: AnnotationsStatus


   .. py:method:: get_feature_group_schema(feature_group_id, project_id = None)

      Returns a schema for a given FeatureGroup in a project.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param project_id: The unique ID associated with the project.
      :type project_id: str

      :returns: A list of objects for each column in the specified feature group.
      :rtype: list[Feature]


   .. py:method:: get_point_in_time_feature_group_creation_options()

      Returns the options that can be used to generate PIT features.

      :returns: List of possible generated aggregation function options.
      :rtype: list[GeneratedPitFeatureConfigOption]


   .. py:method:: describe_feature_group(feature_group_id)

      Describe a Feature Group.

      :param feature_group_id: A unique string identifier associated with the feature group.
      :type feature_group_id: str

      :returns: The feature group object.
      :rtype: FeatureGroup


   .. py:method:: describe_feature_group_by_table_name(table_name)

      Describe a Feature Group by its table name.

      :param table_name: The unique table name of the Feature Group to look up.
      :type table_name: str

      :returns: The Feature Group.
      :rtype: FeatureGroup


   .. py:method:: list_feature_groups(limit = 100, start_after_id = None, feature_group_template_id = None, is_including_detached_from_template = False)

      List all the feature groups

      :param limit: The number of feature groups to retrieve.
      :type limit: int
      :param start_after_id: An offset parameter to exclude all feature groups up to a specified ID.
      :type start_after_id: str
      :param feature_group_template_id: If specified, limit the results to feature groups attached to this template ID.
      :type feature_group_template_id: str
      :param is_including_detached_from_template: When feature_group_template_id is specified, include feature groups that have been detached from that template ID.
      :type is_including_detached_from_template: bool

      :returns: All the feature groups in the organization associated with the specified project.
      :rtype: list[FeatureGroup]


   .. py:method:: describe_project_feature_group(project_id, feature_group_id)

      Describe a feature group associated with a project

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str

      :returns: The project feature group object.
      :rtype: ProjectFeatureGroup


   .. py:method:: list_project_feature_groups(project_id, filter_feature_group_use = None, limit = 100, start_after_id = None)

      List all the feature groups associated with a project

      :param project_id: The unique ID associated with the project.
      :type project_id: str
      :param filter_feature_group_use: The feature group use filter, when given as an argument only allows feature groups present in this project to be returned if they are of the given use. Possible values are: 'USER_CREATED', 'BATCH_PREDICTION_OUTPUT'.
      :type filter_feature_group_use: str
      :param limit: The maximum number of feature groups to be retrieved.
      :type limit: int
      :param start_after_id: An offset parameter to exclude all feature groups up to a specified ID.
      :type start_after_id: str

      :returns: All the Feature Groups in a project.
      :rtype: list[ProjectFeatureGroup]


   .. py:method:: list_python_function_feature_groups(name, limit = 100)

      List all the feature groups associated with a python function.

      :param name: The name used to identify the Python function.
      :type name: str
      :param limit: The maximum number of feature groups to be retrieved.
      :type limit: int

      :returns: All the feature groups associated with the specified Python function ID.
      :rtype: list[FeatureGroup]


   .. py:method:: get_execute_feature_group_operation_result_part_count(feature_group_operation_run_id)

      Gets the number of parts in the result of the execution of fg operation

      :param feature_group_operation_run_id: The unique ID associated with the execution.
      :type feature_group_operation_run_id: str


   .. py:method:: download_execute_feature_group_operation_result_part_chunk(feature_group_operation_run_id, part, offset = 0, chunk_size = 10485760)

      Downloads a chunk of the result of the execution of fg operation

      :param feature_group_operation_run_id: The unique ID associated with the execution.
      :type feature_group_operation_run_id: str
      :param part: The part number of the result
      :type part: int
      :param offset: The offset in the part
      :type offset: int
      :param chunk_size: The size of the chunk
      :type chunk_size: int


   .. py:method:: get_feature_group_version_export_download_url(feature_group_export_id)

      Get a link to download the feature group version.

      :param feature_group_export_id: Unique identifier of the Feature Group Export to get a signed URL for.
      :type feature_group_export_id: str

      :returns: Instance containing the download URL and expiration time for the Feature Group Export.
      :rtype: FeatureGroupExportDownloadUrl


   .. py:method:: describe_feature_group_export(feature_group_export_id)

      A feature group export

      :param feature_group_export_id: Unique identifier of the feature group export.
      :type feature_group_export_id: str

      :returns: The feature group export object.
      :rtype: FeatureGroupExport


   .. py:method:: list_feature_group_exports(feature_group_id)

      Lists all of the feature group exports for the feature group

      :param feature_group_id: Unique identifier of the feature group
      :type feature_group_id: str

      :returns: List of feature group exports
      :rtype: list[FeatureGroupExport]


   .. py:method:: get_feature_group_export_connector_errors(feature_group_export_id)

      Returns a stream containing the write errors of the feature group export database connection, if any writes failed to the database connector.

      :param feature_group_export_id: Unique identifier of the feature group export to get the errors for.
      :type feature_group_export_id: str


   .. py:method:: list_feature_group_modifiers(feature_group_id)

      List the users who can modify a given feature group.

      :param feature_group_id: Unique string identifier of the feature group.
      :type feature_group_id: str

      :returns: Information about the modification lock status and groups/organizations added to the feature group.
      :rtype: ModificationLockInfo


   .. py:method:: get_materialization_logs(feature_group_version, stdout = False, stderr = False)

      Returns logs for a materialized feature group version.

      :param feature_group_version: Unique string identifier for the feature group instance to export.
      :type feature_group_version: str
      :param stdout: Set to True to get info logs.
      :type stdout: bool
      :param stderr: Set to True to get error logs.
      :type stderr: bool

      :returns: A function logs object.
      :rtype: FunctionLogs


   .. py:method:: list_feature_group_versions(feature_group_id, limit = 100, start_after_version = None)

      Retrieves a list of all feature group versions for the specified feature group.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param limit: The maximum length of the returned versions.
      :type limit: int
      :param start_after_version: Results will start after this version.
      :type start_after_version: str

      :returns: A list of feature group versions.
      :rtype: list[FeatureGroupVersion]


   .. py:method:: describe_feature_group_version(feature_group_version)

      Describe a feature group version.

      :param feature_group_version: The unique identifier associated with the feature group version.
      :type feature_group_version: str

      :returns: The feature group version.
      :rtype: FeatureGroupVersion


   .. py:method:: get_feature_group_version_metrics(feature_group_version, selected_columns = None, include_charts = False, include_statistics = True)

      Get metrics for a specific feature group version.

      :param feature_group_version: A unique string identifier associated with the feature group version.
      :type feature_group_version: str
      :param selected_columns: A list of columns to order first.
      :type selected_columns: list
      :param include_charts: A flag indicating whether charts should be included in the response. Default is false.
      :type include_charts: bool
      :param include_statistics: A flag indicating whether statistics should be included in the response. Default is true.
      :type include_statistics: bool

      :returns: The metrics for the specified feature group version.
      :rtype: DataMetrics


   .. py:method:: describe_feature_group_template(feature_group_template_id)

      Describe a Feature Group Template.

      :param feature_group_template_id: The unique identifier of a feature group template.
      :type feature_group_template_id: str

      :returns: The feature group template object.
      :rtype: FeatureGroupTemplate


   .. py:method:: list_feature_group_templates(limit = 100, start_after_id = None, feature_group_id = None, should_include_system_templates = False)

      List feature group templates, optionally scoped by the feature group that created the templates.

      :param limit: Maximum number of templates to be retrieved.
      :type limit: int
      :param start_after_id: Offset parameter to exclude all templates up to the specified feature group template ID.
      :type start_after_id: str
      :param feature_group_id: If specified, limit to templates created from this feature group.
      :type feature_group_id: str
      :param should_include_system_templates: If True, will include built-in templates.
      :type should_include_system_templates: bool

      :returns: All the feature groups in the organization, optionally limited by the feature group that created the template(s).
      :rtype: list[FeatureGroupTemplate]


   .. py:method:: list_project_feature_group_templates(project_id, limit = 100, start_after_id = None, should_include_all_system_templates = False)

      List feature group templates for feature groups associated with the project.

      :param project_id: Unique string identifier to limit to templates associated with this project, e.g. templates associated with feature groups in this project.
      :type project_id: str
      :param limit: Maximum number of templates to be retrieved.
      :type limit: int
      :param start_after_id: Offset parameter to exclude all templates till the specified feature group template ID.
      :type start_after_id: str
      :param should_include_all_system_templates: If True, will include built-in templates.
      :type should_include_all_system_templates: bool

      :returns: All the feature groups in the organization, optionally limited by the feature group that created the template(s).
      :rtype: list[FeatureGroupTemplate]


   .. py:method:: suggest_feature_group_template_for_feature_group(feature_group_id)

      Suggest values for a feature gruop template, based on a feature group.

      :param feature_group_id: Unique identifier associated with the feature group to use for suggesting values to use in the template.
      :type feature_group_id: str

      :returns: The suggested feature group template.
      :rtype: FeatureGroupTemplate


   .. py:method:: get_dataset_schema(dataset_id)

      Retrieves the column schema of a dataset.

      :param dataset_id: Unique string identifier of the dataset schema to look up.
      :type dataset_id: str

      :returns: List of column schema definitions.
      :rtype: list[DatasetColumn]


   .. py:method:: set_dataset_database_connector_config(dataset_id, database_connector_id, object_name = None, columns = None, query_arguments = None, sql_query = None)

      Sets database connector config for a dataset. This method is currently only supported for streaming datasets.

      :param dataset_id: Unique String Identifier of the dataset_id.
      :type dataset_id: str
      :param database_connector_id: Unique String Identifier of the Database Connector to import the dataset from.
      :type database_connector_id: str
      :param object_name: If applicable, the name/ID of the object in the service to query.
      :type object_name: str
      :param columns: The columns to query from the external service object.
      :type columns: str
      :param query_arguments: Additional query arguments to filter the data.
      :type query_arguments: str
      :param sql_query: The full SQL query to use when fetching data. If present, this parameter will override `object_name`, `columns` and `query_arguments`.
      :type sql_query: str


   .. py:method:: get_dataset_version_metrics(dataset_version, selected_columns = None, include_charts = False, include_statistics = True)

      Get metrics for a specific dataset version.

      :param dataset_version: A unique string identifier associated with the dataset version.
      :type dataset_version: str
      :param selected_columns: A list of columns to order first.
      :type selected_columns: list
      :param include_charts: A flag indicating whether charts should be included in the response. Default is false.
      :type include_charts: bool
      :param include_statistics: A flag indicating whether statistics should be included in the response. Default is true.
      :type include_statistics: bool

      :returns: The metrics for the specified Dataset version.
      :rtype: DataMetrics


   .. py:method:: get_file_connector_instructions(bucket, write_permission = False)

      Retrieves verification information to create a data connector to a cloud storage bucket.

      :param bucket: The fully-qualified URI of the storage bucket to verify.
      :type bucket: str
      :param write_permission: If `True`, instructions will include steps for allowing Abacus.AI to write to this service.
      :type write_permission: bool

      :returns: An object with a full description of the cloud storage bucket authentication options and bucket policy. Returns an error message if the parameters are invalid.
      :rtype: FileConnectorInstructions


   .. py:method:: list_database_connectors()

      Retrieves a list of all database connectors along with their associated attributes.

      :returns: An object containing the database connector and its attributes.
      :rtype: list[DatabaseConnector]


   .. py:method:: list_file_connectors()

      Retrieves a list of all connected services in the organization and their current verification status.

      :returns: A list of cloud storage buckets connected to the organization.
      :rtype: list[FileConnector]


   .. py:method:: list_database_connector_objects(database_connector_id, fetch_raw_data = False)

      Lists querable objects in the database connector.

      :param database_connector_id: Unique string identifier for the database connector.
      :type database_connector_id: str
      :param fetch_raw_data: If true, return unfiltered objects.
      :type fetch_raw_data: bool


   .. py:method:: get_database_connector_object_schema(database_connector_id, object_name = None, fetch_raw_data = False)

      Get the schema of an object in an database connector.

      :param database_connector_id: Unique string identifier for the database connector.
      :type database_connector_id: str
      :param object_name: Unique identifier for the object in the external system.
      :type object_name: str
      :param fetch_raw_data: If true, return unfiltered list of columns.
      :type fetch_raw_data: bool

      :returns: The schema of the object.
      :rtype: DatabaseConnectorSchema


   .. py:method:: query_database_connector(database_connector_id, query)

      Runs a query in the specified database connector.

      :param database_connector_id: A unique string identifier for the database connector.
      :type database_connector_id: str
      :param query: The query to be run in the database connector.
      :type query: str


   .. py:method:: list_application_connectors()

      Retrieves a list of all application connectors along with their associated attributes.

      :returns: A list of application connectors.
      :rtype: list[ApplicationConnector]


   .. py:method:: list_application_connector_objects(application_connector_id)

      Lists querable objects in the application connector.

      :param application_connector_id: Unique string identifier for the application connector.
      :type application_connector_id: str


   .. py:method:: list_streaming_connectors()

      Retrieves a list of all streaming connectors along with their corresponding attributes.

      :returns: A list of StreamingConnector objects.
      :rtype: list[StreamingConnector]


   .. py:method:: list_streaming_tokens()

      Retrieves a list of all streaming tokens.

      :returns: A list of streaming tokens and their associated attributes.
      :rtype: list[StreamingAuthToken]


   .. py:method:: get_recent_feature_group_streamed_data(feature_group_id)

      Returns recently streamed data to a streaming feature group.

      :param feature_group_id: Unique string identifier associated with the feature group.
      :type feature_group_id: str


   .. py:method:: list_uploads()

      Lists all pending uploads

      :returns: A list of ongoing uploads in the organization.
      :rtype: list[Upload]


   .. py:method:: describe_upload(upload_id)

      Retrieves the current upload status (complete or inspecting) and the list of file parts uploaded for a specified dataset upload.

      :param upload_id: The unique ID associated with the file uploaded or being uploaded in parts.
      :type upload_id: str

      :returns: Details associated with the large dataset file uploaded in parts.
      :rtype: Upload


   .. py:method:: list_datasets(limit = 100, start_after_id = None, exclude_streaming = False)

      Retrieves a list of all datasets in the organization.

      :param limit: Maximum length of the list of datasets.
      :type limit: int
      :param start_after_id: ID of the dataset after which the list starts.
      :type start_after_id: str
      :param exclude_streaming: Exclude streaming datasets from the result.
      :type exclude_streaming: bool

      :returns: List of datasets.
      :rtype: list[Dataset]


   .. py:method:: describe_dataset(dataset_id)

      Retrieves a full description of the specified dataset, with attributes such as its ID, name, source type, etc.

      :param dataset_id: The unique ID associated with the dataset.
      :type dataset_id: str

      :returns: The dataset.
      :rtype: Dataset


   .. py:method:: describe_dataset_version(dataset_version)

      Retrieves a full description of the specified dataset version, including its ID, name, source type, and other attributes.

      :param dataset_version: Unique string identifier associated with the dataset version.
      :type dataset_version: str

      :returns: The dataset version.
      :rtype: DatasetVersion


   .. py:method:: list_dataset_versions(dataset_id, limit = 100, start_after_version = None)

      Retrieves a list of all dataset versions for the specified dataset.

      :param dataset_id: The unique ID associated with the dataset.
      :type dataset_id: str
      :param limit: The maximum length of the list of all dataset versions.
      :type limit: int
      :param start_after_version: The ID of the version after which the list starts.
      :type start_after_version: str

      :returns: A list of dataset versions.
      :rtype: list[DatasetVersion]


   .. py:method:: get_dataset_version_logs(dataset_version)

      Retrieves the dataset import logs.

      :param dataset_version: The unique version ID of the dataset version.
      :type dataset_version: str

      :returns: The logs for the specified dataset version.
      :rtype: DatasetVersionLogs


   .. py:method:: get_docstore_document(doc_id)

      Return a document store document by id.

      :param doc_id: Unique Docstore string identifier for the document.
      :type doc_id: str


   .. py:method:: get_docstore_image(doc_id, max_width = None, max_height = None)

      Return a document store image by id.

      :param doc_id: A unique Docstore string identifier for the image.
      :type doc_id: str
      :param max_width: Rescales the returned image so the width is less than or equal to the given maximum width, while preserving the aspect ratio.
      :type max_width: int
      :param max_height: Rescales the returned image so the height is less than or equal to the given maximum height, while preserving the aspect ratio.
      :type max_height: int


   .. py:method:: get_docstore_page_data(doc_id, page)

      Returns the extracted page data for a document page.

      :param doc_id: A unique Docstore string identifier for the document.
      :type doc_id: str
      :param page: The page number to retrieve. Page numbers start from 0.
      :type page: int

      :returns: The extracted page data.
      :rtype: PageData


   .. py:method:: get_docstore_document_data(doc_id)

      Returns the extracted data for a document.

      :param doc_id: A unique Docstore string identifier for the document.
      :type doc_id: str

      :returns: The extracted document data.
      :rtype: DocumentData


   .. py:method:: describe_train_test_data_split_feature_group(model_id)

      Get the train and test data split for a trained model by its unique identifier. This is only supported for models with custom algorithms.

      :param model_id: The unique ID of the model. By default, the latest model version will be returned if no version is specified.
      :type model_id: str

      :returns: The feature group containing the training data and fold information.
      :rtype: FeatureGroup


   .. py:method:: describe_train_test_data_split_feature_group_version(model_version)

      Get the train and test data split for a trained model by model version. This is only supported for models with custom algorithms.

      :param model_version: The unique version ID of the model version.
      :type model_version: str

      :returns: The feature group version containing the training data and folds information.
      :rtype: FeatureGroupVersion


   .. py:method:: list_models(project_id)

      Retrieves the list of models in the specified project.

      :param project_id: Unique string identifier associated with the project.
      :type project_id: str

      :returns: A list of models.
      :rtype: list[Model]


   .. py:method:: describe_model(model_id)

      Retrieves a full description of the specified model.

      :param model_id: Unique string identifier associated with the model.
      :type model_id: str

      :returns: Description of the model.
      :rtype: Model


   .. py:method:: get_model_metrics(model_id, model_version = None, return_graphs = False, validation = False)

      Retrieves metrics for all the algorithms trained in this model version.

      If only the model's unique identifier (model_id) is specified, the latest trained version of the model (model_version) is used.


      :param model_id: Unique string identifier for the model.
      :type model_id: str
      :param model_version: Version of the model.
      :type model_version: str
      :param return_graphs: If true, will return the information used for the graphs on the model metrics page such as PR Curve per label.
      :type return_graphs: bool
      :param validation: If true, will return the validation metrics instead of the test metrics.
      :type validation: bool

      :returns: An object containing the model metrics and explanations for what each metric means.
      :rtype: ModelMetrics


   .. py:method:: query_test_point_predictions(model_version, algorithm, to_row, from_row = 0, sql_where_clause = '')

      Query the test points predictions data for a specific algorithm.

      :param model_version: The unique ID associated with the model version.
      :type model_version: str
      :param algorithm: The algorithm id
      :type algorithm: str
      :param to_row: Ending row index to return.
      :type to_row: int
      :param from_row: Starting row index to return.
      :type from_row: int
      :param sql_where_clause: The SQL WHERE clause used to filter the data.
      :type sql_where_clause: str

      :returns: TestPointPrediction
      :rtype: TestPointPredictions


   .. py:method:: get_feature_group_schemas_for_model_version(model_version)

      Gets the schema (including feature mappings) for all feature groups used in the model version.

      :param model_version: Unique string identifier for the version of the model.
      :type model_version: str

      :returns: List of schema for all feature groups used in the model version.
      :rtype: list[ModelVersionFeatureGroupSchema]


   .. py:method:: list_model_versions(model_id, limit = 100, start_after_version = None)

      Retrieves a list of versions for a given model.

      :param model_id: Unique string identifier associated with the model.
      :type model_id: str
      :param limit: Maximum length of the list of all dataset versions.
      :type limit: int
      :param start_after_version: Unique string identifier of the version after which the list starts.
      :type start_after_version: str

      :returns: An array of model versions.
      :rtype: list[ModelVersion]


   .. py:method:: describe_model_version(model_version)

      Retrieves a full description of the specified model version.

      :param model_version: Unique string identifier of the model version.
      :type model_version: str

      :returns: A model version.
      :rtype: ModelVersion


   .. py:method:: get_feature_importance_by_model_version(model_version)

      Gets the feature importance calculated by various methods for the model.

      :param model_version: Unique string identifier for the model version.
      :type model_version: str

      :returns: Feature importances for the model.
      :rtype: FeatureImportance


   .. py:method:: get_training_data_logs(model_version)

      Retrieves the data preparation logs during model training.

      :param model_version: The unique version ID of the model version.
      :type model_version: str

      :returns: A list of logs.
      :rtype: list[DataPrepLogs]


   .. py:method:: get_training_logs(model_version, stdout = False, stderr = False)

      Returns training logs for the model.

      :param model_version: The unique version ID of the model version.
      :type model_version: str
      :param stdout: Set True to get info logs.
      :type stdout: bool
      :param stderr: Set True to get error logs.
      :type stderr: bool

      :returns: A function logs object.
      :rtype: FunctionLogs


   .. py:method:: describe_model_artifacts_export(model_artifacts_export_id)

      Get the description and status of the model artifacts export.

      :param model_artifacts_export_id: A unique string identifier for the export.
      :type model_artifacts_export_id: str

      :returns: Object describing the export and its status.
      :rtype: ModelArtifactsExport


   .. py:method:: list_model_artifacts_exports(model_id, limit = 25)

      List all the model artifacts exports.

      :param model_id: A unique string identifier for the model.
      :type model_id: str
      :param limit: Maximum length of the list of all exports.
      :type limit: int

      :returns: List of model artifacts exports.
      :rtype: list[ModelArtifactsExport]


   .. py:method:: list_model_monitors(project_id)

      Retrieves the list of model monitors in the specified project.

      :param project_id: Unique string identifier associated with the project.
      :type project_id: str

      :returns: A list of model monitors.
      :rtype: list[ModelMonitor]


   .. py:method:: describe_model_monitor(model_monitor_id)

      Retrieves a full description of the specified model monitor.

      :param model_monitor_id: Unique string identifier associated with the model monitor.
      :type model_monitor_id: str

      :returns: Description of the model monitor.
      :rtype: ModelMonitor


   .. py:method:: get_prediction_drift(model_monitor_version)

      Gets the label and prediction drifts for a model monitor.

      :param model_monitor_version: Unique string identifier for a model monitor version created under the project.
      :type model_monitor_version: str

      :returns: Object describing training and prediction output label and prediction distributions.
      :rtype: DriftDistributions


   .. py:method:: get_model_monitor_summary(model_monitor_id)

      Gets the summary of a model monitor across versions.

      :param model_monitor_id: A unique string identifier associated with the model monitor.
      :type model_monitor_id: str

      :returns: An object describing integrity, bias violations, model accuracy and drift for the model monitor.
      :rtype: ModelMonitorSummary


   .. py:method:: list_model_monitor_versions(model_monitor_id, limit = 100, start_after_version = None)

      Retrieves a list of versions for a given model monitor.

      :param model_monitor_id: The unique ID associated with the model monitor.
      :type model_monitor_id: str
      :param limit: The maximum length of the list of all model monitor versions.
      :type limit: int
      :param start_after_version: The ID of the version after which the list starts.
      :type start_after_version: str

      :returns: A list of model monitor versions.
      :rtype: list[ModelMonitorVersion]


   .. py:method:: describe_model_monitor_version(model_monitor_version)

      Retrieves a full description of the specified model monitor version.

      :param model_monitor_version: The unique version ID of the model monitor version.
      :type model_monitor_version: str

      :returns: A model monitor version.
      :rtype: ModelMonitorVersion


   .. py:method:: model_monitor_version_metric_data(model_monitor_version, metric_type, actual_values_to_detail = None)

      Provides the data needed for decile metrics associated with the model monitor.

      :param model_monitor_version: Unique string identifier for the model monitor version.
      :type model_monitor_version: str
      :param metric_type: The type of metric to get data for.
      :type metric_type: str
      :param actual_values_to_detail: The actual values to detail.
      :type actual_values_to_detail: list

      :returns: Data associated with the metric.
      :rtype: ModelMonitorVersionMetricData


   .. py:method:: list_organization_model_monitors(only_starred = False)

      Gets a list of Model Monitors for an organization.

      :param only_starred: Whether to return only starred Model Monitors. Defaults to False.
      :type only_starred: bool

      :returns: A list of Model Monitors.
      :rtype: list[ModelMonitor]


   .. py:method:: get_model_monitor_chart_from_organization(chart_type, limit = 15)

      Gets a list of model monitor summaries across monitors for an organization.

      :param chart_type: Type of chart (model_accuracy, bias_violations, data_integrity, or model_drift) to return.
      :type chart_type: str
      :param limit: Maximum length of the model monitors.
      :type limit: int

      :returns: List of ModelMonitorSummaryForOrganization objects describing accuracy, bias, drift, or integrity for all model monitors in an organization.
      :rtype: list[ModelMonitorSummaryFromOrg]


   .. py:method:: get_model_monitor_summary_from_organization()

      Gets a consolidated summary of model monitors for an organization.

      :returns: A list of `ModelMonitorSummaryForOrganization` objects describing accuracy, bias, drift, and integrity for all model monitors in an organization.
      :rtype: list[ModelMonitorOrgSummary]


   .. py:method:: list_eda(project_id)

      Retrieves the list of Exploratory Data Analysis (EDA) in the specified project.

      :param project_id: Unique string identifier associated with the project.
      :type project_id: str

      :returns: List of EDA objects.
      :rtype: list[Eda]


   .. py:method:: describe_eda(eda_id)

      Retrieves a full description of the specified EDA object.

      :param eda_id: Unique string identifier associated with the EDA object.
      :type eda_id: str

      :returns: Description of the EDA object.
      :rtype: Eda


   .. py:method:: list_eda_versions(eda_id, limit = 100, start_after_version = None)

      Retrieves a list of versions for a given EDA object.

      :param eda_id: The unique ID associated with the EDA object.
      :type eda_id: str
      :param limit: The maximum length of the list of all EDA versions.
      :type limit: int
      :param start_after_version: The ID of the version after which the list starts.
      :type start_after_version: str

      :returns: A list of EDA versions.
      :rtype: list[EdaVersion]


   .. py:method:: describe_eda_version(eda_version)

      Retrieves a full description of the specified EDA version.

      :param eda_version: Unique string identifier of the EDA version.
      :type eda_version: str

      :returns: An EDA version.
      :rtype: EdaVersion


   .. py:method:: get_eda_collinearity(eda_version)

      Gets the Collinearity between all features for the Exploratory Data Analysis.

      :param eda_version: Unique string identifier associated with the EDA instance.
      :type eda_version: str

      :returns: An object with a record of correlations between each feature for the EDA.
      :rtype: EdaCollinearity


   .. py:method:: get_eda_data_consistency(eda_version, transformation_feature = None)

      Gets the data consistency for the Exploratory Data Analysis.

      :param eda_version: Unique string identifier associated with the EDA instance.
      :type eda_version: str
      :param transformation_feature: The transformation feature to get consistency for.
      :type transformation_feature: str

      :returns: Object with duplication, deletion, and transformation data for data consistency analysis for an EDA.
      :rtype: EdaDataConsistency


   .. py:method:: get_collinearity_for_feature(eda_version, feature_name = None)

      Gets the Collinearity for the given feature from the Exploratory Data Analysis.

      :param eda_version: Unique string identifier associated with the EDA instance.
      :type eda_version: str
      :param feature_name: Name of the feature for which correlation is shown.
      :type feature_name: str

      :returns: Object with a record of correlations for the provided feature for an EDA.
      :rtype: EdaFeatureCollinearity


   .. py:method:: get_feature_association(eda_version, reference_feature_name, test_feature_name)

      Gets the Feature Association for the given features from the feature group version within the eda_version.

      :param eda_version: Unique string identifier associated with the EDA instance.
      :type eda_version: str
      :param reference_feature_name: Name of the feature for feature association (on x-axis for the plots generated for the Feature association in the product).
      :type reference_feature_name: str
      :param test_feature_name: Name of the feature for feature association (on y-axis for the plots generated for the Feature association in the product).
      :type test_feature_name: str

      :returns: An object with a record of data for the feature association between the two given features for an EDA version.
      :rtype: EdaFeatureAssociation


   .. py:method:: get_eda_forecasting_analysis(eda_version)

      Gets the Forecasting analysis for the Exploratory Data Analysis.

      :param eda_version: Unique string identifier associated with the EDA version.
      :type eda_version: str

      :returns: Object with forecasting analysis that includes sales_across_time, cummulative_contribution, missing_value_distribution, history_length, num_rows_histogram, product_maturity data.
      :rtype: EdaForecastingAnalysis


   .. py:method:: list_holdout_analysis(project_id, model_id = None)

      List holdout analyses for a project. Optionally, filter by model.

      :param project_id: ID of the project to list holdout analyses for
      :type project_id: str
      :param model_id: (optional) ID of the model to filter by
      :type model_id: str

      :returns: The holdout analyses
      :rtype: list[HoldoutAnalysis]


   .. py:method:: describe_holdout_analysis(holdout_analysis_id)

      Get a holdout analysis.

      :param holdout_analysis_id: ID of the holdout analysis to get
      :type holdout_analysis_id: str

      :returns: The holdout analysis
      :rtype: HoldoutAnalysis


   .. py:method:: list_holdout_analysis_versions(holdout_analysis_id)

      List holdout analysis versions for a holdout analysis.

      :param holdout_analysis_id: ID of the holdout analysis to list holdout analysis versions for
      :type holdout_analysis_id: str

      :returns: The holdout analysis versions
      :rtype: list[HoldoutAnalysisVersion]


   .. py:method:: describe_holdout_analysis_version(holdout_analysis_version, get_metrics = False)

      Get a holdout analysis version.

      :param holdout_analysis_version: ID of the holdout analysis version to get
      :type holdout_analysis_version: str
      :param get_metrics: (optional) Whether to get the metrics for the holdout analysis version
      :type get_metrics: bool

      :returns: The holdout analysis version
      :rtype: HoldoutAnalysisVersion


   .. py:method:: describe_monitor_alert(monitor_alert_id)

      Describes a given monitor alert id

      :param monitor_alert_id: Unique identifier of the monitor alert.
      :type monitor_alert_id: str

      :returns: Object containing information about the monitor alert.
      :rtype: MonitorAlert


   .. py:method:: describe_monitor_alert_version(monitor_alert_version)

      Describes a given monitor alert version id

      :param monitor_alert_version: Unique string identifier for the monitor alert.
      :type monitor_alert_version: str

      :returns: An object describing the monitor alert version.
      :rtype: MonitorAlertVersion


   .. py:method:: list_monitor_alerts_for_monitor(model_monitor_id)

      Retrieves the list of monitor alerts for a specified monitor.

      :param model_monitor_id: The unique ID associated with the model monitor.
      :type model_monitor_id: str

      :returns: A list of monitor alerts.
      :rtype: list[MonitorAlert]


   .. py:method:: list_monitor_alert_versions_for_monitor_version(model_monitor_version)

      Retrieves the list of monitor alert versions for a specified monitor instance.

      :param model_monitor_version: The unique ID associated with the model monitor.
      :type model_monitor_version: str

      :returns: A list of monitor alert versions.
      :rtype: list[MonitorAlertVersion]


   .. py:method:: get_model_monitoring_logs(model_monitor_version, stdout = False, stderr = False)

      Returns monitoring logs for the model.

      :param model_monitor_version: The unique version ID of the model monitor version
      :type model_monitor_version: str
      :param stdout: Set True to get info logs
      :type stdout: bool
      :param stderr: Set True to get error logs
      :type stderr: bool

      :returns: A function logs.
      :rtype: FunctionLogs


   .. py:method:: get_drift_for_feature(model_monitor_version, feature_name, nested_feature_name = None)

      Gets the feature drift associated with a single feature in an output feature group from a prediction.

      :param model_monitor_version: Unique string identifier of a model monitor version created under the project.
      :type model_monitor_version: str
      :param feature_name: Name of the feature to view the distribution of.
      :type feature_name: str
      :param nested_feature_name: Optionally, the name of the nested feature that the feature is in.
      :type nested_feature_name: str

      :returns: An object describing the training and prediction output feature distributions.
      :rtype: FeatureDistribution


   .. py:method:: get_outliers_for_feature(model_monitor_version, feature_name = None, nested_feature_name = None)

      Gets a list of outliers measured by a single feature (or overall) in an output feature group from a prediction.

      :param model_monitor_version: Unique string identifier for a model monitor version created under the project.
      :type model_monitor_version: str
      :param feature_name: Name of the feature to view the distribution of.
      :type feature_name: str
      :param nested_feature_name: Optionally, the name of the nested feature that the feature is in.
      :type nested_feature_name: str


   .. py:method:: describe_prediction_operator(prediction_operator_id)

      Describe an existing prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str

      :returns: The requested prediction operator object.
      :rtype: PredictionOperator


   .. py:method:: list_prediction_operators(project_id)

      List all the prediction operators inside a project.

      :param project_id: The unique ID of the project.
      :type project_id: str

      :returns: A list of prediction operator objects.
      :rtype: list[PredictionOperator]


   .. py:method:: list_prediction_operator_versions(prediction_operator_id)

      List all the prediction operator versions for a prediction operator.

      :param prediction_operator_id: The unique ID of the prediction operator.
      :type prediction_operator_id: str

      :returns: A list of prediction operator version objects.
      :rtype: list[PredictionOperatorVersion]


   .. py:method:: describe_deployment(deployment_id)

      Retrieves a full description of the specified deployment.

      :param deployment_id: Unique string identifier associated with the deployment.
      :type deployment_id: str

      :returns: Description of the deployment.
      :rtype: Deployment


   .. py:method:: list_deployments(project_id)

      Retrieves a list of all deployments in the specified project.

      :param project_id: The unique identifier associated with the project.
      :type project_id: str

      :returns: An array of deployments.
      :rtype: list[Deployment]


   .. py:method:: list_deployment_tokens(project_id)

      Retrieves a list of all deployment tokens associated with the specified project.

      :param project_id: The unique ID associated with the project.
      :type project_id: str

      :returns: A list of deployment tokens.
      :rtype: list[DeploymentAuthToken]


   .. py:method:: get_api_endpoint(deployment_token = None, deployment_id = None, streaming_token = None, feature_group_id = None, model_id = None)

      Returns the API endpoint specific to an organization. This function can be utilized using either an API Key or a deployment ID and token for authentication.

      :param deployment_token: Token used for authenticating access to deployed models.
      :type deployment_token: str
      :param deployment_id: Unique identifier assigned to a deployment created under the specified project.
      :type deployment_id: str
      :param streaming_token: Token used for authenticating access to streaming data.
      :type streaming_token: str
      :param feature_group_id: Unique identifier assigned to a feature group.
      :type feature_group_id: str
      :param model_id: Unique identifier assigned to a model.
      :type model_id: str

      :returns: The API endpoint specific to the organization.
      :rtype: ApiEndpoint


   .. py:method:: get_model_training_types_for_deployment(model_id, model_version = None, algorithm = None)

      Returns types of models that can be deployed for a given model instance ID.

      :param model_id: The unique ID associated with the model.
      :type model_id: str
      :param model_version: The unique ID associated with the model version to deploy.
      :type model_version: str
      :param algorithm: The unique ID associated with the algorithm to deploy.
      :type algorithm: str

      :returns: Model training types for deployment.
      :rtype: ModelTrainingTypeForDeployment


   .. py:method:: list_deployment_alerts(deployment_id)

      List the monitor alerts associated with the deployment id.

      :param deployment_id: Unique string identifier for the deployment.
      :type deployment_id: str

      :returns: An array of deployment alerts.
      :rtype: list[MonitorAlert]


   .. py:method:: describe_refresh_policy(refresh_policy_id)

      Retrieve a single refresh policy

      :param refresh_policy_id: The unique ID associated with this refresh policy.
      :type refresh_policy_id: str

      :returns: An object representing the refresh policy.
      :rtype: RefreshPolicy


   .. py:method:: describe_refresh_pipeline_run(refresh_pipeline_run_id)

      Retrieve a single refresh pipeline run

      :param refresh_pipeline_run_id: Unique string identifier associated with the refresh pipeline run.
      :type refresh_pipeline_run_id: str

      :returns: A refresh pipeline run object.
      :rtype: RefreshPipelineRun


   .. py:method:: list_refresh_policies(project_id = None, dataset_ids = [], feature_group_id = None, model_ids = [], deployment_ids = [], batch_prediction_ids = [], model_monitor_ids = [], prediction_metric_ids = [], notebook_ids = [])

      List the refresh policies for the organization. If no filters are specified, all refresh policies are returned.

      :param project_id: Project ID for which we wish to see the refresh policies attached.
      :type project_id: str
      :param dataset_ids: Comma-separated list of Dataset IDs.
      :type dataset_ids: list
      :param feature_group_id: Feature Group ID for which we wish to see the refresh policies attached.
      :type feature_group_id: str
      :param model_ids: Comma-separated list of Model IDs.
      :type model_ids: list
      :param deployment_ids: Comma-separated list of Deployment IDs.
      :type deployment_ids: list
      :param batch_prediction_ids: Comma-separated list of Batch Prediction IDs.
      :type batch_prediction_ids: list
      :param model_monitor_ids: Comma-separated list of Model Monitor IDs.
      :type model_monitor_ids: list
      :param prediction_metric_ids: Comma-separated list of Prediction Metric IDs.
      :type prediction_metric_ids: list
      :param notebook_ids: Comma-separated list of Notebook IDs.
      :type notebook_ids: list

      :returns: List of all refresh policies in the organization.
      :rtype: list[RefreshPolicy]


   .. py:method:: list_refresh_pipeline_runs(refresh_policy_id)

      List the the times that the refresh policy has been run

      :param refresh_policy_id: Unique identifier associated with the refresh policy.
      :type refresh_policy_id: str

      :returns: List of refresh pipeline runs for the given refresh policy ID.
      :rtype: list[RefreshPipelineRun]


   .. py:method:: download_batch_prediction_result_chunk(batch_prediction_version, offset = 0, chunk_size = 10485760)

      Returns a stream containing the batch prediction results.

      :param batch_prediction_version: Unique string identifier of the batch prediction version to get the results from.
      :type batch_prediction_version: str
      :param offset: The offset to read from.
      :type offset: int
      :param chunk_size: The maximum amount of data to read.
      :type chunk_size: int


   .. py:method:: get_batch_prediction_connector_errors(batch_prediction_version)

      Returns a stream containing the batch prediction database connection write errors, if any writes failed for the specified batch prediction job.

      :param batch_prediction_version: Unique string identifier of the batch prediction job to get the errors for.
      :type batch_prediction_version: str


   .. py:method:: list_batch_predictions(project_id)

      Retrieves a list of batch predictions in the project.

      :param project_id: Unique string identifier of the project.
      :type project_id: str

      :returns: List of batch prediction jobs.
      :rtype: list[BatchPrediction]


   .. py:method:: describe_batch_prediction(batch_prediction_id)

      Describe the batch prediction.

      :param batch_prediction_id: The unique identifier associated with the batch prediction.
      :type batch_prediction_id: str

      :returns: The batch prediction description.
      :rtype: BatchPrediction


   .. py:method:: list_batch_prediction_versions(batch_prediction_id, limit = 100, start_after_version = None)

      Retrieves a list of versions of a given batch prediction

      :param batch_prediction_id: Unique identifier of the batch prediction.
      :type batch_prediction_id: str
      :param limit: Number of versions to list.
      :type limit: int
      :param start_after_version: Version to start after.
      :type start_after_version: str

      :returns: List of batch prediction versions.
      :rtype: list[BatchPredictionVersion]


   .. py:method:: describe_batch_prediction_version(batch_prediction_version)

      Describes a Batch Prediction Version.

      :param batch_prediction_version: Unique string identifier of the Batch Prediction Version.
      :type batch_prediction_version: str

      :returns: The Batch Prediction Version.
      :rtype: BatchPredictionVersion


   .. py:method:: get_batch_prediction_version_logs(batch_prediction_version)

      Retrieves the batch prediction logs.

      :param batch_prediction_version: The unique version ID of the batch prediction version.
      :type batch_prediction_version: str

      :returns: The logs for the specified batch prediction version.
      :rtype: BatchPredictionVersionLogs


   .. py:method:: get_data(feature_group_id, primary_key = None, num_rows = None)

      Gets the feature group rows for online updatable feature groups.

      If primary key is set, row corresponding to primary_key is returned.
      If num_rows is set, we return maximum of num_rows latest updated rows.


      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str
      :param primary_key: The primary key value for which to find the feature group row
      :type primary_key: str
      :param num_rows: Maximum number of rows to return from the feature group
      :type num_rows: int

      :returns: A list of feature group rows.
      :rtype: list[FeatureGroupRow]


   .. py:method:: list_pending_feature_group_documents(feature_group_id)

      Lists all pending documents added to feature group.

      :param feature_group_id: The unique ID associated with the feature group.
      :type feature_group_id: str

      :returns: A list of pending feature group documents.
      :rtype: list[FeatureGroupDocument]


   .. py:method:: describe_python_function(name)

      Describe a Python Function.

      :param name: The name to identify the Python function. Must be a valid Python identifier.
      :type name: str

      :returns: The Python function object.
      :rtype: PythonFunction


   .. py:method:: list_python_functions(function_type = 'FEATURE_GROUP')

      List all python functions within the organization.

      :param function_type: Optional argument to specify the type of function to list Python functions for. Default is FEATURE_GROUP, but can also be PLOTLY_FIG.
      :type function_type: str

      :returns: A list of PythonFunction objects.
      :rtype: list[PythonFunction]


   .. py:method:: list_pipelines(project_id = None)

      Lists the pipelines for an organization or a project

      :param project_id: Unique string identifier for the project to list graph dashboards from.
      :type project_id: str

      :returns: A list of pipelines.
      :rtype: list[Pipeline]


   .. py:method:: describe_pipeline_version(pipeline_version)

      Describes a specified pipeline version

      :param pipeline_version: Unique string identifier for the pipeline version
      :type pipeline_version: str

      :returns: Object describing the pipeline version
      :rtype: PipelineVersion


   .. py:method:: describe_pipeline_step(pipeline_step_id)

      Deletes a step from a pipeline.

      :param pipeline_step_id: The ID of the pipeline step.
      :type pipeline_step_id: str

      :returns: An object describing the pipeline step.
      :rtype: PipelineStep


   .. py:method:: describe_pipeline_step_by_name(pipeline_id, step_name)

      Describes a pipeline step by the step name.

      :param pipeline_id: The ID of the pipeline.
      :type pipeline_id: str
      :param step_name: The name of the step.
      :type step_name: str

      :returns: An object describing the pipeline step.
      :rtype: PipelineStep


   .. py:method:: describe_pipeline_step_version(pipeline_step_version)

      Describes a pipeline step version.

      :param pipeline_step_version: The ID of the pipeline step version.
      :type pipeline_step_version: str

      :returns: An object describing the pipeline step version.
      :rtype: PipelineStepVersion


   .. py:method:: list_pipeline_version_logs(pipeline_version)

      Gets the logs for the steps in a given pipeline version.

      :param pipeline_version: The id of the pipeline version.
      :type pipeline_version: str

      :returns: Object describing the logs for the steps in the pipeline.
      :rtype: PipelineVersionLogs


   .. py:method:: get_step_version_logs(pipeline_step_version)

      Gets the logs for a given step version.

      :param pipeline_step_version: The id of the pipeline step version.
      :type pipeline_step_version: str

      :returns: Object describing the pipeline step logs.
      :rtype: PipelineStepVersionLogs


   .. py:method:: describe_graph_dashboard(graph_dashboard_id)

      Describes a given graph dashboard.

      :param graph_dashboard_id: Unique identifier for the graph dashboard.
      :type graph_dashboard_id: str

      :returns: An object containing information about the graph dashboard.
      :rtype: GraphDashboard


   .. py:method:: list_graph_dashboards(project_id = None)

      Lists the graph dashboards for a project

      :param project_id: Unique string identifier for the project to list graph dashboards from.
      :type project_id: str

      :returns: A list of graph dashboards.
      :rtype: list[GraphDashboard]


   .. py:method:: describe_graph_for_dashboard(graph_reference_id)

      Describes a python plot to a graph dashboard

      :param graph_reference_id: Unique string identifier for the python function id for the graph
      :type graph_reference_id: str

      :returns: An object describing the graph dashboard.
      :rtype: PythonPlotFunction


   .. py:method:: describe_algorithm(algorithm)

      Retrieves a full description of the specified algorithm.

      :param algorithm: The name of the algorithm.
      :type algorithm: str

      :returns: The description of the algorithm.
      :rtype: Algorithm


   .. py:method:: list_algorithms(problem_type = None, project_id = None)

      List all custom algorithms, with optional filtering on Problem Type and Project ID

      :param problem_type: The problem type to query. If `None`, return all algorithms in the organization.
      :type problem_type: str
      :param project_id: The ID of the project.
      :type project_id: str

      :returns: A list of algorithms.
      :rtype: list[Algorithm]


   .. py:method:: describe_custom_loss_function(name)

      Retrieve a full description of a previously registered custom loss function.

      :param name: Registered name of the custom loss function.
      :type name: str

      :returns: The description of the custom loss function with the given name.
      :rtype: CustomLossFunction


   .. py:method:: list_custom_loss_functions(name_prefix = None, loss_function_type = None)

      Retrieves a list of registered custom loss functions and their descriptions.

      :param name_prefix: The prefix of the names of the loss functions to list.
      :type name_prefix: str
      :param loss_function_type: The category of loss functions to search in.
      :type loss_function_type: str

      :returns: The description of the custom loss function with the given name.
      :rtype: CustomLossFunction


   .. py:method:: describe_custom_metric(name)

      Retrieves a full description of a previously registered custom metric function.

      :param name: Registered name of the custom metric.
      :type name: str

      :returns: The description of the custom metric with the given name.
      :rtype: CustomMetric


   .. py:method:: describe_custom_metric_version(custom_metric_version)

      Describes a given custom metric version

      :param custom_metric_version: A unique string identifier for the custom metric version.
      :type custom_metric_version: str

      :returns: An object describing the custom metric version.
      :rtype: CustomMetricVersion


   .. py:method:: list_custom_metrics(name_prefix = None, problem_type = None)

      Retrieves a list of registered custom metrics.

      :param name_prefix: The prefix of the names of the custom metrics.
      :type name_prefix: str
      :param problem_type: The associated problem type of the custom metrics.
      :type problem_type: str

      :returns: A list of custom metrics.
      :rtype: list[CustomMetric]


   .. py:method:: describe_module(name)

      Retrieves a full description of the specified module.

      :param name: The name of the module.
      :type name: str

      :returns: The description of the module.
      :rtype: Module


   .. py:method:: list_modules()

      List all the modules

      :returns: A list of modules
      :rtype: list[Module]


   .. py:method:: get_organization_secret(secret_key)

      Gets a secret.

      :param secret_key: The secret key.
      :type secret_key: str

      :returns: The secret.
      :rtype: OrganizationSecret


   .. py:method:: list_organization_secrets()

      Lists all secrets for an organization.

      :returns: list of secrets belonging to the organization.
      :rtype: list[OrganizationSecret]


   .. py:method:: query_feature_group_code_generator(query, language, project_id = None)

      Send a query to the feature group code generator tool to generate code for the query.

      :param query: A natural language query which specifies what the user wants out of the feature group or its code.
      :type query: str
      :param language: The language in which code is to be generated. One of 'sql' or 'python'.
      :type language: str
      :param project_id: A unique string identifier of the project in context of which the query is.
      :type project_id: str

      :returns: The response from the model, raw text and parsed components.
      :rtype: LlmResponse


   .. py:method:: get_natural_language_explanation(feature_group_id = None, feature_group_version = None, model_id = None)

      Returns the saved natural language explanation of an artifact with given ID. The artifact can be - Feature Group or Feature Group Version or Model

      :param feature_group_id: A unique string identifier associated with the Feature Group.
      :type feature_group_id: str
      :param feature_group_version: A unique string identifier associated with the Feature Group Version.
      :type feature_group_version: str
      :param model_id: A unique string identifier associated with the Model.
      :type model_id: str

      :returns: The object containing natural language explanation(s) as field(s).
      :rtype: NaturalLanguageExplanation


   .. py:method:: generate_natural_language_explanation(feature_group_id = None, feature_group_version = None, model_id = None)

      Generates natural language explanation of an artifact with given ID. The artifact can be - Feature Group or Feature Group Version or Model

      :param feature_group_id: A unique string identifier associated with the Feature Group.
      :type feature_group_id: str
      :param feature_group_version: A unique string identifier associated with the Feature Group Version.
      :type feature_group_version: str
      :param model_id: A unique string identifier associated with the Model.
      :type model_id: str

      :returns: The object containing natural language explanation(s) as field(s).
      :rtype: NaturalLanguageExplanation


   .. py:method:: get_chat_session(chat_session_id)

      Gets a chat session from Data Science Co-pilot.

      :param chat_session_id: Unique ID of the chat session.
      :type chat_session_id: str

      :returns: The chat session with Data Science Co-pilot
      :rtype: ChatSession


   .. py:method:: list_chat_sessions(most_recent_per_project = False)

      Lists all chat sessions for the current user

      :param most_recent_per_project: An optional parameter whether to only return the most recent chat session per project. Default False.
      :type most_recent_per_project: bool

      :returns: The chat sessions with Data Science Co-pilot
      :rtype: ChatSession


   .. py:method:: get_deployment_conversation(deployment_conversation_id = None, external_session_id = None, deployment_id = None, deployment_token = None)

      Gets a deployment conversation.

      :param deployment_conversation_id: Unique ID of the conversation. One of deployment_conversation_id or external_session_id must be provided.
      :type deployment_conversation_id: str
      :param external_session_id: External session ID of the conversation.
      :type external_session_id: str
      :param deployment_id: The deployment this conversation belongs to. This is required if not logged in.
      :type deployment_id: str
      :param deployment_token: The deployment token to authenticate access to the deployment. This is required if not logged in.
      :type deployment_token: str

      :returns: The deployment conversation.
      :rtype: DeploymentConversation


   .. py:method:: list_deployment_conversations(deployment_id)

      Lists all conversations for the given deployment and current user.

      :param deployment_id: The deployment to get conversations for.
      :type deployment_id: str

      :returns: The deployment conversations.
      :rtype: list[DeploymentConversation]


   .. py:method:: export_deployment_conversation(deployment_conversation_id = None, external_session_id = None)

      Export a Deployment Conversation.

      :param deployment_conversation_id: A unique string identifier associated with the deployment conversation.
      :type deployment_conversation_id: str
      :param external_session_id: The external session id associated with the deployment conversation. One of deployment_conversation_id or external_session_id must be provided.
      :type external_session_id: str

      :returns: The deployment conversation html export.
      :rtype: DeploymentConversationExport


   .. py:method:: get_app_user_group(user_group_id)

      Gets an App User Group.

      :param user_group_id: The ID of the App User Group.
      :type user_group_id: str

      :returns: The App User Group.
      :rtype: AppUserGroup


   .. py:method:: describe_external_application(external_application_id)

      Describes an External Application.

      :param external_application_id: The ID of the External Application.
      :type external_application_id: str

      :returns: The External Application.
      :rtype: ExternalApplication


   .. py:method:: list_external_applications()

      Lists External Applications in an organization.

      :returns: List of External Applications.
      :rtype: list[ExternalApplication]


   .. py:method:: describe_agent(agent_id)

      Retrieves a full description of the specified model.

      :param agent_id: Unique string identifier associated with the model.
      :type agent_id: str

      :returns: Description of the agent.
      :rtype: Agent


   .. py:method:: describe_agent_version(agent_version)

      Retrieves a full description of the specified agent version.

      :param agent_version: Unique string identifier of the agent version.
      :type agent_version: str

      :returns: A agent version.
      :rtype: AgentVersion


   .. py:method:: search_feature_groups(text, num_results = 10, project_id = None, feature_group_ids = None)

      Search feature groups based on text and filters.

      :param text: Text to use for approximately matching feature groups.
      :type text: str
      :param num_results: The maximum number of search results to retrieve. The length of the returned list is less than or equal to num_results.
      :type num_results: int
      :param project_id: The ID of the project in which to restrict the search, if specified.
      :type project_id: str
      :param feature_group_ids: A list of feagure group IDs to restrict the search to.
      :type feature_group_ids: list

      :returns: A list of search results, each containing the retrieved object and its relevance score
      :rtype: list[OrganizationSearchResult]


   .. py:method:: list_document_retrievers(project_id, limit = 100, start_after_id = None)

      List all the document retrievers.

      :param project_id: The ID of project that the document retriever is created in.
      :type project_id: str
      :param limit: The number of document retrievers to return.
      :type limit: int
      :param start_after_id: An offset parameter to exclude all document retrievers up to this specified ID.
      :type start_after_id: str

      :returns: All the document retrievers in the organization associated with the specified project.
      :rtype: list[DocumentRetriever]


   .. py:method:: describe_document_retriever(document_retriever_id)

      Describe a Document Retriever.

      :param document_retriever_id: A unique string identifier associated with the document retriever.
      :type document_retriever_id: str

      :returns: The document retriever object.
      :rtype: DocumentRetriever


   .. py:method:: describe_document_retriever_by_name(name)

      Describe a document retriever by its name.

      :param name: The unique name of the document retriever to look up.
      :type name: str

      :returns: The Document Retriever.
      :rtype: DocumentRetriever


   .. py:method:: list_document_retriever_versions(document_retriever_id, limit = 100, start_after_version = None)

      List all the document retriever versions with a given ID.

      :param document_retriever_id: A unique string identifier associated with the document retriever.
      :type document_retriever_id: str
      :param limit: The number of vector store versions to retrieve.
      :type limit: int
      :param start_after_version: An offset parameter to exclude all document retriever versions up to this specified one.
      :type start_after_version: str

      :returns: All the document retriever versions associated with the document retriever.
      :rtype: list[DocumentRetrieverVersion]


   .. py:method:: describe_document_retriever_version(document_retriever_version)

      Describe a document retriever version.

      :param document_retriever_version: A unique string identifier associated with the document retriever version.
      :type document_retriever_version: str

      :returns: The document retriever version object.
      :rtype: DocumentRetrieverVersion



.. py:data:: _request_context

   

.. py:class:: PredictionClient(client_options = None)


   Bases: :py:obj:`abacusai.client.BaseApiClient`

   Abacus.AI Prediction API Client. Does not utilize authentication and only contains public prediction methods

   :param client_options: Optional API client configurations
   :type client_options: ClientOptions

   .. py:method:: predict_raw(deployment_token, deployment_id, **kwargs)

      Raw interface for returning predictions from Plug and Play deployments.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param \*\*kwargs: Arbitrary key/value pairs may be passed in and is sent as part of the request body.
      :type \*\*kwargs: dict


   .. py:method:: lookup_features(deployment_token, deployment_id, query_data, limit_results = None, result_columns = None)

      Returns the feature group deployed in the feature store project.

      :param deployment_token: A deployment token used to authenticate access to created deployments. This token only authorizes predictions on deployments in this project, so it can be safely embedded inside an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the key is the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the value is the unique value of the same entity.
      :type query_data: dict
      :param limit_results: If provided, will limit the number of results to the value specified.
      :type limit_results: int
      :param result_columns: If provided, will limit the columns present in each result to the columns specified in this list.
      :type result_columns: list


   .. py:method:: predict(deployment_token, deployment_id, query_data)

      Returns a prediction for Predictive Modeling

      :param deployment_token: A deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, and is safe to embed in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the key is the column name (e.g. a column with name 'user_id' in the dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed, and the value is the unique value of the same entity.
      :type query_data: dict


   .. py:method:: predict_multiple(deployment_token, deployment_id, query_data)

      Returns a list of predictions for predictive modeling.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, and is safe to embed in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A list of dictionaries, where the 'key' is the column name (e.g. a column with name 'user_id' in the dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed, and the 'value' is the unique value of the same entity.
      :type query_data: list


   .. py:method:: predict_from_datasets(deployment_token, deployment_id, query_data)

      Returns a list of predictions for Predictive Modeling.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the 'key' is the source dataset name, and the 'value' is a list of records corresponding to the dataset rows.
      :type query_data: dict


   .. py:method:: predict_lead(deployment_token, deployment_id, query_data, explain_predictions = False, explainer_type = None)

      Returns the probability of a user being a lead based on their interaction with the service/product and their own attributes (e.g. income, assets, credit score, etc.). Note that the inputs to this method, wherever applicable, should be the column names in the dataset mapped to the column mappings in our system (e.g. column 'user_id' mapped to mapping 'LEAD_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary containing user attributes and/or user's interaction data with the product/service (e.g. number of clicks, items in cart, etc.).
      :type query_data: dict
      :param explain_predictions: Will explain predictions for leads
      :type explain_predictions: bool
      :param explainer_type: Type of explainer to use for explanations
      :type explainer_type: str


   .. py:method:: predict_churn(deployment_token, deployment_id, query_data)

      Returns the probability of a user to churn out in response to their interactions with the item/product/service. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'churn_result' mapped to mapping 'CHURNED_YN' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where the 'key' will be the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the 'value' will be the unique value of the same entity.
      :type query_data: dict


   .. py:method:: predict_takeover(deployment_token, deployment_id, query_data)

      Returns a probability for each class label associated with the types of fraud or a 'yes' or 'no' type label for the possibility of fraud. Note that the inputs to this method, wherever applicable, will be the column names in the dataset mapped to the column mappings in our system (e.g., column 'account_name' mapped to mapping 'ACCOUNT_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary containing account activity characteristics (e.g., login id, login duration, login type, IP address, etc.).
      :type query_data: dict


   .. py:method:: predict_fraud(deployment_token, deployment_id, query_data)

      Returns the probability of a transaction performed under a specific account being fraudulent or not. Note that the inputs to this method, wherever applicable, should be the column names in your dataset mapped to the column mappings in our system (e.g. column 'account_number' mapped to the mapping 'ACCOUNT_ID' in our system).

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary containing transaction attributes (e.g. credit card type, transaction location, transaction amount, etc.).
      :type query_data: dict


   .. py:method:: predict_class(deployment_token, deployment_id, query_data, threshold = None, threshold_class = None, thresholds = None, explain_predictions = False, fixed_features = None, nested = None, explainer_type = None)

      Returns a classification prediction

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the 'Key' is the column name (e.g. a column with the name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the 'Value' is the unique value of the same entity.
      :type query_data: dict
      :param threshold: A float value that is applied on the popular class label.
      :type threshold: float
      :param threshold_class: The label upon which the threshold is added (binary labels only).
      :type threshold_class: str
      :param thresholds: Maps labels to thresholds (multi-label classification only). Defaults to F1 optimal threshold if computed for the given class, else uses 0.5.
      :type thresholds: list
      :param explain_predictions: If True, returns the SHAP explanations for all input features.
      :type explain_predictions: bool
      :param fixed_features: A set of input features to treat as constant for explanations - only honored when the explainer type is KERNEL_EXPLAINER
      :type fixed_features: list
      :param nested: If specified generates prediction delta for each index of the specified nested feature.
      :type nested: str
      :param explainer_type: The type of explainer to use.
      :type explainer_type: str


   .. py:method:: predict_target(deployment_token, deployment_id, query_data, explain_predictions = False, fixed_features = None, nested = None, explainer_type = None)

      Returns a prediction from a classification or regression model. Optionally, includes explanations.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the 'key' is the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the entity against which a prediction is performed and the 'value' is the unique value of the same entity.
      :type query_data: dict
      :param explain_predictions: If true, returns the SHAP explanations for all input features.
      :type explain_predictions: bool
      :param fixed_features: Set of input features to treat as constant for explanations - only honored when the explainer type is KERNEL_EXPLAINER
      :type fixed_features: list
      :param nested: If specified, generates prediction delta for each index of the specified nested feature.
      :type nested: str
      :param explainer_type: The type of explainer to use.
      :type explainer_type: str


   .. py:method:: get_anomalies(deployment_token, deployment_id, threshold = None, histogram = False)

      Returns a list of anomalies from the training dataset.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param threshold: The threshold score of what is an anomaly. Valid values are between 0.8 and 0.99.
      :type threshold: float
      :param histogram: If True, will return a histogram of the distribution of all points.
      :type histogram: bool


   .. py:method:: is_anomaly(deployment_token, deployment_id, query_data = None)

      Returns a list of anomaly attributes based on login information for a specified account. Note that the inputs to this method, wherever applicable, should be the column names in the dataset mapped to the column mappings in our system (e.g. column 'account_name' mapped to mapping 'ACCOUNT_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: The input data for the prediction.
      :type query_data: dict


   .. py:method:: get_event_anomaly_score(deployment_token, deployment_id, query_data = None)

      Returns an anomaly score for an event.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: The input data for the prediction.
      :type query_data: dict


   .. py:method:: get_forecast(deployment_token, deployment_id, query_data, future_data = None, num_predictions = None, prediction_start = None, explain_predictions = False, explainer_type = None, get_item_data = False)

      Returns a list of forecasts for a given entity under the specified project deployment. Note that the inputs to the deployed model will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'holiday_yn' mapped to mapping 'FUTURE' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where 'Key' will be the column name (e.g. a column with name 'store_id' in your dataset) mapped to the column mapping ITEM_ID that uniquely identifies the entity against which forecasting is performed and 'Value' will be the unique value of the same entity.
      :type query_data: dict
      :param future_data: This will be a list of values known ahead of time that are relevant for forecasting (e.g. State Holidays, National Holidays, etc.). Each element is a dictionary, where the key and the value both will be of type 'str'. For example future data entered for a Store may be [{"Holiday":"No", "Promo":"Yes", "Date": "2015-07-31 00:00:00"}].
      :type future_data: list
      :param num_predictions: The number of timestamps to predict in the future.
      :type num_predictions: int
      :param prediction_start: The start date for predictions (e.g., "2015-08-01T00:00:00" as input for mid-night of 2015-08-01).
      :type prediction_start: str
      :param explain_predictions: Will explain predictions for forecasting
      :type explain_predictions: bool
      :param explainer_type: Type of explainer to use for explanations
      :type explainer_type: str
      :param get_item_data: Will return the data corresponding to items in query
      :type get_item_data: bool


   .. py:method:: get_k_nearest(deployment_token, deployment_id, vector, k = None, distance = None, include_score = False, catalog_id = None)

      Returns the k nearest neighbors for the provided embedding vector.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param vector: Input vector to perform the k nearest neighbors with.
      :type vector: list
      :param k: Overrideable number of items to return.
      :type k: int
      :param distance: Specify the distance function to use when finding nearest neighbors.
      :type distance: str
      :param include_score: If True, will return the score alongside the resulting embedding value.
      :type include_score: bool
      :param catalog_id: An optional parameter honored only for embeddings that provide a catalog id
      :type catalog_id: str


   .. py:method:: get_multiple_k_nearest(deployment_token, deployment_id, queries)

      Returns the k nearest neighbors for the queries provided.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param queries: List of mappings of format {"catalogId": "cat0", "vectors": [...], "k": 20, "distance": "euclidean"}. See `getKNearest` for additional information about the supported parameters.
      :type queries: list


   .. py:method:: get_labels(deployment_token, deployment_id, query_data)

      Returns a list of scored labels for a document.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: Dictionary where key is "Content" and value is the text from which entities are to be extracted.
      :type query_data: dict


   .. py:method:: get_entities_from_pdf(deployment_token, deployment_id, pdf = None, doc_id = None, return_extracted_features = False, verbose = False, save_extracted_features = None)

      Extracts text from the provided PDF and returns a list of recognized labels and their scores.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param pdf: (Optional) The pdf to predict on. One of pdf or docId must be specified.
      :type pdf: io.TextIOBase
      :param doc_id: (Optional) The pdf to predict on. One of pdf or docId must be specified.
      :type doc_id: str
      :param return_extracted_features: (Optional) If True, will return all extracted features (e.g. all tokens in a page) from the PDF. Default is False.
      :type return_extracted_features: bool
      :param verbose: (Optional) If True, will return all the extracted tokens probabilities for all the trained labels. Default is False.
      :type verbose: bool
      :param save_extracted_features: (Optional) If True, will save extracted features (i.e. page tokens) so that they can be fetched using the prediction docId. Default is False.
      :type save_extracted_features: bool


   .. py:method:: get_recommendations(deployment_token, deployment_id, query_data, num_items = None, page = None, exclude_item_ids = None, score_field = None, scaling_factors = None, restrict_items = None, exclude_items = None, explore_fraction = None, diversity_attribute_name = None, diversity_max_results_per_value = None)

      Returns a list of recommendations for a given user under the specified project deployment. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'time' mapped to mapping 'TIMESTAMP' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where 'Key' will be the column name (e.g. a column with name 'user_name' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the user against which recommendations are made and 'Value' will be the unique value of the same item. For example, if you have the column name 'user_name' mapped to the column mapping 'USER_ID', then the query must have the exact same column name (user_name) as key and the name of the user (John Doe) as value.
      :type query_data: dict
      :param num_items: The number of items to recommend on one page. By default, it is set to 50 items per page.
      :type num_items: int
      :param page: The page number to be displayed. For example, let's say that the num_items is set to 10 with the total recommendations list size of 50 recommended items, then an input value of 2 in the 'page' variable will display a list of items that rank from 11th to 20th.
      :type page: int
      :param exclude_item_ids: [DEPRECATED]
      :type exclude_item_ids: list
      :param score_field: The relative item scores are returned in a separate field named with the same name as the key (score_field) for this argument.
      :type score_field: str
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted.  Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there's a type of item that might be less popular but you want to promote it or there's an item that always comes up and you want to demote it.
      :type scaling_factors: list
      :param restrict_items: It allows you to restrict the recommendations to certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", "value3", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1", "value3", ...]" to which to restrict the recommendations to. Let's take an example where the input to restrict_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. This input will restrict the recommendations to SUVs and Sedans. This type of restriction is particularly useful if there's a list of items that you know is of use in some particular scenario and you want to restrict the recommendations only to that list.
      :type restrict_items: list
      :param exclude_items: It allows you to exclude certain items from the list of recommendations. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" to exclude from the recommendations. Let's take an example where the input to exclude_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. The resulting recommendation list will exclude all SUVs and Sedans. This is
      :type exclude_items: list
      :param explore_fraction: Explore fraction.
      :type explore_fraction: float
      :param diversity_attribute_name: item attribute column name which is used to ensure diversity of prediction results.
      :type diversity_attribute_name: str
      :param diversity_max_results_per_value: maximum number of results per value of diversity_attribute_name.
      :type diversity_max_results_per_value: int


   .. py:method:: get_personalized_ranking(deployment_token, deployment_id, query_data, preserve_ranks = None, preserve_unknown_items = False, scaling_factors = None)

      Returns a list of items with personalized promotions for a given user under the specified project deployment. Note that the inputs to this method, wherever applicable, should be the column names in the dataset mapped to the column mappings in our system (e.g. column 'item_code' mapped to mapping 'ITEM_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This should be a dictionary with two key-value pairs. The first pair represents a 'Key' where the column name (e.g. a column with name 'user_id' in the dataset) mapped to the column mapping USER_ID uniquely identifies the user against whom a prediction is made and a 'Value' which is the identifier value for that user. The second pair will have a 'Key' which will be the name of the column name (e.g. movie_name) mapped to ITEM_ID (unique item identifier) and a 'Value' which will be a list of identifiers that uniquely identifies those items.
      :type query_data: dict
      :param preserve_ranks: List of dictionaries of format {"column": "col0", "values": ["value0, value1"]}, where the ranks of items in query_data is preserved for all the items in "col0" with values, "value0" and "value1". This option is useful when the desired items are being recommended in the desired order and the ranks for those items need to be kept unchanged during recommendation generation.
      :type preserve_ranks: list
      :param preserve_unknown_items: If true, any items that are unknown to the model, will not be reranked, and the original position in the query will be preserved.
      :type preserve_unknown_items: bool
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted. Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there's a type of item that might be less popular but you want to promote it or there's an item that always comes up and you want to demote it.
      :type scaling_factors: list


   .. py:method:: get_ranked_items(deployment_token, deployment_id, query_data, preserve_ranks = None, preserve_unknown_items = False, score_field = None, scaling_factors = None, diversity_attribute_name = None, diversity_max_results_per_value = None)

      Returns a list of re-ranked items for a selected user when a list of items is required to be reranked according to the user's preferences. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'item_code' mapped to mapping 'ITEM_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary with two key-value pairs. The first pair represents a 'Key' where the column name (e.g. a column with name 'user_id' in your dataset) mapped to the column mapping USER_ID uniquely identifies the user against whom a prediction is made and a 'Value' which is the identifier value for that user. The second pair will have a 'Key' which will be the name of the column name (e.g. movie_name) mapped to ITEM_ID (unique item identifier) and a 'Value' which will be a list of identifiers that uniquely identifies those items.
      :type query_data: dict
      :param preserve_ranks: List of dictionaries of format {"column": "col0", "values": ["value0, value1"]}, where the ranks of items in query_data is preserved for all the items in "col0" with values, "value0" and "value1". This option is useful when the desired items are being recommended in the desired order and the ranks for those items need to be kept unchanged during recommendation generation.
      :type preserve_ranks: list
      :param preserve_unknown_items: If true, any items that are unknown to the model, will not be reranked, and the original position in the query will be preserved
      :type preserve_unknown_items: bool
      :param score_field: The relative item scores are returned in a separate field named with the same name as the key (score_field) for this argument.
      :type score_field: str
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted. Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there is a type of item that might be less popular but you want to promote it or there is an item that always comes up and you want to demote it.
      :type scaling_factors: list
      :param diversity_attribute_name: item attribute column name which is used to ensure diversity of prediction results.
      :type diversity_attribute_name: str
      :param diversity_max_results_per_value: maximum number of results per value of diversity_attribute_name.
      :type diversity_max_results_per_value: int


   .. py:method:: get_related_items(deployment_token, deployment_id, query_data, num_items = None, page = None, scaling_factors = None, restrict_items = None, exclude_items = None)

      Returns a list of related items for a given item under the specified project deployment. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'item_code' mapped to mapping 'ITEM_ID' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: This will be a dictionary where the 'key' will be the column name (e.g. a column with name 'user_name' in your dataset) mapped to the column mapping USER_ID that uniquely identifies the user against which related items are determined and the 'value' will be the unique value of the same item. For example, if you have the column name 'user_name' mapped to the column mapping 'USER_ID', then the query must have the exact same column name (user_name) as key and the name of the user (John Doe) as value.
      :type query_data: dict
      :param num_items: The number of items to recommend on one page. By default, it is set to 50 items per page.
      :type num_items: int
      :param page: The page number to be displayed. For example, let's say that the num_items is set to 10 with the total recommendations list size of 50 recommended items, then an input value of 2 in the 'page' variable will display a list of items that rank from 11th to 20th.
      :type page: int
      :param scaling_factors: It allows you to bias the model towards certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1"], "factor": 1.1}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" in reference to which the model recommendations need to be biased; and the key, "factor" takes the factor by which the item scores are adjusted.  Let's take an example where the input to scaling_factors is [{"column": "VehicleType", "values": ["SUV", "Sedan"], "factor": 1.4}]. After we apply the model to get item probabilities, for every SUV and Sedan in the list, we will multiply the respective probability by 1.1 before sorting. This is particularly useful if there's a type of item that might be less popular but you want to promote it or there's an item that always comes up and you want to demote it.
      :type scaling_factors: list
      :param restrict_items: It allows you to restrict the recommendations to certain items. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", "value3", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1", "value3", ...]" to which to restrict the recommendations to. Let's take an example where the input to restrict_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. This input will restrict the recommendations to SUVs and Sedans. This type of restriction is particularly useful if there's a list of items that you know is of use in some particular scenario and you want to restrict the recommendations only to that list.
      :type restrict_items: list
      :param exclude_items: It allows you to exclude certain items from the list of recommendations. The input to this argument is a list of dictionaries where the format of each dictionary is as follows: {"column": "col0", "values": ["value0", "value1", ...]}. The key, "column" takes the name of the column, "col0"; the key, "values" takes the list of items, "["value0", "value1"]" to exclude from the recommendations. Let's take an example where the input to exclude_items is [{"column": "VehicleType", "values": ["SUV", "Sedan"]}]. The resulting recommendation list will exclude all SUVs and Sedans. This is particularly useful if there's a list of items that you know is of no use in some particular scenario and you don't want to show those items present in that list.
      :type exclude_items: list


   .. py:method:: get_chat_response(deployment_token, deployment_id, messages, llm_name = None, num_completion_tokens = None, system_message = None, temperature = 0.0, filter_key_values = None, search_score_cutoff = None, chat_config = None, ignore_documents = False)

      Return a chat response which continues the conversation based on the input messages and search results.

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param messages: A list of chronologically ordered messages, starting with a user message and alternating sources. A message is a dict with attributes:     is_user (bool): Whether the message is from the user.      text (str): The message's text.
      :type messages: list
      :param llm_name: Name of the specific LLM backend to use to power the chat experience
      :type llm_name: str
      :param num_completion_tokens: Default for maximum number of tokens for chat answers
      :type num_completion_tokens: int
      :param system_message: The generative LLM system message
      :type system_message: str
      :param temperature: The generative LLM temperature
      :type temperature: float
      :param filter_key_values: A dictionary mapping column names to a list of values to restrict the retrieved search results.
      :type filter_key_values: dict
      :param search_score_cutoff: Cutoff for the document retriever score. Matching search results below this score will be ignored.
      :type search_score_cutoff: float
      :param chat_config: A dictionary specifying the query chat config override.
      :type chat_config: dict
      :param ignore_documents: If True, will ignore any documents and search results, and only use the messages to generate a response.
      :type ignore_documents: bool


   .. py:method:: get_conversation_response(deployment_id, message, deployment_conversation_id = None, external_session_id = None, llm_name = None, num_completion_tokens = None, system_message = None, temperature = 0.0, filter_key_values = None, search_score_cutoff = None, chat_config = None, ignore_documents = False)

      Return a conversation response which continues the conversation based on the input message and deployment conversation id (if exists).

      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param message: A message from the user
      :type message: str
      :param deployment_conversation_id: The unique identifier of a deployment conversation to continue. If not specified, a new one will be created.
      :type deployment_conversation_id: str
      :param external_session_id: The user supplied unique identifier of a deployment conversation to continue. If specified, we will use this instead of a internal deployment conversation id.
      :type external_session_id: str
      :param llm_name: Name of the specific LLM backend to use to power the chat experience
      :type llm_name: str
      :param num_completion_tokens: Default for maximum number of tokens for chat answers
      :type num_completion_tokens: int
      :param system_message: The generative LLM system message
      :type system_message: str
      :param temperature: The generative LLM temperature
      :type temperature: float
      :param filter_key_values: A dictionary mapping column names to a list of values to restrict the retrived search results.
      :type filter_key_values: dict
      :param search_score_cutoff: Cutoff for the document retriever score. Matching search results below this score will be ignored.
      :type search_score_cutoff: float
      :param chat_config: A dictionary specifiying the query chat config override.
      :type chat_config: dict
      :param ignore_documents: If True, will ignore any documents and search results, and only use the message and past conversation to generate a response.
      :type ignore_documents: bool


   .. py:method:: get_search_results(deployment_token, deployment_id, query_data, num = 15)

      Return the most relevant search results to the search query from the uploaded documents.

      :param deployment_token: A token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it can be securely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where the key is "Content" and the value is the text from which entities are to be extracted.
      :type query_data: dict
      :param num: Number of search results to return.
      :type num: int


   .. py:method:: get_sentiment(deployment_token, deployment_id, document)

      Predicts sentiment on a document

      :param deployment_token: A token used to authenticate access to deployments created in this project. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for a deployment created under this project.
      :type deployment_id: str
      :param document: The document to be analyzed for sentiment.
      :type document: str


   .. py:method:: get_entailment(deployment_token, deployment_id, document)

      Predicts the classification of the document

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param document: The document to be classified.
      :type document: str


   .. py:method:: get_classification(deployment_token, deployment_id, document)

      Predicts the classification of the document

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param document: The document to be classified.
      :type document: str


   .. py:method:: get_summary(deployment_token, deployment_id, query_data)

      Returns a JSON of the predicted summary for the given document. Note that the inputs to this method, wherever applicable, will be the column names in your dataset mapped to the column mappings in our system (e.g. column 'text' mapped to mapping 'DOCUMENT' in our system).

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: Raw data dictionary containing the required document data - must have a key 'document' corresponding to a DOCUMENT type text as value.
      :type query_data: dict


   .. py:method:: predict_language(deployment_token, deployment_id, query_data)

      Predicts the language of the text

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments within this project, making it safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: The input string to detect.
      :type query_data: str


   .. py:method:: get_assignments(deployment_token, deployment_id, query_data, forced_assignments = None, solve_time_limit_seconds = None, include_all_assignments = False)

      Get all positive assignments that match a query.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: Specifies the set of assignments being requested. The value for the key can be: 1. A simple scalar value, which is matched exactly 2. A list of values, which matches any element in the list 3. A dictionary with keys lower_in/lower_ex and upper_in/upper_ex, which matches values in an inclusive/exclusive range
      :type query_data: dict
      :param forced_assignments: Set of assignments to force and resolve before returning query results.
      :type forced_assignments: dict
      :param solve_time_limit_seconds: Maximum time in seconds to spend solving the query.
      :type solve_time_limit_seconds: float
      :param include_all_assignments: If True, will return all assignments, including assignments with value 0. Default is False.
      :type include_all_assignments: bool


   .. py:method:: get_alternative_assignments(deployment_token, deployment_id, query_data, add_constraints = None, solve_time_limit_seconds = None)

      Get alternative positive assignments for given query. Optimal assignments are ignored and the alternative assignments are returned instead.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param query_data: Specifies the set of assignments being requested. The value for the key can be: 1. A simple scalar value, which is matched exactly 2. A list of values, which matches any element in the list 3. A dictionary with keys lower_in/lower_ex and upper_in/upper_ex, which matches values in an inclusive/exclusive range
      :type query_data: dict
      :param add_constraints: List of constraints dict to apply to the query. The constraint dict should have the following keys: 1. query (dict): Specifies the set of assignments involved in the constraint. The format is same as query_data. 2. operator (str): Constraint operator '=' or '<=' or '>='. 3. constant (int): Constraint RHS constant value.
      :type add_constraints: list
      :param solve_time_limit_seconds: Maximum time in seconds to spend solving the query.
      :type solve_time_limit_seconds: float


   .. py:method:: check_constraints(deployment_token, deployment_id, query_data)

      Check for any constraints violated by the overrides.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param query_data: Assignment overrides to the solution.
      :type query_data: dict


   .. py:method:: predict_with_binary_data(deployment_token, deployment_id, blob)

      Make predictions for a given blob, e.g. image, audio

      :param deployment_token: A token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param blob: The multipart/form-data of the data.
      :type blob: io.TextIOBase


   .. py:method:: describe_image(deployment_token, deployment_id, image, categories, top_n = None)

      Describe the similarity between an image and a list of categories.

      :param deployment_token: Authentication token to access created deployments. This token is only authorized to predict on deployments in the current project, and can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: Unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param image: Image to describe.
      :type image: io.TextIOBase
      :param categories: List of candidate categories to compare with the image.
      :type categories: list
      :param top_n: Return the N most similar categories.
      :type top_n: int


   .. py:method:: get_text_from_document(deployment_token, deployment_id, document = None, adjust_doc_orientation = False, return_detected_images = False, save_predicted_pdf = False, save_extracted_features = False)

      Generate text from a document

      :param deployment_token: Authentication token to access created deployments. This token is only authorized to predict on deployments in the current project, and can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: Unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param document: Input document which can be an image, pdf, or word document (Some formats might not be supported yet)
      :type document: io.TextIOBase
      :param adjust_doc_orientation: (Optional) whether to detect the document page orientation and rotate it if needed.
      :type adjust_doc_orientation: bool
      :param return_detected_images: whether the detected images should be saved in docstore or not (if true, adds a docstore id to the response (may not be available for some algorithms))
      :type return_detected_images: bool
      :param save_predicted_pdf: (Optional) If True, will save the predicted pdf bytes so that they can be fetched using the prediction docId. Default is False.
      :type save_predicted_pdf: bool
      :param save_extracted_features: (Optional) If True, will save extracted features (i.e. page tokens) so that they can be fetched using the prediction docId. Default is False.
      :type save_extracted_features: bool


   .. py:method:: transcribe_audio(deployment_token, deployment_id, audio)

      Transcribe the audio

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to make predictions on deployments in this project, so it can be safely embedded in an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier of a deployment created under the project.
      :type deployment_id: str
      :param audio: The audio to transcribe.
      :type audio: io.TextIOBase


   .. py:method:: classify_image(deployment_token, deployment_id, image = None, doc_id = None)

      Classify an image.

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier to a deployment created under the project.
      :type deployment_id: str
      :param image: The binary data of the image to classify. One of image or doc_id must be specified.
      :type image: io.TextIOBase
      :param doc_id: The document ID of the image. One of image or doc_id must be specified.
      :type doc_id: str


   .. py:method:: classify_pdf(deployment_token, deployment_id, pdf = None)

      Returns a classification prediction from a PDF

      :param deployment_token: The deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: The unique identifier for a deployment created under the project.
      :type deployment_id: str
      :param pdf: (Optional) The pdf to predict on. One of pdf or docId must be specified.
      :type pdf: io.TextIOBase


   .. py:method:: get_cluster(deployment_token, deployment_id, query_data)

      Predicts the cluster for given data.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param query_data: A dictionary where each 'key' represents a column name and its corresponding 'value' represents the value of that column. For Timeseries Clustering, the 'key' should be ITEM_ID, and its value should represent a unique item ID that needs clustering.
      :type query_data: dict


   .. py:method:: get_objects_from_image(deployment_token, deployment_id, image)

      Classify an image.

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier to a deployment created under the project.
      :type deployment_id: str
      :param image: The binary data of the image to detect objects from.
      :type image: io.TextIOBase


   .. py:method:: score_image(deployment_token, deployment_id, image)

      Score on image.

      :param deployment_token: A deployment token to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier to a deployment created under the project.
      :type deployment_id: str
      :param image: The binary data of the image to get the score.
      :type image: io.TextIOBase


   .. py:method:: transfer_style(deployment_token, deployment_id, source_image, style_image)

      Change the source image to adopt the visual style from the style image.

      :param deployment_token: A token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param source_image: The source image to apply the makeup.
      :type source_image: io.TextIOBase
      :param style_image: The image that has the style as a reference.
      :type style_image: io.TextIOBase


   .. py:method:: generate_image(deployment_token, deployment_id, query_data)

      Generate an image from text prompt.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model within an application or website.
      :type deployment_token: str
      :param deployment_id: A unique identifier to a deployment created under the project.
      :type deployment_id: str
      :param query_data: Specifies the text prompt. For example, {'prompt': 'a cat'}
      :type query_data: dict


   .. py:method:: execute_agent(deployment_token, deployment_id, arguments = None, keyword_arguments = None)

      Executes a deployed AI agent function using the arguments as keyword arguments to the agent execute function.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param arguments: Positional arguments to the agent execute function.
      :type arguments: list
      :param keyword_arguments: A dictionary where each 'key' represents the paramter name and its corresponding 'value' represents the value of that parameter for the agent execute function.
      :type keyword_arguments: dict


   .. py:method:: execute_conversation_agent(deployment_token, deployment_id, arguments = None, keyword_arguments = None, deployment_conversation_id = None, external_session_id = None, regenerate = False, doc_infos = None)

      Executes a deployed AI agent function using the arguments as keyword arguments to the agent execute function.

      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments in this project, so it is safe to embed this model inside of an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param arguments: Positional arguments to the agent execute function.
      :type arguments: list
      :param keyword_arguments: A dictionary where each 'key' represents the paramter name and its corresponding 'value' represents the value of that parameter for the agent execute function.
      :type keyword_arguments: dict
      :param deployment_conversation_id: A unique string identifier for the deployment conversation used for the conversation.
      :type deployment_conversation_id: str
      :param external_session_id: A unique string identifier for the session used for the conversation. If both deployment_conversation_id and external_session_id are not provided, a new session will be created.
      :type external_session_id: str
      :param regenerate: If True, will regenerate the response from the last query.
      :type regenerate: bool
      :param doc_infos: An optional list of documents use for the conversation. A keyword 'doc_id' is expected to be present in each document for retrieving contents from docstore.
      :type doc_infos: list


   .. py:method:: lookup_matches(deployment_token, deployment_id, data = None, filters = None, num = None, result_columns = None, max_words = None, num_retrieval_margin_words = None, max_words_per_chunk = None, score_multiplier_column = None)

      Lookup document retrievers and return the matching documents from the document retriever deployed with given query.

      Original documents are splitted into chunks and stored in the document retriever. This lookup function will return the relevant chunks
      from the document retriever. The returned chunks could be expanded to include more words from the original documents and merged if they
      are overlapping, and permitted by the settings provided. The returned chunks are sorted by relevance.


      :param deployment_token: The deployment token used to authenticate access to created deployments. This token is only authorized to predict on deployments within this project, making it safe to embed this model in an application or website.
      :type deployment_token: str
      :param deployment_id: A unique string identifier for the deployment created under the project.
      :type deployment_id: str
      :param data: The query to search for.
      :type data: str
      :param filters: A dictionary mapping column names to a list of values to restrict the retrieved search results.
      :type filters: dict
      :param num: If provided, will limit the number of results to the value specified.
      :type num: int
      :param result_columns: If provided, will limit the column properties present in each result to those specified in this list.
      :type result_columns: list
      :param max_words: If provided, will limit the total number of words in the results to the value specified.
      :type max_words: int
      :param num_retrieval_margin_words: If provided, will add this number of words from left and right of the returned chunks.
      :type num_retrieval_margin_words: int
      :param max_words_per_chunk: If provided, will limit the number of words in each chunk to the value specified. If the value provided is smaller than the actual size of chunk on disk, which is determined during document retriever creation, the actual size of chunk will be used. I.e, chunks looked up from document retrievers will not be split into smaller chunks during lookup due to this setting.
      :type max_words_per_chunk: int
      :param score_multiplier_column: If provided, will use the values in this column to modify the relevance score of the returned chunks. Values in this column must be numeric.
      :type score_multiplier_column: str

      :returns: The relevant documentation results found from the document retriever.
      :rtype: list[DocumentRetrieverLookupResult]



.. py:class:: StreamingClient(client_options = None)


   Bases: :py:obj:`abacusai.client.BaseApiClient`

   Abacus.AI Streaming API Client. Does not utilize authentication and only contains public streaming methods

   :param client_options: Optional API client configurations
   :type client_options: ClientOptions

   .. py:method:: upsert_item_embeddings(streaming_token, model_id, item_id, vector, catalog_id = None)

      Upserts an embedding vector for an item id for a model_id.

      :param streaming_token: The streaming token for authenticating requests to the model.
      :type streaming_token: str
      :param model_id: A unique string identifier for the model to upsert item embeddings to.
      :type model_id: str
      :param item_id: The item id for which its embeddings will be upserted.
      :type item_id: str
      :param vector: The embedding vector.
      :type vector: list
      :param catalog_id: The name of the catalog in the model to update.
      :type catalog_id: str


   .. py:method:: delete_item_embeddings(streaming_token, model_id, item_ids, catalog_id = None)

      Deletes KNN embeddings for a list of item IDs for a given model ID.

      :param streaming_token: The streaming token for authenticating requests to the model.
      :type streaming_token: str
      :param model_id: A unique string identifier for the model from which to delete item embeddings.
      :type model_id: str
      :param item_ids: A list of item IDs whose embeddings will be deleted.
      :type item_ids: list
      :param catalog_id: An optional name to specify which catalog in a model to update.
      :type catalog_id: str


   .. py:method:: upsert_multiple_item_embeddings(streaming_token, model_id, upserts, catalog_id = None)

      Upserts a knn embedding for multiple item ids for a model_id.

      :param streaming_token: The streaming token for authenticating requests to the model.
      :type streaming_token: str
      :param model_id: The unique string identifier of the model to upsert item embeddings to.
      :type model_id: str
      :param upserts: A list of dictionaries of the form {'itemId': ..., 'vector': [...]} for each upsert.
      :type upserts: list
      :param catalog_id: Name of the catalog in the model to update.
      :type catalog_id: str


   .. py:method:: append_data(feature_group_id, streaming_token, data)

      Appends new data into the feature group for a given lookup key recordId.

      :param feature_group_id: Unique string identifier for the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: The streaming token for authenticating requests.
      :type streaming_token: str
      :param data: The data to record as a JSON object.
      :type data: dict


   .. py:method:: upsert_multiple_data(feature_group_id, streaming_token, data)

      Update new data into the feature group for a given lookup key recordId if the recordId is found; otherwise, insert new data into the feature group.

      :param feature_group_id: Unique string identifier for the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: The streaming token for authenticating requests.
      :type streaming_token: str
      :param data: The data to record, as a list of JSON objects.
      :type data: list


   .. py:method:: append_multiple_data(feature_group_id, streaming_token, data)

      Appends new data into the feature group for a given lookup key recordId.

      :param feature_group_id: Unique string identifier of the streaming feature group to record data to.
      :type feature_group_id: str
      :param streaming_token: Streaming token for authenticating requests.
      :type streaming_token: str
      :param data: Data to record, as a list of JSON objects.
      :type data: list



.. py:data:: __version__
   :value: '1.0.5'

   

